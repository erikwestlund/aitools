---
title: "Comparing AI Tool Outputs: Maternal Health Dataset Analysis"
format:
  html:
    theme: cosmo
    toc: true
    toc-location: right
    toc-depth: 3
    embed-resources: true
---

```{=html}
<style>
.comparison-table table {
  font-size: 0.78em;
}
.comparison-table th, .comparison-table td {
  padding: 4px 6px;
}
</style>
```

::: {.callout-note}
## AI-Generated Document
This summary was generated by Claude (Opus 4.6) from all 22 notebooks in this experiment. It was then double-checked and streamlined with GPT-5.3. Content was verified against source notebooks but should still be read with appropriate skepticism.
:::

## Overview

This document compares how AI coding assistants approached the same maternal health dataset task. Each CLI tool/harness + model + prompt combination produced one notebook; all 22 are linked in [Rendered Notebooks](#rendered-notebooks).

### Exercise Setup: Detailed vs Simple Prompts

This exercise tested how much prompt structure changes analysis quality when the dataset and environment are held constant.

- **Shared conditions**: All runs used the same synthetic dataset (`simulated_maternal_health_data.csv`, n = 50,000), same project files, and same requirement to produce a complete Quarto analysis notebook.
- **Detailed prompt goal**: Evaluate whether models can execute a clearly specified statistical workflow. The prompt explicitly set the outcome (`comorbidity_count`), requested relationship-focused visualizations (postnatal care, insurance, race/ethnicity, distance), and asked for count-model reasoning (Poisson vs negative binomial, diagnostics, interpretation).
- **Simple prompt goal**: Evaluate how models choose the analysis when guidance is minimal. The prompt asked models to explore the data and build an appropriate model but did not specify the target variable, feature set, diagnostics depth, or modeling family.
- **What this isolates**: The gap between detailed and simple runs reflects how well each model+harness pair handles autonomy vs instruction-following: variable selection, model family choice, diagnostics, visualization quality, and reporting discipline.

### Ground-Truth Data Generating Process

Important context: models were **not** given this DAG during notebook generation. So this is an ex-post validity check against the known simulation design.

Most runs did what AI agents often do: optimize for patterns in available columns and prompt instructions. Very few reasoned explicitly about underlying causal mechanics. This is a reminder that AI tools cannot substitute for careful, scientific reasoning.

#### Simplified DAG

```{r dagitty-simplified, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=5}
if (requireNamespace("dagitty", quietly = TRUE)) {
  library(dagitty)

  g <- dagitty("dag {
    state_pec [pos=\"0,2\"]
    race [pos=\"0,0\"]
    parent_ses [pos=\"1,1\"]
    provider_quality [pos=\"2,2\"]
    trust [pos=\"2,0\"]
    capacity [pos=\"3,1.5\"]
    comorbidity_count [pos=\"3,0\"]
    risk_profile [pos=\"4,0.5\"]
    received_care [pos=\"5,1\"]

    state_pec -> provider_quality
    state_pec -> parent_ses
    race -> trust
    parent_ses -> capacity
    parent_ses -> comorbidity_count
    provider_quality -> trust
    provider_quality -> received_care
    comorbidity_count -> risk_profile
    risk_profile -> received_care
    trust -> received_care
    capacity -> received_care
  }")

  plot(g)
} else {
  cat("`dagitty` is not installed, so the DAG figure is not rendered in this environment.")
}
```

- **True outcome mechanism (simple-prompt target)**: `received_comprehensive_postnatal_care` is generated from latent drivers (`personal_capacity`, `willingness_to_pay`, `provider_quality`, `provider_trust`, `risk_aversion`, `risk_profile`) that are themselves functions of observed covariates.
- **True comorbidity mechanism (detailed-prompt target)**: Comorbidities are generated through a directed chain (e.g., age/obesity -> diabetes/hypertension -> gestational hypertension/preeclampsia), and `comorbidity_count` is a derived sum, not a primitive causal node.
- **Where analyses most often drift from data generating process**: (1) treating race/insurance effects as fully direct without acknowledging mediation via SES, trust, and access pathways; (2) ignoring provider-level clustering even though `provider_quality` is state/provider structured; (3) conditioning on postnatal care when modeling comorbidity count, i.e., controlling for a post-outcome variable (bad control) that can induce collider bias; (4) over-interpreting linear terms where data generating process components are nonlinear or latent.
- **Interpretation standard used here**: analyses are judged on robustness and transparency given limited prompt context; this DAG-informed section clarifies where estimates are most likely to be biased relative to simulation truth.

Three prompt types were used:

- **Chat**: A conversational prompt given to browser-based agents (ChatGPT and Claude)
- **Detailed**: A structured prompt specifying the outcome variable (`comorbidity_count`), required visualizations, and modeling approach (count regression)
- **Simple**: A minimal prompt asking the model to explore the data and build a model, with no guidance on outcome or method

Each prompt was run across multiple CLI tool/harness + model combinations. CLI tools were **Claude Code**, **Codex CLI**, and **OpenCode** (plus browser agents for the chat prompt). Models were Opus 4.6, Sonnet 4.6, GPT-5.2, GPT-5.3, Gemini 3.1 Pro, Haiku 4.5, Kimi K2.5, and GLM-5.

### Runs With Failures

| Run | Issue |
|---|---|
| Simple -- OpenCode + Kimi K2.5 | Hallucinated data structure (used columns from Kaggle's maternal health risk dataset rather than the actual data; code would error on render) |
| Detailed -- OpenCode + GLM-5 | Produced code but with issues: narrow predictor set, count-based rather than proportion-based visualizations, duplicate prompt text |
| Simple -- OpenCode + GLM-5 | Produced analysis but conclusion contains unfilled placeholder text (`"Key risk factors include [clinical variables]..."`) |


## Harnesses and Models {#harnesses-models}

### What Is a CLI Tool/Harness?

A **harness** (here, essentially an agent CLI tool) is the software layer between a user and a language model. It controls context delivery (files, errors, project structure), available tools (editing, shell, web), and output format. The same model can produce different results across harnesses.

### CLI Tools/Harnesses Used

| Harness | Type | Description | Docs |
|---|---|---|---|
| **Claude Code** | CLI agent | Anthropic's official CLI for Claude. Runs in the terminal, can read/write files, execute shell commands, and manage project context via `CLAUDE.md` files. Operates agentic loops where the model plans and executes multi-step tasks. | [claude.ai/claude-code](https://claude.ai/claude-code) |
| **Codex CLI** | CLI agent | OpenAI's open-source CLI agent for Codex/GPT models. Similar to Claude Code: reads files, runs commands, edits code. Uses a sandboxed execution environment. | [github.com/openai/codex](https://github.com/openai/codex) |
| **OpenCode** | CLI agent | An open-source, model-agnostic CLI agent that supports many providers (Anthropic, OpenAI, Google, Moonshot, Zhipu, etc.) through a unified interface. Provides file editing, shell access, and LSP integration. | [opencode.ai](https://opencode.ai) |
| **Browser Agent** | Chat interface | Standard browser chat interfaces (ChatGPT at chat.openai.com, Claude at claude.ai). The user pastes prompt and data context manually; the model responds in a single turn without file system access. | -- |

### Models Used

All prices are per 1M tokens (input/output) via [OpenCode Zen](https://opencode.ai/docs/zen/) (proxy/router). Browser pricing differs (typically subscription-based).

| Model | Provider | Input / Output | Description |
|---|---|---|---|
| **Claude Opus 4.6** | Anthropic | $5.00 / $25.00 | Anthropic's most capable model. Strong at complex reasoning, long-context analysis, and careful instruction following. |
| **Claude Sonnet 4.6** | Anthropic | $3.00 / $15.00 | Mid-tier Claude model. Balances capability with speed and cost. Often nearly as capable as Opus for straightforward coding tasks. |
| **Claude Haiku 4.5** | Anthropic | $1.00 / $5.00 | Anthropic's smallest and fastest model. Optimized for speed and low cost; less capable on complex multi-step tasks. |
| **GPT-5.3 Codex** | OpenAI | $1.75 / $14.00 | OpenAI's code-specialized model variant. Strong at code generation and tool use. |
| **GPT-5.2 / Codex** | OpenAI | $1.75 / $14.00 | Slightly older OpenAI model. Available in both standard and Codex (code-tuned) variants. |
| **Gemini 3.1 Pro** | Google | $2.00 / $12.00 | Google's mid-to-large model. Long context window (up to 1M tokens). Competitive on coding and reasoning benchmarks. |
| **Kimi K2.5** | Moonshot AI | $0.60 / $3.00 | Chinese AI lab Moonshot's model. Very low cost. Strong on Chinese-language tasks; English coding performance varies. |
| **GLM-5** | Zhipu AI | $1.00 / $3.20 | Chinese AI lab Zhipu's latest model (successor to ChatGLM series). Low cost; less established on English coding benchmarks. |
| **ChatGPT** | OpenAI | Subscription | The model behind the ChatGPT browser interface. Used here via the browser chat interface, not an API. |

Note: Prices shown reflect the context windows used here (under 200K tokens).


## Rendered Notebooks {#rendered-notebooks}

All notebooks are available as rendered HTML. Each used the same synthetic maternal health dataset (`simulated_maternal_health_data.csv`, n = 50,000).

::: {.panel-tabset}

### Chat Prompt

- [Browser Agent + ChatGPT](rendered_notebooks/02-tools-chat-browser-agent-chatgpt.html)
- [Browser Agent + Claude](rendered_notebooks/02-tools-chat-browser-agent-claude.html)

### Detailed Prompt

- [Claude Code + Opus 4.6](rendered_notebooks/02-tools-detailed-claude-code-opus-4-6.html)
- [Claude Code + Sonnet 4.6](rendered_notebooks/02-tools-detailed-claude-code-sonnet-4-6.html)
- [Codex CLI + GPT-5.2](rendered_notebooks/02-tools-detailed-codex-cli-codex-gpt-5-2.html)
- [Codex CLI + GPT-5.3](rendered_notebooks/02-tools-detailed-codex-cli-codex-gpt-5-3.html)
- [OpenCode + Opus 4.6](rendered_notebooks/02-tools-detailed-opencode-opus-4-6.html)
- [OpenCode + Haiku 4.5](rendered_notebooks/02-tools-detailed-opencode-haiku-4-5.html)
- [OpenCode + Gemini 3.1 Pro](rendered_notebooks/02-tools-detailed-opencode-gemini-3-1-pro.html)
- [OpenCode + Codex GPT-5.3](rendered_notebooks/02-tools-detailed-opencode-codex-gpt-5-3.html)
- [OpenCode + Kimi K2.5](rendered_notebooks/02-tools-detailed-opencode-kimi-k2-5.html)
- OpenCode + GLM-5 (not rendered; code has structural issues)

### Simple Prompt

- [Claude Code + Opus 4.6](rendered_notebooks/02-tools-simple-claude-code-opus-4-6.html)
- [Claude Code + Sonnet 4.6](rendered_notebooks/02-tools-simple-claude-code-sonnet-4-6.html)
- [Codex CLI + GPT-5.2](rendered_notebooks/02-tools-simple-codex-cli-codex-gpt-5-2.html)
- [Codex CLI + GPT-5.3](rendered_notebooks/02-tools-simple-codex-cli-codex-gpt-5-3.html)
- [OpenCode + Opus 4.6](rendered_notebooks/02-tools-simple-opencode-opus-4-6.html)
- [OpenCode + Haiku 4.5](rendered_notebooks/02-tools-simple-opencode-haiku-4-5.html)
- [OpenCode + GLM-5](rendered_notebooks/02-tools-simple-opencode-glm-5.html)
- [OpenCode + Gemini 3.1 Pro](rendered_notebooks/02-tools-simple-opencode-gemini-3-1-pro.html)
- [OpenCode + Codex GPT-5.3](rendered_notebooks/02-tools-simple-opencode-codex-gpt-5-3.html)
- [OpenCode + Kimi K2.5](rendered_notebooks/02-tools-simple-opencode-kimi-k2-5.html) (hallucinated data; errors on render)

:::


## At-a-Glance Feature Comparison {#feature-comparison}

Each model was evaluated on the same set of features. A check (&#10003;) indicates the notebook included that element; blank means it did not.

::: {.panel-tabset}

### Chat & Detailed Prompt

::: {.comparison-table}

| Feature | Browser ChatGPT | Browser Claude | CC Opus | CC Sonnet | Codex GPT-5.2 | Codex GPT-5.3 | OC Opus | OC Haiku | OC GLM-5 | OC Gemini | OC Codex 5.3 | OC Kimi |
|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **janitor::clean_names()** | &#10003; | &#10003; | | | | | | | &#10003; | &#10003; | | &#10003; |
| **Ordered factors** | &#10003; | &#10003; | &#10003; | &#10003; | | &#10003; | &#10003; | | | | | &#10003; |
| **Scaled continuous vars** | | | | | | | | &#10003; | | | | |
| **Fitted Poisson first** | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | | | &#10003; | |
| **Fitted NB** | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; |
| **Formal overdispersion test** | &#10003; | &#10003; | | | | | &#10003; | &#10003; | | | | |
| **Tested zero-inflation** | | &#10003; | | | | &#10003; | | | | | | |
| **Random effects (GLMM)** | &#10003; | | | | | | | | | | | |
| **AIC/BIC comparison** | | &#10003; | &#10003; | &#10003; | | &#10003; | &#10003; | &#10003; | | | | &#10003; |
| **DHARMa diagnostics** | &#10003; | &#10003; | | | | | | | | | | |
| **Residual plots** | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | | &#10003; | |
| **Rootogram/expected counts** | | | | | | | &#10003; | | | | | |
| **Coefficient/forest plot** | | | | | | | &#10003; | | | | | |
| **Relational visualizations** | &#10003; | &#10003; | &#10003; | &#10003; | | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; |
| **Influence diagnostics** | | | | | | | | | | | | &#10003; |
| **Explicit limitations section** | | | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; |
| **# Predictors** | 6 | 9 | 7 | 7 | 6 | 6 | 9 | 5 | 4 | 4 | 8 | 7 |
| **# Visualizations** | 5 | 6 | 5 | 5 | 4 | 4 | 10+ | 7 | 5 | 3 | 3 | 6 |

:::

CC = Claude Code, OC = OpenCode, Codex = Codex CLI.

### Simple Prompt

::: {.comparison-table}

| Feature | CC Opus | CC Sonnet | OC Opus | OC Haiku | OC GLM-5 | OC Gemini | OC Codex 5.3 | OC Kimi* | Codex GPT-5.2 | Codex GPT-5.3 |
|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **Chose logistic regression** | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | | &#10003; | &#10003; |
| **Used correct data columns** | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | &#10003; | | &#10003; | &#10003; |
| **Ordered factors** | &#10003; | &#10003; | &#10003; | | | | | | | |
| **Quadratic/nonlinear terms** | | | | | | | &#10003; | | &#10003; | |
| **Composite comorbidity var** | | | &#10003; | | | | | | | |
| **State as predictor** | | | | &#10003; | | | &#10003; | | &#10003; | |
| **Train/test split** | | | | &#10003; | | | &#10003; | | | |
| **Confusion matrix** | | &#10003; | | &#10003; | | | &#10003; | &#10003; | | &#10003; |
| **AUC/ROC** | | | | &#10003; | &#10003; | | &#10003; | | | |
| **Residual diagnostics** | | &#10003; | &#10003; | | | | | | | |
| **Pseudo-R&sup2;** | &#10003; | &#10003; | &#10003; | | | | | | &#10003; | &#10003; |
| **Brier score** | | | | | | | | | &#10003; | |
| **OR/forest plot** | &#10003; | | &#10003; | | | | | | | |
| **Relational visualizations** | &#10003; | &#10003; | &#10003; | | | | | | | &#10003; |
| **Explicit limitations** | | &#10003; | &#10003; | | | | | | &#10003; | &#10003; |
| **# Predictors** | 16 | 16 | 10 | All | 15 | 6 | All+splines | 6 | All+state | 16 |
| **# Visualizations** | 6 | 5 | 7 | 1 | 1 | 0 | 0 | 6 | 0 | 4 |

:::

\* Kimi K2.5 produced output but hallucinated the data structure (used Kaggle maternal health risk columns instead of the actual dataset; would error on render).

:::

## Chat & Detailed Prompt Results

The chat and detailed prompts both directed models to analyze `comorbidity_count` (the sum of 8 binary comorbidity indicators). All completed notebooks used negative binomial regression; most compared it against Poisson to address overdispersion.

### Data Cleaning

Data-cleaning approaches varied considerably:

- **Column naming**: Most used `janitor::clean_names()`; Claude Code and Codex variants used manual transformations or custom `normalize_names()` functions
- **Factor ordering**: Claude Code (Opus/Sonnet), OpenCode Opus, and OpenCode Kimi created ordered income/education factors with explicit levels; others used unordered factors or levels that sometimes mismatched data values
- **Binary variables**: Most converted comorbidities to integer (0/1); Gemini 3.1 Pro and Kimi K2.5 used logical (TRUE/FALSE); Haiku 4.5 used labeled factors ("No"/"Yes"); Browser Claude used robust `case_when()` logic for mixed string/logical inputs
- **Scaling**: Haiku 4.5 was the only notebook that centered and scaled continuous predictors (age, distance) before modeling
- **Derived variables**: Browser Claude created `log1p_distance` and `distance_10`; OpenCode Codex GPT-5.3 created `distance_to_provider_10` (distance/10)

### Visualizations

All notebooks were asked to visualize relationships among postnatal care receipt, insurance, race/ethnicity, and distance to provider. Quality varied:

- **Most comprehensive visualization set**: OpenCode + Opus 4.6 produced 10+ plots, including a coefficient forest plot and observed-vs-expected rootogram, formatted with `kableExtra` and `patchwork`
- **Unique approaches**: Codex CLI GPT-5.3 created a race/ethnicity-by-insurance heatmap; Browser ChatGPT compared observed vs Poisson-expected comorbidity distributions; Kimi K2.5 included an insurance-by-race/ethnicity interaction plot
- **Most limited visualization scope**: Codex CLI GPT-5.2 produced only univariate bar charts/histograms, with no cross-variable relationships

### Modeling Decisions

All completed notebooks used negative binomial models, but the path differed:

- **Overdispersion thresholds**: Codex variants used explicit thresholds (1.2 or 1.5 Pearson/df); others used AIC comparisons or variance/mean ratios without explicit cutoffs
- **Test methods**: OpenCode Opus 4.6 and Haiku 4.5 used formal likelihood ratio tests; Browser ChatGPT used DHARMa `testDispersion()`; most others used informal AIC comparisons
- **Mixed effects**: Browser ChatGPT was the only notebook to use a GLMM (`lme4::glmer` with `provider_id` as random intercept) -- all others used standard GLMs
- **Zero-inflation**: Browser Claude was the only notebook to formally test zero-inflation, fitting a ZINB model via `pscl::zeroinfl()` and comparing with a Vuong test; Codex CLI GPT-5.3 checked observed vs predicted zero rates
- **No Poisson comparison**: GLM-5, Gemini 3.1 Pro, and Kimi K2.5 skipped Poisson and fit only NB models

### Model Specifications

Predictor sets ranged from 4 to 9 variables:

- **Largest predictor set** (9 predictors): OpenCode Opus 4.6 and Browser Claude included age, income, education, race/ethnicity, insurance, job type, dependents, distance, and postnatal care receipt
- **Smallest predictor set** (4 predictors): GLM-5 and Gemini 3.1 Pro used only age, insurance, distance, and race/ethnicity
- **Postnatal care as predictor**: Browser Claude, OpenCode Opus 4.6, Haiku 4.5, and OpenCode Codex GPT-5.3 included `received_comprehensive_postnatal_care` as a predictor of comorbidity count
- **Distance scaling**: OpenCode Codex GPT-5.3 divided distance by 10; Browser Claude used log1p transformation; Browser ChatGPT used `scale()`; Haiku centered and scaled
- **State**: No chat/detailed notebook included state as a predictor (though several noted this as a limitation)

### Diagnostics Quality

Diagnostic sophistication varied widely:

- **Highest diagnostic depth**: OpenCode Opus 4.6 (formal LR test, rootogram, residuals-vs-fitted with loess, deviance residual histogram, coefficient forest plot); Browser Claude (full DHARMa suite: `testDispersion()`, `testZeroInflation()`, `testUniformity()`, plus Vuong test and AIC/BIC table)
- **Good**: Haiku 4.5 (Pearson residuals vs fitted, histogram, Q-Q, index plots, formal LR test); Claude Code variants and Codex CLI GPT-5.3 (standard base R `plot()` residual panels plus AIC comparisons); Kimi K2.5 (Cook's distance and influence diagnostics)
- **Minimal**: Gemini 3.1 Pro produced no residual diagnostics at all; GLM-5 had only a Q-Q plot

### Responsible Reporting

All completed notebooks noted that associations should not be interpreted causally, but depth varied:

- **Most explicit limitations coverage**: Claude Code (Opus and Sonnet) each listed 6 explicit limitations, including cross-sectional design, unmodeled clustering, self-reported income bias, synthetic data, and prompt-driven covariate selection
- **OpenCode Opus 4.6** provided the most detailed prose discussion of model assumptions and their implications
- **Kimi K2.5** included a thorough limitations section covering causality, data quality, generalizability, clustering, missing data, and unmeasured confounding, plus 5 future research recommendations
- **Haiku 4.5** addressed causality, synthetic data, clustering, race as social construct, and unmeasured confounders
- **Browser agents**: ChatGPT had no explicit limitations section; Claude had minimal discussion
- **Most limited limitations coverage**: GLM-5 and Codex CLI GPT-5.2 mentioned limitations only briefly


## Simple Prompt Results

The simple prompt gave no guidance on outcome variable or modeling strategy. Every completed notebook using correct columns chose logistic regression on `received_comprehensive_postnatal_care`.

### Kimi K2.5: A Different Analysis Entirely

OpenCode + Kimi K2.5 hallucinated the data structure. Instead of actual columns, it referenced `RiskLevel`, `SystolicBP`, `DiastolicBP`, `BS`, `BodyTemp`, and `HeartRate` -- columns from the Kaggle "Maternal Health Risk Data Set." It then fit a multinomial logistic model for 3-level risk categories. This code would fail on render because those columns are absent from the CSV. This appears to be pattern retrieval from a similarly named dataset rather than reading the provided data.

It would likely be easy to steer the model back to the correct columns with a short corrective prompt. I left this run unchanged because the failure mode itself is instructive.

### Modeling Strategy

The 9 completed notebooks using correct data all chose logistic regression, but implementation varied:

- **Standard GLM**: Most used `glm(family = binomial)` with linear predictor terms
- **Nonlinear terms**: OpenCode Codex GPT-5.3 used natural splines (`ns(age, df = 4)` and `ns(distance_to_provider, df = 4)`); Codex CLI GPT-5.2 used a quadratic age term (`age + I(age^2)`) and log-transformed distance
- **Composite variables**: OpenCode Opus 4.6 uniquely created a `comorbidity_count` predictor (sum of 8 binary indicators) instead of including each individually
- **Predictive framing**: OpenCode Haiku 4.5 and OpenCode Codex GPT-5.3 used 80/20 train/test splits with classification metrics -- a machine-learning framing rather than statistical inference

### Predictor Selection

Predictor count and choice varied widely:

- **Most parsimonious** (6 predictors): Gemini 3.1 Pro used only age, education, insurance, distance, obesity, and hypertension
- **Kitchen sink** (`~ .`): Haiku 4.5 included everything except `id` and `provider_id`, including all 51 state levels
- **State fixed effects**: Codex CLI GPT-5.2 and OpenCode Codex GPT-5.3 also included state
- **Composite approach**: OpenCode Opus 4.6 collapsed 8 comorbidity indicators into a single `comorbidity_count` variable for parsimony
- **Standard approach** (16 predictors): Claude Code variants and Codex CLI GPT-5.3 included all substantive predictors except state, with individual comorbidity terms

### Visualization Quality

- **Most comprehensive visualization set**: OpenCode Opus 4.6 produced 7 subplots across 4 figure chunks using patchwork, including an odds-ratio forest plot with confidence intervals
- **Good**: Claude Code Opus (5 plots + forest plot), Claude Code Sonnet (5 plots including age-by-education interaction), Codex CLI GPT-5.3 (4 plots)
- **Minimal**: Haiku 4.5 and GLM-5 each produced only 1 plot
- **None**: Gemini 3.1 Pro, OpenCode Codex GPT-5.3, and Codex CLI GPT-5.2 produced zero visualizations

### Diagnostics

- **Highest diagnostic depth**: OpenCode Opus 4.6 used binned residuals (appropriate for logistic regression), deviance residuals, Pearson residuals, and pseudo-R&sup2;
- **Classification-oriented**: Haiku 4.5 hand-coded AUC via the trapezoid rule; Codex GPT-5.3 implemented a custom rank-based AUC function; GLM-5 used the `pROC` package; Codex CLI GPT-5.2 reported Brier score alongside accuracy
- **Standard**: Claude Code variants and Codex CLI GPT-5.3 reported AIC, McFadden pseudo-R&sup2;, and odds ratio tables
- **Minimal**: Gemini 3.1 Pro reported only the model summary with `exp(coef())` -- no AIC, no residuals, no classification metrics

### Conclusions Drawn

Despite different predictor sets, completed notebooks converged on similar substantive findings:

- **Insurance**: All found significant associations between insurance type and postnatal care receipt
- **Education**: Higher education levels consistently associated with higher care receipt
- **Distance**: Greater distance to provider associated with lower care receipt
- **Comorbidities**: Claude Code Opus 4.6 noted the "paradoxical" finding that comorbidities were positively associated with care (likely reflecting clinical follow-up rather than health status)

### Limitations

- **Most explicit limitations coverage** (6 items): OpenCode Opus 4.6 explicitly discussed observational design, clustering, self-reported income, synthetic data, no interaction terms, and multiple comparisons
- **Good**: Claude Code Sonnet (4 items), Codex CLI GPT-5.3 (3 items), Codex CLI GPT-5.2 (noted modest fit and suggested additional modeling strategies)
- **Incomplete**: GLM-5's conclusion contained unfilled placeholder text
- **None mentioned**: Haiku 4.5, Gemini 3.1 Pro, OpenCode Codex GPT-5.3, and Claude Code Opus 4.6 (which discussed findings interpretively but had no formal limitations section)


## Harness Comparison: Same Model, Different Harness {#harness-comparison}

Two models were run across multiple harnesses, allowing direct comparison of how the CLI tool/harness shapes output for the same underlying model.

### Opus 4.6: Claude Code vs OpenCode

Opus 4.6 was run via Claude Code and OpenCode for both detailed and simple prompts -- 4 notebooks total.

::: {.panel-tabset}

#### Detailed Prompt

| Dimension | Claude Code | OpenCode |
|---|---|---|
| **Structure** | Clean, pedagogical sections with narrative transitions | More comprehensive; publication-style formatting with `kableExtra` |
| **Predictors** | 7 (excluded job_type, postnatal care) | 9 (broadest set; included job_type, postnatal care) |
| **Overdispersion** | Informal: deviance/df + AIC comparison | Formal: AIC + likelihood ratio test with chi-square p-value |
| **Visualizations** | 5 (relational bar charts, boxplots, distribution) | 10+ (added forest plot, rootogram, residual panels via patchwork) |
| **Diagnostics** | Base R `plot()` 2x2 panel | LR test, rootogram, residuals-vs-fitted with loess, deviance histogram |
| **Limitations** | 6 explicit items, well-organized list | Detailed prose discussing each assumption and its implications |
| **Formatting** | Standard kable tables | `kableExtra` with significance stars, polished layout |

**Takeaway**:

Both runs completed end-to-end analyses. The Claude Code version used a more instructional structure. The OpenCode version included a larger predictor set, more formal tests, more plots, and more diagnostics. In practice, the two versions differ mostly in depth and presentation style.

#### Simple Prompt

| Dimension | Claude Code | OpenCode |
|---|---|---|
| **Outcome** | Logistic on postnatal care | Logistic on postnatal care |
| **Predictors** | 16 individual (all comorbidities separate) | 10 (collapsed comorbidities into composite count) |
| **Visualizations** | 5 plots + OR forest plot | 7 sub-plots via patchwork + OR forest plot |
| **Diagnostics** | AIC, pseudo-R&sup2;, OR table | AIC, pseudo-R&sup2;, binned residuals, deviance residuals, Pearson residuals |
| **Unique** | Comorbidity prevalence table, paradoxical finding discussion | Binned residual plot (rare, appropriate for logistic regression) |
| **Limitations** | Implicit only (interpretive notes, no formal section) | 6 explicit items including clustering, interactions, multiple comparisons |

**Takeaway**:

The same pattern appears in the simple prompt. Claude Code used a more teaching-oriented structure. OpenCode included additional diagnostics (including binned residuals) and a longer limitations section. OpenCode also used a composite comorbidity predictor instead of 8 separate binary terms.

:::

### Codex GPT-5.3: Codex CLI vs OpenCode

GPT-5.3 was run via Codex CLI and OpenCode for both prompts -- 4 notebooks total.

::: {.panel-tabset}

#### Detailed Prompt

| Dimension | Codex CLI | OpenCode |
|---|---|---|
| **Structure** | Straightforward code-first approach | Similar code-first approach |
| **Predictors** | 6 (care status, distance, insurance, race, age, dependents) | 8 (added edu, income) |
| **Overdispersion** | Pearson/df with threshold 1.2; also AIC | Pearson/df with threshold 1.5 |
| **Visualizations** | 4 (including unique heatmap tile) | 3 (boxplot, bar charts) |
| **Diagnostics** | Residuals + observed vs predicted zero rates | Base R residuals + Q-Q plot |
| **Limitations** | 5 items (mean-variance, independence, functional form, omitted vars, outcome construction) | 4 items (confounding, outcome construction, residual caveats, data specificity) |
| **Notable** | Heatmap of care rate by race x insurance | Scaled distance by dividing by 10; hand-computed Wald CIs |

**Takeaway**:

The two harnesses produced similar GPT-5.3 outputs on the detailed prompt. Codex CLI added a heatmap, a zero-rate check, and a longer limitations section. Relative to the Opus comparison, differences were smaller.

#### Simple Prompt

| Dimension | Codex CLI | OpenCode |
|---|---|---|
| **Predictors** | 16 (no state) | All including state + natural splines |
| **Visualizations** | 4 (care by insurance, care by race, distance histogram, distance boxplot) | 0 |
| **Diagnostics** | AIC, pseudo-R&sup2;, confusion matrix | Custom AUC, classification metrics, prevalence-based threshold |
| **Limitations** | 3 explicit items | None |
| **Approach** | Statistical inference framing | Machine learning framing (train/test split) |

**Takeaway**:

Here, harnesses produced markedly different analyses from the same model. Codex CLI used a traditional statistical-analysis framing with visualizations, pseudo-R&sup2;, and a limitations section. OpenCode used a predictive-modeling framing with splines, train/test splits, and custom AUC code, but no visualizations or limitations section.

:::

### Summary: Does Harness Matter?

**Yes, but the effect depends on the model.**

- **For Opus 4.6**, harness effects were large. OpenCode runs included more predictors, tests, and diagnostics; Claude Code runs were more templated and instructional in structure.
- **For GPT-5.3**, harness effects were smaller on the detailed prompt and larger on the simple prompt (statistical vs ML framing split).
- **Claude Code reduced between-run variance.** Opus and Sonnet in Claude Code produced very similar notebook structure, predictors, diagnostics, and limitations.
- **OpenCode showed wider spread across models.** OpenCode outputs ranged from very detailed runs to clear failure modes (for example, Kimi K2.5 hallucinating the data structure).


## Would Different Approaches Change the Scientific Conclusions? {#scientific-conclusions}

Notebooks used different modeling choices, and those choices changed coefficient size and significance calls. Below are coefficient/SE snapshots for common terms.

### Detailed Prompt: Coefficient Snapshots (Count Models)

| Run | Model | Age coef (SE) | Distance coef (SE) | Insurance: Private coef (SE) | Insurance: State-provided coef (SE) |
|---|---|---:|---:|---:|---:|
| Browser ChatGPT | GLMM | 0.0155 (0.00086) | -0.00210 (0.00528) [a] | -0.00008 (0.01249) | -0.0164 (0.01381) |
| Claude Code + Opus 4.6 | NB | 0.0159 (0.00093) | -0.00014 (0.00038) | -0.0030 (0.01326) | -0.0166 (0.01443) |
| OpenCode + Opus 4.6 | NB | 0.0164 (0.00083) | -0.00022 (0.00034) | -0.0140 (0.01287) | -0.0282 (0.01308) |
| OpenCode + Codex GPT-5.3 | NB | 0.016 (0.001) [b] | -0.001 (0.004) [c] | -0.005 (0.013) | -0.017 (0.014) |

[a] `scale(distance_to_provider)` in that run (standardized distance, not raw miles).

[b] Values rounded to 3 decimals in rendered output.

[c] `distance_to_provider_10` in that run (distance divided by 10).

### Simple Prompt: Coefficient Snapshots (Logistic Models)

| Run | Age coef (SE) | Distance coef (SE) | Insurance: Private coef (SE) | Insurance: State-provided coef (SE) |
|---|---:|---:|---:|---:|
| Claude Code + Opus 4.6 | -0.00334 (0.00164) [d] | -0.00252 (0.00067)*** | 0.19172 (0.02515)*** | 0.11964 (0.02586)*** |
| Claude Code + Sonnet 4.6 | -0.00201 (0.00185) | -0.00244 (0.00075)** | 0.16606 (0.02841)*** | 0.07981 (0.02861)** |
| OpenCode + Opus 4.6 | -0.00311 (0.00164) [d] | -0.00254 (0.00067)*** | 0.19162 (0.02514)*** | 0.11986 (0.02585)*** |
| OpenCode + Haiku 4.5 | -0.00088 (0.00185) | -0.00271 (0.00075)*** | 0.09487 (0.02952)** | 0.01126 (0.03014) |
| OpenCode + Gemini 3.1 Pro | -0.00391 (0.00157)* | -0.00256 (0.00067)*** | 0.26084 (0.02247)*** | 0.12962 (0.02572)*** |

[d] Marginal (p < 0.10 but >= 0.05).

Quick read: distance remains negative and usually highly significant across runs; insurance coefficients stay positive but vary in magnitude (especially in Haiku); age is the least stable in significance.

### What Remained Robust

Despite all these differences, a few findings held across every specification:

- **Age positively predicts comorbidity count** (all detailed notebooks agree)
- **Insurance type is significantly associated** with both comorbidity count and postnatal care receipt (direction varies by reference category)
- **Higher education is associated with higher postnatal care receipt** (all simple notebooks that included it agree)
- **Distance to provider has a small or null association** with comorbidity count but a more meaningful negative association with postnatal care receipt


## Key Takeaways

**Model choice affects analysis depth.** In this set of runs, Opus 4.6 and GPT-5.3 generally included more diagnostics, visualizations, and limitations detail. Gemini 3.1 Pro and GLM-5 generally included fewer of these elements. Haiku 4.5 and Kimi K2.5 varied more across prompt types.

**Harness (CLI tool) shapes output style.** Claude Code outputs were more uniform in structure. OpenCode outputs showed more variation in scope and style. Codex CLI was between those two patterns.

**Core model-family choices were similar across completed runs.** Most detailed runs used NB-family count models and most simple runs used logistic models. Differences were mainly in assumption checks, diagnostics, limitations reporting, and visualization coverage.

**Failure modes varied.** One run hallucinated the data structure entirely (Simple Kimi K2.5). One produced incomplete placeholder text (Simple GLM-5). One had structural code issues (Detailed GLM-5). These failures were more common in OpenCode, where models had less scaffolding.

**Prompt specificity changed behavior for some models.** Kimi K2.5 produced a detailed-prompt run with influence diagnostics and detailed limitations, then hallucinated the dataset structure on the simple prompt. Haiku 4.5 showed a similar but less extreme shift from detailed to simple prompts. Opus and GPT-5.3 changed less across prompt specificity in this experiment.


## Did Detailed Prompts Produce More Unified Results?

Short answer: **partly yes**. Detailed prompts increased convergence on analysis target and workflow, but did not fully standardize model specification.

Important: outcomes and model families differ between detailed and simple prompts, so coefficient magnitudes are **not** directly comparable across prompt types.

| Convergence indicator | Detailed prompt | Simple prompt |
|---|---|---|
| Target variable convergence | High: prompt specified `comorbidity_count` | Moderate-high: most chose `received_comprehensive_postnatal_care`, but not required |
| Model-family convergence | High: count-model workflow with NB emphasis | High: logistic was the dominant choice |
| Predictor-set dispersion | Moderate: roughly 4 to 9 predictors in many runs | High: ranged from parsimonious to near-kitchen-sink specifications |
| Use of additional structural terms | Mixed: some included postnatal care as a predictor of comorbidity count | Mixed: some included state fixed effects/nonlinear terms; others did not |
| Diagnostics/reporting consistency | Moderate-high: diagnostics sections usually present | Mixed: diagnostics depth ranged from minimal to extensive |
| Failure-mode frequency | Lower in detailed runs | Higher in simple runs (including hallucinated structure and placeholder text) |

Interpretation: detailed prompts made outputs more similar on target and workflow. Variation still remained in covariates, transformations/scaling, diagnostics depth, and control choices that affect interpretation.
