[
  {
    "objectID": "slides/03-context-management.html#follow-along-on-github",
    "href": "slides/03-context-management.html#follow-along-on-github",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Follow Along on GitHub",
    "text": "Follow Along on GitHub\nAll the code, datasets, and examples for this session are available on GitHub.\n\nFollow along: You can clone or pull down the repository to run the examples yourself.\nReproducible: All workflows and agent tasks shown today are in the repo.\nExperiment: Feel free to run the agent prompts on your own machine."
  },
  {
    "objectID": "slides/03-context-management.html#section",
    "href": "slides/03-context-management.html#section",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Context Management\n\n\nStructuring Prompts and Context Files"
  },
  {
    "objectID": "slides/03-context-management.html#the-importance-of-context",
    "href": "slides/03-context-management.html#the-importance-of-context",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Importance of Context",
    "text": "The Importance of Context\nAn LLM is a reasoning engine, but without context, it operates in a vacuum.\n\nUnder-contextualized: “Clean this data and run a model.” (Model guesses what clean means, guesses the model).\nOver-contextualized: “Here is 50 pages of our entire codebase and every README we have.” (Model loses focus, token costs explode).\n\nGoal: Provide exactly enough context to frame the problem and set constraints, and no more."
  },
  {
    "objectID": "slides/03-context-management.html#what-is-a-context-file",
    "href": "slides/03-context-management.html#what-is-a-context-file",
    "title": "AI Tools for Data Science and Statistics",
    "section": "What is a Context File?",
    "text": "What is a Context File?\nContext files (AGENTS.md, CLAUDE.md, .cursorrules) are markdown files placed in the root of your project directory.\nThey provide “always-on” instructions to AI agents about: - Your coding style and conventions (e.g., “Use snake_case and dplyr”). - How to run tests or build the project. - Specific domain constraints (e.g., “Do not interpret correlations as causal”).\nWhen an agent loads your repository, it automatically reads these rules."
  },
  {
    "objectID": "slides/03-context-management.html#do-context-files-actually-work",
    "href": "slides/03-context-management.html#do-context-files-actually-work",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Do Context Files Actually Work?",
    "text": "Do Context Files Actually Work?\nGloaguen et al. (2025): “Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?”\nTested AI agents on 138 real-world GitHub issues using popular LLMs with and without context files.\nThe Findings: - LLM-generated context files reduced task success rates. - Developer-provided context files only marginally improved performance (+4%). - Context files increased inference costs by over 20%."
  },
  {
    "objectID": "slides/03-context-management.html#why-do-they-backfire",
    "href": "slides/03-context-management.html#why-do-they-backfire",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Why Do They Backfire?",
    "text": "Why Do They Backfire?\nWhy does giving the agent more instructions make it perform worse?\n\nOver-exploration: Agents followed context file instructions to test and search extensively, spending more steps traversing files instead of solving the problem.\nIncreased Reasoning Burden: Adding complex instructions required the LLMs to spend 10-22% more “reasoning tokens” per task.\nRedundancy: LLM-generated context files often repeated existing documentation, adding noise without signal."
  },
  {
    "objectID": "slides/03-context-management.html#summary",
    "href": "slides/03-context-management.html#summary",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Summary",
    "text": "Summary\nIf context files make tasks harder, how should we use them?\nKeep them minimal and constraint-focused: 1. State your primary toolchain (“Use R, tidyverse, and ggplot2”). 2. State absolute constraints (“Never run git commit without asking”). 3. State interpretation rules (“Default to descriptive language; do not make causal claims”).\nDo not try to explain your entire data pipeline in CLAUDE.md. Let the agent read the specific code it needs."
  },
  {
    "objectID": "slides/03-context-management.html#good-vs.-bad-prompts-for-stats",
    "href": "slides/03-context-management.html#good-vs.-bad-prompts-for-stats",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Good vs. Bad Prompts for Stats",
    "text": "Good vs. Bad Prompts for Stats\nBad Prompt: “Look at my dataset and tell me what the relationship is between age and blood pressure.”\nGood Prompt: “Using data/clinical.csv, fit a linear regression of systolic blood pressure on age, controlling for BMI. Check for heteroskedasticity. Return the model summary table using gtsummary and describe the age coefficient descriptively without causal language.”\nSpecificity prevents hallucinations."
  },
  {
    "objectID": "slides/03-context-management.html#section-1",
    "href": "slides/03-context-management.html#section-1",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "The Code-Not-Data Principle\n\n\nBridging AI and Secure Environments"
  },
  {
    "objectID": "slides/03-context-management.html#the-phi-problem",
    "href": "slides/03-context-management.html#the-phi-problem",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The PHI Problem",
    "text": "The PHI Problem\nIn biostatistics and clinical research, Protected Health Information (PHI) is heavily restricted.\n\nYou cannot upload PHI to ChatGPT.\nYou cannot let Claude Code read a folder containing identifiable patient data.\nLocal models are safer, but often lack the reasoning capability of frontier cloud models for complex logic.\n\nHow do we use frontier AI tools without leaking data?"
  },
  {
    "objectID": "slides/03-context-management.html#the-code-not-data-principle",
    "href": "slides/03-context-management.html#the-code-not-data-principle",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Code-Not-Data Principle",
    "text": "The Code-Not-Data Principle\nSend the schema, not the rows.\nIf the AI knows the structure of the data, it can write the code to analyze it, without ever seeing the actual observations.\n\nExport a codebook or synthetic schema.\nUse the AI in a safe environment to build the analytical pipeline.\nMove the code back into the restricted environment to run on the real data."
  },
  {
    "objectID": "slides/03-context-management.html#synthetic-data-as-a-strategy",
    "href": "slides/03-context-management.html#synthetic-data-as-a-strategy",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Synthetic Data as a Strategy",
    "text": "Synthetic Data as a Strategy\nSynthetic data accelerates the Code-Not-Data loop:\n\nLook at your real data (inside the secure enclave).\nGenerate a synthetic dataset with the exact same column names, types, and rough distributions.\nMove the synthetic dataset to your laptop.\nLet the AI agent freely write cleaning, EDA, and modeling scripts against the synthetic data.\nReview the code.\nPush the code to Git, pull it inside the secure enclave, and run it on the real data."
  },
  {
    "objectID": "slides/03-context-management.html#git-as-a-bridge",
    "href": "slides/03-context-management.html#git-as-a-bridge",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Git as a Bridge",
    "text": "Git as a Bridge\nGit is the perfect transport mechanism for this workflow.\n\nIt tracks provenance (who wrote this code?).\nIt provides a clear audit trail (what did the AI change?).\nIt enforces a strict boundary: you commit and push .R scripts, while data/ is strictly .gitignore’d.\n\nThis cleanly separates the untrusted reasoning environment from the secure execution environment."
  },
  {
    "objectID": "slides/03-context-management.html#hands-on-secure-workflow-demo",
    "href": "slides/03-context-management.html#hands-on-secure-workflow-demo",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Hands-On: Secure Workflow Demo",
    "text": "Hands-On: Secure Workflow Demo\nWe will now demonstrate this loop:\n\nWe have a synthetic dataset: data/synthetic/simulated_maternal_health_data.csv.\nWe will use an AI agent to build a data cleaning and visualization pipeline.\nWe will enforce strict context rules and iterate.\nWe will audit the output to catch where the AI confidently misleads."
  },
  {
    "objectID": "slides/01-foundations.html#section",
    "href": "slides/01-foundations.html#section",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Good Old-Fashioned AI\n\n\nBefore LLMs: 70 years of trying to make machines think."
  },
  {
    "objectID": "slides/01-foundations.html#goals-for-today",
    "href": "slides/01-foundations.html#goals-for-today",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Goals for Today",
    "text": "Goals for Today\n\nUnderstand what an LLM is (and is not)\nPlace the current moment in historical context\nSee the recurring failure modes the research keeps finding\nLeave with a clear mindset: skeptical, empirical, and tool-agnostic"
  },
  {
    "objectID": "slides/01-foundations.html#the-initial-vision-1950s-1960s",
    "href": "slides/01-foundations.html#the-initial-vision-1950s-1960s",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Initial Vision (1950s-1960s)",
    "text": "The Initial Vision (1950s-1960s)\n\n1950: Turing proposes the imitation game. “Can machines think?”\n1956: Dartmouth workshop coins “Artificial Intelligence.” Minsky, Simon, and others predict human-level AI within a generation.\n1958: Rosenblatt builds the Perceptron, the first neural network.\n\n\nThe New York Times reports the Navy expects it to “walk, talk, see, write, reproduce itself, and be conscious.”"
  },
  {
    "objectID": "slides/01-foundations.html#the-promise",
    "href": "slides/01-foundations.html#the-promise",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Promise",
    "text": "The Promise\nMarvin Minsky (1967): “Within a generation… the problem of creating ‘artificial intelligence’ will substantially be solved.”\n\nThey were not close."
  },
  {
    "objectID": "slides/01-foundations.html#the-symbolic-era-gofai",
    "href": "slides/01-foundations.html#the-symbolic-era-gofai",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Symbolic Era: GOFAI",
    "text": "The Symbolic Era: GOFAI\n“Good Old-Fashioned AI” (Haugeland 1985) posits that intelligence is symbol manipulation.\nThe approach: Encode human knowledge as logical rules, then reason over them.\n\n\n\n\n\n\n\n\n\nSystem\nWhat It Did\nLimitation\n\n\n\n\nELIZA (1966)\nPattern-matched to simulate a therapist\nNo understanding; pure string tricks\n\n\nSHRDLU (1971)\nUnderstood natural language about blocks on a table\nOnly worked in a tiny toy world\n\n\nMYCIN (1976)\nDiagnosed bacterial infections with ~600 rules\nCouldn’t learn; every rule hand-written\n\n\nCYC (1984-)\nAttempted to encode all common sense\n40+ years later, still not done"
  },
  {
    "objectID": "slides/01-foundations.html#the-critics-dreyfus",
    "href": "slides/01-foundations.html#the-critics-dreyfus",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Critics: Dreyfus",
    "text": "The Critics: Dreyfus\nHubert Dreyfus, What Computers Can’t Do (1972):\nHuman expertise is embodied and intuitive, not rule-based. A chess master doesn’t search a tree. They see the board. GOFAI can’t capture this.\n(His brother Stuart Dreyfus, an operations researcher, co-developed the skill acquisition model behind this critique.)"
  },
  {
    "objectID": "slides/01-foundations.html#the-critics-searle",
    "href": "slides/01-foundations.html#the-critics-searle",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Critics: Searle",
    "text": "The Critics: Searle\nJohn Searle “Chinese Room” (1980):\nA system can manipulate symbols perfectly and understand nothing. Syntax is not semantics.\n\nDreyfus again What Computers Still Can’t Do (1992): After 20 more years, the same problems remain."
  },
  {
    "objectID": "slides/01-foundations.html#first-ai-winter-1974-1980",
    "href": "slides/01-foundations.html#first-ai-winter-1974-1980",
    "title": "AI Tools for Data Science and Statistics",
    "section": "First AI Winter (1974-1980)",
    "text": "First AI Winter (1974-1980)\nOverpromising leads to underfunding.\n\nLighthill Report (UK, 1973): AI has failed to deliver on its promises\nDARPA cuts funding; the field contracts"
  },
  {
    "objectID": "slides/01-foundations.html#second-ai-winter-1987-1993",
    "href": "slides/01-foundations.html#second-ai-winter-1987-1993",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Second AI Winter (1987-1993)",
    "text": "Second AI Winter (1987-1993)\nIt happens again.\n\nExpert systems boom, then bust. Too brittle, too expensive to maintain\nJapan’s Fifth Generation Computer Systems fails\nNeural network research stalls"
  },
  {
    "objectID": "slides/01-foundations.html#ai-revival-1990s-now",
    "href": "slides/01-foundations.html#ai-revival-1990s-now",
    "title": "AI Tools for Data Science and Statistics",
    "section": "AI Revival (1990s-now)",
    "text": "AI Revival (1990s-now)\nAI stopped trying to be “intelligent” and started being useful:\n\nMachine learning replaces hand-coded rules with learning from data\nStatistical methods dominate: SVMs, random forests, boosting. These are tools many of you already likely use.\nDeep learning breakthrough (2012): AlexNet wins ImageNet by a landslide using a neural network and GPUs\n\n\nThe shift: from “encode what we know” to “learn from what we have.”"
  },
  {
    "objectID": "slides/01-foundations.html#what-searle-and-dreyfus-might-say-now",
    "href": "slides/01-foundations.html#what-searle-and-dreyfus-might-say-now",
    "title": "AI Tools for Data Science and Statistics",
    "section": "What Searle and Dreyfus Might Say Now",
    "text": "What Searle and Dreyfus Might Say Now\nInterpretive lens (not an empirical result): what their critiques would predict about LLM behavior.\n\n\n\n\n\n\n\n\nGOFAI Critique\nLLM Version\n\n\n\n\nCan’t handle ambiguity\nHandles surface ambiguity; fails on deep reasoning\n\n\nBrittle when rules don’t cover the case\nFails catastrophically on buggy or novel contexts\n\n\nSyntax without semantics (Searle)\nGenerates fluent text without “understanding” it"
  },
  {
    "objectID": "slides/01-foundations.html#what-searle-and-dreyfus-might-say-now-cont.",
    "href": "slides/01-foundations.html#what-searle-and-dreyfus-might-say-now-cont.",
    "title": "AI Tools for Data Science and Statistics",
    "section": "What Searle and Dreyfus Might Say Now (cont.)",
    "text": "What Searle and Dreyfus Might Say Now (cont.)\n\n\n\n\n\n\n\n\nGOFAI Critique\nLLM Version\n\n\n\n\nNo embodied knowledge (Dreyfus)\nNo experience, no clinical intuition, no common sense grounding\n\n\nCan’t plan or reason (both)\nLLMs fail on classical planning tasks (Valmeekam et al. 2023; Kambhampati et al. 2024)"
  },
  {
    "objectID": "slides/01-foundations.html#the-question-hasnt-changed",
    "href": "slides/01-foundations.html#the-question-hasnt-changed",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Question Hasn’t Changed",
    "text": "The Question Hasn’t Changed\nIs sophisticated pattern matching the same as understanding?\n\nFor this course, we won’t try to settle that question philosophically. We’ll ask a narrower one:\nWhen should you trust LLM outputs in scientific work, and what failure modes should you expect?"
  },
  {
    "objectID": "slides/01-foundations.html#the-hype-cycle-is-not-new",
    "href": "slides/01-foundations.html#the-hype-cycle-is-not-new",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Hype Cycle Is Not New",
    "text": "The Hype Cycle Is Not New\n\n\n\n\n\n\n\n\n\nEra\nThe Promise\nWhat Actually Happened\n\n\n\n\n1960s\n“AI in a generation”\nTwo AI winters\n\n\n1980s\nExpert systems will replace professionals\nToo brittle; collapsed\n\n\n2010s\nSelf-driving cars by 2020\nStill not solved in 2026\n\n\n2020s\nAGI is imminent; all jobs automated\n???\n\n\n\n\n\nWe are somewhere on this curve. Knowing the history helps you locate where."
  },
  {
    "objectID": "slides/01-foundations.html#section-1",
    "href": "slides/01-foundations.html#section-1",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "What is a Large Language Model?"
  },
  {
    "objectID": "slides/01-foundations.html#the-simplest-explanation",
    "href": "slides/01-foundations.html#the-simplest-explanation",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Simplest Explanation",
    "text": "The Simplest Explanation\nAn LLM is a function trained to predict the next token.\n\nGiven a sequence of tokens, it outputs a probability distribution over what comes next.\n\n\nThis training objective is why it is often called “sophisticated autocomplete.” At scale, the same objective can produce behavior that looks like reasoning on some tasks, but it is still fragile."
  },
  {
    "objectID": "slides/01-foundations.html#tokens-and-embeddings",
    "href": "slides/01-foundations.html#tokens-and-embeddings",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Tokens and Embeddings",
    "text": "Tokens and Embeddings\n\nText is split into tokens (subword units, not whole words)\n\n“biostatistics” \\(\\rightarrow\\) “bio” + “stat” + “istics”\n\nEach token is mapped to a vector (a list of numbers)\nThese vectors capture meaning through position in high-dimensional space\n\n\nPCA analogy: Just as PCA finds axes of variation in your data, embeddings find axes of meaning in language. Words with similar meanings cluster together."
  },
  {
    "objectID": "slides/01-foundations.html#attention",
    "href": "slides/01-foundations.html#attention",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Attention",
    "text": "Attention\nCore innovation (Vaswani et al. 2017):\nWords look at other words to determine their meaning.\n\n\n“They walked along the bank of the river.”\n\n\n“She deposited the check at the bank.”\n\nSame word, different meaning. Attention resolves this by weighing context."
  },
  {
    "objectID": "slides/01-foundations.html#attention-an-analogy",
    "href": "slides/01-foundations.html#attention-an-analogy",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Attention: An Analogy",
    "text": "Attention: An Analogy\nWittgenstein’s “meaning is use” [Philosophical Investigations; (1953)]:\nA word’s meaning isn’t fixed; it comes from how it’s used in context.\n\nAttention mechanisms capture something structurally similar."
  },
  {
    "objectID": "slides/01-foundations.html#how-do-llms-learn-12",
    "href": "slides/01-foundations.html#how-do-llms-learn-12",
    "title": "AI Tools for Data Science and Statistics",
    "section": "How Do LLMs “Learn?” (1/2)",
    "text": "How Do LLMs “Learn?” (1/2)\nThree stages. Most capability comes from stage 1.\n\nPre-training: Predict the next token across massive text corpora (internet, books, code, curated datasets). The model learns statistical patterns of language and knowledge. It stores weights and distributed patterns, not documents."
  },
  {
    "objectID": "slides/01-foundations.html#how-do-llms-learn-22",
    "href": "slides/01-foundations.html#how-do-llms-learn-22",
    "title": "AI Tools for Data Science and Statistics",
    "section": "How Do LLMs “Learn?” (2/2)",
    "text": "How Do LLMs “Learn?” (2/2)\nStages 2-3 mostly shape behavior, not knowledge.\n\nInstruction tuning (Supervised Fine-Tuning): Fine-tune on curated prompt-response examples so the model follows instructions and behaves like an assistant.\nPreference optimization (Reinforcement Learning from Human Feedback, or RLHF): Humans compare pairs of model outputs and pick the better one. A reward model learns these preferences. The LLM is then fine-tuned with reinforcement learning to maximize that reward signal, producing responses humans rate as more helpful and aligned."
  },
  {
    "objectID": "slides/01-foundations.html#key-concepts-for-users",
    "href": "slides/01-foundations.html#key-concepts-for-users",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Key Concepts for Users",
    "text": "Key Concepts for Users\n\n\n\n\n\n\n\n\nConcept\nWhat It Means\n\n\n\n\nContext window\nHow much text the model can “see” at once (200K-400K tokens for frontier models)\n\n\nTemperature\nControls randomness. Low = more deterministic, high = less deterministic\n\n\nStochasticity\nSame prompt can give different answers each time\n\n\nNo memory\nEach conversation starts from scratch (unless you provide context)"
  },
  {
    "objectID": "slides/01-foundations.html#what-an-llm-is-not",
    "href": "slides/01-foundations.html#what-an-llm-is-not",
    "title": "AI Tools for Data Science and Statistics",
    "section": "What an LLM Is Not",
    "text": "What an LLM Is Not\n\nNot a database. It doesn’t “look up” answers. It generates them.\nNot a search engine. It doesn’t retrieve documents (unless given tools to do so).\nNot deterministic. Same input \\(\\neq\\) same output.\n\n\n\nNot “thinking” the way you do. It produces both correct reasoning and convincing nonsense, with no reliable way to tell the difference from the inside.\nNot an expert. It can sound authoritative while being completely wrong."
  },
  {
    "objectID": "slides/01-foundations.html#section-2",
    "href": "slides/01-foundations.html#section-2",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "A Brief History of LLMs (2017-2026)"
  },
  {
    "objectID": "slides/01-foundations.html#the-foundation-2017-2020",
    "href": "slides/01-foundations.html#the-foundation-2017-2020",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Foundation (2017-2020)",
    "text": "The Foundation (2017-2020)\n\n2017: “Attention Is All You Need” introduces the Transformer architecture\n2018: GPT-1 (117M parameters). Language modeling as pre-training\n2019: GPT-2 (1.5B). “Too dangerous to release”\n2020: GPT-3 (175B). Can perform tasks from just a few examples in the prompt"
  },
  {
    "objectID": "slides/01-foundations.html#where-we-were-on-math",
    "href": "slides/01-foundations.html#where-we-were-on-math",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Where We Were on Math",
    "text": "Where We Were on Math\nThe best models scored 3–7% on the MATH benchmark:\n\nGPT-2 1.5B reached 7% after pretraining on a math corpus and fine-tuning\nGPT-3 175B managed only 5% few-shot\n\n(Hendrycks et al. 2021)\n\nKeep these numbers in mind."
  },
  {
    "objectID": "slides/01-foundations.html#the-scaling-era-2021-2022",
    "href": "slides/01-foundations.html#the-scaling-era-2021-2022",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Scaling Era (2021-2022)",
    "text": "The Scaling Era (2021-2022)\n\n2021: Codex (GPT-3 fine-tuned on code) powers GitHub Copilot\n2022: ChatGPT launches (November). AI enters mainstream consciousness"
  },
  {
    "objectID": "slides/01-foundations.html#the-leap-2023",
    "href": "slides/01-foundations.html#the-leap-2023",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Leap (2023)",
    "text": "The Leap (2023)\n\nGPT-4 arrives. A step change in capability:\n\n80% on HumanEval (code generation, 0-shot) (Ni et al. 2024)\n43% on MATH (competition math) (Fu et al. 2023)\n86% on MMLU (academic knowledge) (Fu et al. 2023)\n\nOpen-source accelerates: LLaMA, Code Llama, StarCoder, Mistral"
  },
  {
    "objectID": "slides/01-foundations.html#the-reasoning-era-2024-2025",
    "href": "slides/01-foundations.html#the-reasoning-era-2024-2025",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Reasoning Era (2024-2025)",
    "text": "The Reasoning Era (2024-2025)\n\n2024: OpenAI releases o1. A “reasoning model” that thinks step-by-step\n2025: o3-mini, DeepSeek-R1 push reasoning further\n\no3-mini: 13% on Humanity’s Last Exam (vs. GPT-4o at 3%) (Phan et al. 2025)\n\n\n\nKey insight: Reasoning models use the same underlying architecture."
  },
  {
    "objectID": "slides/01-foundations.html#what-makes-reasoning-models-different",
    "href": "slides/01-foundations.html#what-makes-reasoning-models-different",
    "title": "AI Tools for Data Science and Statistics",
    "section": "What Makes Reasoning Models Different?",
    "text": "What Makes Reasoning Models Different?\nThe difference is how they spend compute:\n\nA standard model answers immediately. One pass through the network.\nA reasoning model “thinks out loud” before answering (you see this as a loading delay)\nIt spends more tokens per question, trading speed and cost for accuracy\n\n\no3-mini scored ~5x higher than GPT-4o on Humanity’s Last Exam (Phan et al. 2025)"
  },
  {
    "objectID": "slides/01-foundations.html#what-hasnt-changed-yet",
    "href": "slides/01-foundations.html#what-hasnt-changed-yet",
    "title": "AI Tools for Data Science and Statistics",
    "section": "What Hasn’t Changed Yet",
    "text": "What Hasn’t Changed Yet\nEven with better models, these problems are probably not solved:\n\nBuggy context catastrophe. Models still don’t detect upstream bugs (Dinh et al. 2023)\nPoor calibration. Models are often badly miscalibrated on hard questions (Phan et al. 2025)\nBenchmark contamination. Static benchmark scores can be substantially inflated under contamination (Chen, Pusarla, and Ray 2025)"
  },
  {
    "objectID": "slides/01-foundations.html#today-february-2026",
    "href": "slides/01-foundations.html#today-february-2026",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Today (February 2026)",
    "text": "Today (February 2026)\nThis snapshot is here to show churn, not to anchor on details. Specs and pricing change quickly; the broader patterns are what matter.\n\n\n\n\n\nClaude Opus 4.6\nGPT-5.3-Codex\n\n\n\n\nDeveloper\nAnthropic\nOpenAI\n\n\nContext window\n200K tokens (1M beta)\n400K tokens\n\n\nPricing (input)\n$5 / 1M tokens\n$1.75 / 1M tokens\n\n\nArchitecture\nReasoning model\nReasoning (code-optimized)\n\n\n\n\nPricing and specs from vendor documentation as of Feb 2026; subject to change."
  },
  {
    "objectID": "slides/01-foundations.html#the-speed-of-change",
    "href": "slides/01-foundations.html#the-speed-of-change",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Speed of Change",
    "text": "The Speed of Change\nMATH benchmark progression (competition-level mathematics):\n\n\n\n\n\n\n\n\n\n\nYear\nModel\nMATH Score\nConditions\n\n\n\n\n2021\nGPT-2 1.5B\n7%\nMath-pretrained + fine-tuned (Hendrycks et al. 2021)\n\n\n2021\nGPT-3 175B\n5%\nFew-shot (Hendrycks et al. 2021)\n\n\n2023\nGPT-4\n43%\n(Fu et al. 2023)\n\n\n2025\no3-mini\n~87%\nVendor-reported\n\n\n2026\nGPT-5.3-Codex\n~96%\nVendor-reported"
  },
  {
    "objectID": "slides/01-foundations.html#section-3",
    "href": "slides/01-foundations.html#section-3",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "What Does the Research Say?"
  },
  {
    "objectID": "slides/01-foundations.html#a-simple-map-of-failure-modes",
    "href": "slides/01-foundations.html#a-simple-map-of-failure-modes",
    "title": "AI Tools for Data Science and Statistics",
    "section": "A Simple Map of Failure Modes",
    "text": "A Simple Map of Failure Modes\nFour patterns show up repeatedly in the literature:\n\nBrittleness to context: small upstream errors can cascade\nOverconfidence: models sound certain when they are wrong\nContamination: headline benchmark scores can be inflated\nDistribution shift: real-world tasks differ from curated benchmarks\n\n\nThe point is not to be pessimistic, but to develop good scientific instincts about when the tool is likely to help."
  },
  {
    "objectID": "slides/01-foundations.html#where-llms-succeed",
    "href": "slides/01-foundations.html#where-llms-succeed",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Where LLMs Succeed",
    "text": "Where LLMs Succeed\n\n\n\n\n\n\n\n\n\nTask\nPerformance\nSource\n\n\n\n\nCode generation (HumanEval, 0-shot)\n80% (GPT-4)\n(Ni et al. 2024)\n\n\nGrade-school math (GSM8k)\n92%\n(Fu et al. 2023)\n\n\nAcademic knowledge (MMLU)\n86%\n(Fu et al. 2023)\n\n\nUndergraduate statistics (StatEval, Statistics subset)\n87% (GPT-5)\n(Lu et al. 2025)"
  },
  {
    "objectID": "slides/01-foundations.html#where-llms-succeed-less",
    "href": "slides/01-foundations.html#where-llms-succeed-less",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Where LLMs Succeed… Less",
    "text": "Where LLMs Succeed… Less\n\n\n\n\nTask\nPerformance\nSource\n\n\n\n\nPython problems (MBPP, few-shot)\n74% (GPT-4)\n(Ni et al. 2024)\n\n\nData science code (DS-1000)\n43% (Codex)\n(Lai et al. 2023)\n\n\n\n\n\nPattern: LLMs excel at clear patterns and well-represented training data. Note the gap between generic coding benchmarks and data-science-specific ones."
  },
  {
    "objectID": "slides/01-foundations.html#where-llms-struggle",
    "href": "slides/01-foundations.html#where-llms-struggle",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Where LLMs Struggle",
    "text": "Where LLMs Struggle\n\n\n\n\n\n\n\n\n\nTask\nPerformance\nSource\n\n\n\n\nResearch-level statistics\n~58% (best closed model)\n(Lu et al. 2025)\n\n\nExpert-level questions (HLE)\n3-13%\n(Phan et al. 2025)\n\n\nReal-world code translation\n8% (GPT-4)\n(Pan et al. 2024)\n\n\nCode completion with buggy context\n1-3%\n(Dinh et al. 2023)\n\n\nAgent self-repair\n4%\n(Islam et al. 2026)"
  },
  {
    "objectID": "slides/01-foundations.html#the-pattern",
    "href": "slides/01-foundations.html#the-pattern",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Pattern",
    "text": "The Pattern\nLLMs fail when tasks require genuine reasoning, when context is messy, or when there’s no clear template to follow."
  },
  {
    "objectID": "slides/01-foundations.html#the-training-data-problem",
    "href": "slides/01-foundations.html#the-training-data-problem",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Training Data Problem",
    "text": "The Training Data Problem\nLLMs learn from what they’ve seen. Performance drops sharply outside that distribution.\n\nHeavily represented: Python, JavaScript, standard ML workflows, common statistical methods\nUnderrepresented: Niche R packages, domain-specific pipelines, novel methods, non-English documentation\nNot represented at all: Your unpublished data, your institution’s internal tools, anything after the training cutoff\n\n\nIf your task looks like something on Stack Overflow, the model will do well. If it doesn’t, expect more errors."
  },
  {
    "objectID": "slides/01-foundations.html#the-buggy-context-problem",
    "href": "slides/01-foundations.html#the-buggy-context-problem",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Buggy Context Problem",
    "text": "The Buggy Context Problem\nA single bug in the surrounding code is catastrophic:\n“Buggy-HumanEval” is a code-completion variant of HumanEval where the model must finish a program given a prefix.\n\nReference prefix: surrounding code is correct\nBuggy prefix: the prefix contains a small upstream bug; the model continues anyway\n\nPass@1 = percent of problems solved with one sampled completion."
  },
  {
    "objectID": "slides/01-foundations.html#buggy-context-results",
    "href": "slides/01-foundations.html#buggy-context-results",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Buggy Context: Results",
    "text": "Buggy Context: Results\n\n\n\n\nModel\nReference Prefix\nBuggy Prefix\n\n\n\n\nCodeGen-2B\n55%\n3%\n\n\nInCoder-6B\n51%\n1%\n\n\nCodeGen-350M\n43%\n0.7%\n\n\nInCoder-1B\n41%\n0.5%\n\n\n\n\nIn their analysis, the model fails to react to the potential bug in 90% of instances. (Dinh et al. 2023)\n\nTakeaway: Context quality &gt; model quality."
  },
  {
    "objectID": "slides/01-foundations.html#overconfidence-the-numbers",
    "href": "slides/01-foundations.html#overconfidence-the-numbers",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Overconfidence: The Numbers",
    "text": "Overconfidence: The Numbers\nLLMs are wrong frequently and act certain almost always:\n\nCalibration error on expert questions: 70-89% RMS error across all models (Phan et al. 2025)\nOn MATH, confidence is near 100% regardless of correctness (Hendrycks et al. 2021)\nBugs are common in practice; one empirical study catalogs 10 recurring bug patterns from 333 collected bugs in LLM-generated code (Tambon et al. 2024)"
  },
  {
    "objectID": "slides/01-foundations.html#overconfidence-security",
    "href": "slides/01-foundations.html#overconfidence-security",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Overconfidence: Security",
    "text": "Overconfidence: Security\n\n~40% of Copilot scenarios produced at least one vulnerable code suggestion (Pearce et al. 2022)\n30% of snippets contain security vulnerabilities across 38 CWE categories (Zhou et al. 2024)\n\n\nIt gets worse: Users with AI assistants wrote significantly less secure code and were more confident in it (Perry et al. 2023)."
  },
  {
    "objectID": "slides/01-foundations.html#benchmarks-lie-contamination",
    "href": "slides/01-foundations.html#benchmarks-lie-contamination",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Benchmarks Lie: Contamination",
    "text": "Benchmarks Lie: Contamination\nUnder data contamination, static benchmark scores can jump ~4x (e.g., 22% \\(\\rightarrow\\) 82% on Pass@K), creating a false sense of reasoning capability. (Chen, Pusarla, and Ray 2025)"
  },
  {
    "objectID": "slides/01-foundations.html#benchmarks-lie-too-easy-too-artificial",
    "href": "slides/01-foundations.html#benchmarks-lie-too-easy-too-artificial",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Benchmarks Lie: Too Easy, Too Artificial",
    "text": "Benchmarks Lie: Too Easy, Too Artificial\nEven uncontaminated benchmarks are too easy: Code that passes HumanEval often fails on more rigorous test suites. The benchmark rewards surface correctness, not robustness. (Liu et al. 2023)\n\nBenchmark vs. real-world gap:\nGPT-4 code translation: 47% overall, 8% on real-world projects. (Pan et al. 2024)\n\n\nBe skeptical of headline numbers."
  },
  {
    "objectID": "slides/01-foundations.html#the-models-have-changed",
    "href": "slides/01-foundations.html#the-models-have-changed",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Models Have Changed",
    "text": "The Models Have Changed\nFrontier comparison. Literature-era best vs. February 2026:\n\n\n\n\n\n\n\n\n\n\nBenchmark\nBest in Papers\n2026 Frontier\nChange\n\n\n\n\nMATH (competition)\n43% (GPT-4) (Fu et al. 2023)\n~96% (GPT-5.3-Codex, reported)\n+53 pts\n\n\nHLE (expert questions)\n13% (o3-mini) (Phan et al. 2025)\n53% (Opus 4.6, with tools, reported)\n+40 pts\n\n\nHumanEval (code gen)\n80% (GPT-4) (Ni et al. 2024)\n~95% (Opus 4.6, reported)\n+15 pts\n\n\nMMLU (academic)\n86% (GPT-4) (Fu et al. 2023)\n~93% (GPT-5.3-Codex, reported)\n+7 pts\n\n\nSWE-bench Verified (Jimenez et al. 2024)\n–\n81% (Opus 4.6, reported)\n–\n\n\n\n\nEnormous improvement in 2-3 years. But consider Goodhart’s law: when a measure becomes a target, is it still a good measure?"
  },
  {
    "objectID": "slides/01-foundations.html#summary-what-the-research-shows",
    "href": "slides/01-foundations.html#summary-what-the-research-shows",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Summary: What the Research Shows",
    "text": "Summary: What the Research Shows\n\n\nLLMs do best within their training data. They do well at things they “know about.”\n\n\n\n\nContext quality is important. Upstream bugs can propagate; LLMs excel at narrow tasks.\n\n\n\n\nValidate everything. Research shows LLMs create bugs, security vulnerabilities."
  },
  {
    "objectID": "slides/01-foundations.html#summary-what-this-means-for-you-12",
    "href": "slides/01-foundations.html#summary-what-this-means-for-you-12",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Summary: What This Means for You (1/2)",
    "text": "Summary: What This Means for You (1/2)\n\nPrompting matters. How you ask changes what you get. For some reasoning tasks, chain-of-thought prompting yields gains ranging from ~0 to 40+ percentage points depending on the task/model. (Wei et al. 2022)\n\nFew-shot examples (show what good output looks like)\nChain-of-thought prompting (ask it to reason step-by-step) (Wei et al. 2022)\nExplicit framing (role, format constraints, and failure modes to avoid)"
  },
  {
    "objectID": "slides/01-foundations.html#summary-what-this-means-for-you-22",
    "href": "slides/01-foundations.html#summary-what-this-means-for-you-22",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Summary: What This Means for You (2/2)",
    "text": "Summary: What This Means for You (2/2)\n\nModel selection matters. The gap between models is enormous (3% to 53% on expert questions; the high end is with tools).\n\n\n\nHuman expertise is essential. Developers who know code is AI-generated catch 13 percentage points more bugs (Tang et al. 2024)."
  },
  {
    "objectID": "slides/01-foundations.html#section-4",
    "href": "slides/01-foundations.html#section-4",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Hype, Skepticism, and Nuance"
  },
  {
    "objectID": "slides/01-foundations.html#the-bull-case",
    "href": "slides/01-foundations.html#the-bull-case",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Bull Case",
    "text": "The Bull Case\nThe optimists have real evidence:\n\nMATH benchmark: ~7% \\(\\rightarrow\\) ~96% in a few years\nSWE-bench Verified: 81% (Opus 4.6 fixing real GitHub issues)\nCosts dropped 10-20x in 2 years (GPT-4 at $30/M tokens \\(\\rightarrow\\) GPT-5.3-Codex at $1.75/M)\nMassive corporate investment; consultancies projecting sweeping automation\n\nMost of these are vendor- or leaderboard-reported rather than peer-reviewed results."
  },
  {
    "objectID": "slides/01-foundations.html#the-bear-case",
    "href": "slides/01-foundations.html#the-bear-case",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Bear Case",
    "text": "The Bear Case\nThe skeptics also have real evidence:\n\nBenchmark contamination inflates scores ~3x (Chen, Pusarla, and Ray 2025)\nLLM-generated code contains bugs; ~40% of Copilot scenarios produced at least one vulnerable suggestion (Tambon et al. 2024; Pearce et al. 2022)\nBuggy context can drop pass@1 from ~41-55% to ~0.5-3% (Dinh et al. 2023)\n\n\n\nReal-world code translation: 8% (vs. 47% overall) (Pan et al. 2024)\nBest agent self-repair: 4% accuracy (Islam et al. 2026)"
  },
  {
    "objectID": "slides/01-foundations.html#summary",
    "href": "slides/01-foundations.html#summary",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Summary",
    "text": "Summary\n\n\nGood at:\n\nBoilerplate\nSyntax and API lookup\nStandard patterns\nExplaining code\nBrainstorming approaches\n\n\nPoor at:\n\nMessy or buggy contexts\nNovel algorithms\nKnowing when it’s wrong"
  },
  {
    "objectID": "slides/01-foundations.html#biostatistics",
    "href": "slides/01-foundations.html#biostatistics",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Biostatistics",
    "text": "Biostatistics\nIn my experience, AI tools can:\n\nWrite code for data cleaning, visualization, standard analyses (with supervision)\nGenerate boilerplate for reports and documentation\nTranslate between programming languages\nExplain unfamiliar code or statistical concepts"
  },
  {
    "objectID": "slides/01-foundations.html#biostatistics-what-ai-can-do-prudently",
    "href": "slides/01-foundations.html#biostatistics-what-ai-can-do-prudently",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Biostatistics: What AI Can Do… Prudently?",
    "text": "Biostatistics: What AI Can Do… Prudently?\nAI can attempt all of the following. The question is whether it does them well enough to trust:\n\nDesign a study\nInterpret clinical context\nMake judgment calls about model assumptions\nNavigate IRB requirements and data use agreements\nExplain findings to a collaborator\n\n\nAI cannot take responsibility. The analyst does."
  },
  {
    "objectID": "slides/01-foundations.html#discussion-prompts-12",
    "href": "slides/01-foundations.html#discussion-prompts-12",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Discussion Prompts (1/2)",
    "text": "Discussion Prompts (1/2)\n\nWould you trust AI-generated code in a clinical trial analysis? Under what conditions?\n\n\n\nIf an LLM writes 70% of a methods section, who is the author?\n\n\n\n\nShould researchers be required to disclose AI tool use? At what level of detail? (see Hosseini, Resnik, and Holmes 2023; Liao and Vaughan 2024)"
  },
  {
    "objectID": "slides/01-foundations.html#discussion-prompts-22",
    "href": "slides/01-foundations.html#discussion-prompts-22",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Discussion Prompts (2/2)",
    "text": "Discussion Prompts (2/2)\n\nLLMs are trained on public code and text, often without consent. Is this ethical? Does it matter if the model is open-source vs. proprietary? (see Weidinger et al. 2022)\n\n\n\nAs a student, what is the appropriate way to use AI Tools?"
  },
  {
    "objectID": "slides/01-foundations.html#demo-ai-generated-eda",
    "href": "slides/01-foundations.html#demo-ai-generated-eda",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Demo: AI-Generated EDA",
    "text": "Demo: AI-Generated EDA\nBefore we wrap up, let’s look at a real example: a complete exploratory data analysis notebook written entirely by an LLM.\n\nPalmer Penguins data\nFive different model-harnesses\nLets run the same prompt, examine results.\n\nThe notebook stems (all the same to start) are in the course repo: demos/1-eda"
  },
  {
    "objectID": "slides/01-foundations.html#whats-next",
    "href": "slides/01-foundations.html#whats-next",
    "title": "AI Tools for Data Science and Statistics",
    "section": "What’s Next",
    "text": "What’s Next\nCourse project: Brainstorm a task that feels beyond your current ability. Email your topic to ewestlund@jhu.edu before Session 2.\n\n\n\n\nChen, Simin, Pranav Pusarla, and Baishakhi Ray. 2025. “DyCodeEval: Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination.” In Proceedings of the 42nd International Conference on Machine Learning (ICML). Vol. 267. PMLR. https://arxiv.org/abs/2503.04149.\n\n\nDinh, Tuan, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, and George Karypis. 2023. “Large Language Models of Code Fail at Completing Code with Potential Bugs.” In Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS). https://arxiv.org/abs/2306.03438.\n\n\nDreyfus, Hubert L. 1972. What Computers Can’t Do: A Critique of Artificial Reason. New York: Harper & Row.\n\n\n———. 1992. What Computers Still Can’t Do: A Critique of Artificial Reason. Cambridge, MA: MIT Press.\n\n\nFu, Yao, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. 2023. “Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models’ Reasoning Performance.” https://arxiv.org/abs/2305.17306.\n\n\nHaugeland, John. 1985. Artificial Intelligence: The Very Idea. Cambridge, MA: MIT Press.\n\n\nHendrycks, Dan, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. “Measuring Mathematical Problem Solving with the MATH Dataset.” In Proceedings of the 35th Conference on Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track. https://arxiv.org/abs/2103.03874.\n\n\nHosseini, Mohammad, David B. Resnik, and Kristi Holmes. 2023. “The Ethics of Disclosing the Use of Artificial Intelligence Tools in Writing Scholarly Manuscripts.” Research Ethics 19 (4): 449–65. https://doi.org/10.1177/17470161231180449.\n\n\nIslam, Niful, Ragib Shahariar Ayon, Deepak George Thomas, Shibbir Ahmed, and Mohammad Wardat. 2026. “When Agents Fail: A Comprehensive Study of Bugs in LLM Agents with Automated Labeling.” https://arxiv.org/abs/2601.15232.\n\n\nJimenez, Carlos E., John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. 2024. “SWE-Bench: Can Language Models Resolve Real-World GitHub Issues?” In International Conference on Learning Representations (ICLR 2024). https://arxiv.org/abs/2310.06770.\n\n\nKambhampati, Subbarao, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Siddarth, Anil Garg, and Raghav Mangla. 2024. “Position: LLMs Can’t Plan, but Can Help Planning in LLM-Modulo Frameworks.” In Proceedings of the 41st International Conference on Machine Learning (ICML 2024), 235:22895–907. PMLR. https://arxiv.org/abs/2402.01817.\n\n\nLai, Yuhang, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2023. “DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.” In Proceedings of the 40th International Conference on Machine Learning (ICML 2023), 202:18319–45. PMLR. https://arxiv.org/abs/2211.11501.\n\n\nLiao, Q. Vera, and Jennifer Wortman Vaughan. 2024. “AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap.” Harvard Data Science Review, no. Special Issue 5. https://doi.org/10.1162/99608f92.8036d03b.\n\n\nLiu, Jiawei, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. “Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation.” In Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS). https://arxiv.org/abs/2305.01210.\n\n\nLu, Yuchen, Run Yang, Yichen Zhang, Shuguang Yu, Runpeng Dai, Ziwei Wang, Jiayi Xiang, et al. 2025. “StatEval: A Comprehensive Benchmark for Large Language Models in Statistics.” https://arxiv.org/abs/2510.09517.\n\n\nNi, Ansong, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, et al. 2024. “L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models.” https://arxiv.org/abs/2309.17446.\n\n\nPan, Rangeet, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pougeum Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. 2024. “Lost in Translation: A Study of Bugs Introduced by Large Language Models While Translating Code.” In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering (ICSE ’24). Lisbon, Portugal. https://doi.org/10.1145/3597503.3639226.\n\n\nPearce, Hammond, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. 2022. “Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions.” In IEEE Symposium on Security and Privacy (s&p 2022), 754–68. https://doi.org/10.1109/SP46214.2022.9833571.\n\n\nPerry, Neil, Megha Srivastava, Deepak Kumar, and Dan Boneh. 2023. “Do Users Write More Insecure Code with AI Assistants?” In ACM Conference on Computer and Communications Security (CCS 2023). https://doi.org/10.1145/3576915.3623157.\n\n\nPhan, Long, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, et al. 2025. “Humanity’s Last Exam.” Nature. https://doi.org/10.1038/s41586-025-09962-4.\n\n\nSearle, John R. 1980. “Minds, Brains, and Programs.” Behavioral and Brain Sciences 3 (3): 417–24. https://doi.org/10.1017/S0140525X00005756.\n\n\nTambon, Florian, Arghavan Moradi Dakhel, Amin Nikanjam, Foutse Khomh, Michel C. Desmarais, and Giuliano Antoniol. 2024. “Bugs in Large Language Models Generated Code: An Empirical Study.” https://arxiv.org/abs/2403.08937.\n\n\nTang, Ningzhi, Meng Chen, Zheng Ning, Aakash Bansal, Yu Huang, Collin McMillan, and Toby Jia-Jun Li. 2024. “A Study on Developer Behaviors for Validating and Repairing LLM-Generated Code Using Eye Tracking and IDE Actions.” https://arxiv.org/abs/2405.16081.\n\n\nValmeekam, Karthik, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. 2023. “On the Planning Abilities of Large Language Models — a Critical Investigation.” In Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). https://arxiv.org/abs/2305.15771.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Advances in Neural Information Processing Systems 30 (NeurIPS).\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V. Le, and Denny Zhou. 2022. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” In Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS 2022). https://arxiv.org/abs/2201.11903.\n\n\nWeidinger, Laura, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, et al. 2022. “Taxonomy of Risks Posed by Language Models.” In ACM Conference on Fairness, Accountability, and Transparency (FAccT 2022). https://doi.org/10.1145/3531146.3533088.\n\n\nWittgenstein, Ludwig. 1953. Philosophical Investigations. Oxford: Blackwell.\n\n\nZhou, Xiyu, Peng Liang, Beiqi Zhang, Zengyang Li, Aakash Ahmad, Mojtaba Shahin, and Muhammad Waseem. 2024. “Exploring the Problems, Their Causes and Solutions of AI Pair Programming: A Study on GitHub and Stack Overflow.” Journal of Systems and Software 219: 112204. https://doi.org/10.1016/j.jss.2024.112204."
  },
  {
    "objectID": "demos/2-tools-introduction/summary.html",
    "href": "demos/2-tools-introduction/summary.html",
    "title": "Comparing AI Tool Outputs: Maternal Health Dataset Analysis",
    "section": "",
    "text": "NoteAI-Generated Document\n\n\n\nThis summary was generated by Claude (Opus 4.6) from all 22 notebooks in this experiment. It was then double-checked and streamlined with GPT-5.3. Content was verified against source notebooks but should still be read with appropriate skepticism."
  },
  {
    "objectID": "demos/2-tools-introduction/summary.html#overview",
    "href": "demos/2-tools-introduction/summary.html#overview",
    "title": "Comparing AI Tool Outputs: Maternal Health Dataset Analysis",
    "section": "Overview",
    "text": "Overview\nThis document compares how AI coding assistants approached the same maternal health dataset task. Each CLI tool/harness + model + prompt combination produced one notebook; all 22 are linked in Rendered Notebooks.\n\nExercise Setup: Detailed vs Simple Prompts\nThis exercise tested how much prompt structure changes analysis quality when the dataset and environment are held constant.\n\nShared conditions: All runs used the same synthetic dataset (simulated_maternal_health_data.csv, n = 50,000), same project files, and same requirement to produce a complete Quarto analysis notebook.\nDetailed prompt goal: Evaluate whether models can execute a clearly specified statistical workflow. The prompt explicitly set the outcome (comorbidity_count), requested relationship-focused visualizations (postnatal care, insurance, race/ethnicity, distance), and asked for count-model reasoning (Poisson vs negative binomial, diagnostics, interpretation).\nSimple prompt goal: Evaluate how models choose the analysis when guidance is minimal. The prompt asked models to explore the data and build an appropriate model but did not specify the target variable, feature set, diagnostics depth, or modeling family.\nWhat this isolates: The gap between detailed and simple runs reflects how well each model+harness pair handles autonomy vs instruction-following: variable selection, model family choice, diagnostics, visualization quality, and reporting discipline.\n\n\n\nGround-Truth Data Generating Process\nImportant context: models were not given this DAG during notebook generation. So this is an ex-post validity check against the known simulation design.\nMost runs did what AI agents often do: optimize for patterns in available columns and prompt instructions. Very few reasoned explicitly about underlying causal mechanics. This is a reminder that AI tools cannot substitute for careful, scientific reasoning.\n\nSimplified DAG\n\n\n\n\n\n\n\n\n\n\nTrue outcome mechanism (simple-prompt target): received_comprehensive_postnatal_care is generated from latent drivers (personal_capacity, willingness_to_pay, provider_quality, provider_trust, risk_aversion, risk_profile) that are themselves functions of observed covariates.\nTrue comorbidity mechanism (detailed-prompt target): Comorbidities are generated through a directed chain (e.g., age/obesity -&gt; diabetes/hypertension -&gt; gestational hypertension/preeclampsia), and comorbidity_count is a derived sum, not a primitive causal node.\nWhere analyses most often drift from data generating process: (1) treating race/insurance effects as fully direct without acknowledging mediation via SES, trust, and access pathways; (2) ignoring provider-level clustering even though provider_quality is state/provider structured; (3) conditioning on postnatal care when modeling comorbidity count, i.e., controlling for a post-outcome variable (bad control) that can induce collider bias; (4) over-interpreting linear terms where data generating process components are nonlinear or latent.\nInterpretation standard used here: analyses are judged on robustness and transparency given limited prompt context; this DAG-informed section clarifies where estimates are most likely to be biased relative to simulation truth.\n\nThree prompt types were used:\n\nChat: A conversational prompt given to browser-based agents (ChatGPT and Claude)\nDetailed: A structured prompt specifying the outcome variable (comorbidity_count), required visualizations, and modeling approach (count regression)\nSimple: A minimal prompt asking the model to explore the data and build a model, with no guidance on outcome or method\n\nEach prompt was run across multiple CLI tool/harness + model combinations. CLI tools were Claude Code, Codex CLI, and OpenCode (plus browser agents for the chat prompt). Models were Opus 4.6, Sonnet 4.6, GPT-5.2, GPT-5.3, Gemini 3.1 Pro, Haiku 4.5, Kimi K2.5, and GLM-5.\n\n\n\nRuns With Failures\n\n\n\n\n\n\n\nRun\nIssue\n\n\n\n\nSimple – OpenCode + Kimi K2.5\nHallucinated data structure (used columns from Kaggle’s maternal health risk dataset rather than the actual data; code would error on render)\n\n\nDetailed – OpenCode + GLM-5\nProduced code but with issues: narrow predictor set, count-based rather than proportion-based visualizations, duplicate prompt text\n\n\nSimple – OpenCode + GLM-5\nProduced analysis but conclusion contains unfilled placeholder text (\"Key risk factors include [clinical variables]...\")"
  },
  {
    "objectID": "demos/2-tools-introduction/summary.html#harnesses-models",
    "href": "demos/2-tools-introduction/summary.html#harnesses-models",
    "title": "Comparing AI Tool Outputs: Maternal Health Dataset Analysis",
    "section": "Harnesses and Models",
    "text": "Harnesses and Models\n\nWhat Is a CLI Tool/Harness?\nA harness (here, essentially an agent CLI tool) is the software layer between a user and a language model. It controls context delivery (files, errors, project structure), available tools (editing, shell, web), and output format. The same model can produce different results across harnesses.\n\n\nCLI Tools/Harnesses Used\n\n\n\nHarness\nType\nDescription\nDocs\n\n\n\n\nClaude Code\nCLI agent\nAnthropic’s official CLI for Claude. Runs in the terminal, can read/write files, execute shell commands, and manage project context via CLAUDE.md files. Operates agentic loops where the model plans and executes multi-step tasks.\nclaude.ai/claude-code\n\n\nCodex CLI\nCLI agent\nOpenAI’s open-source CLI agent for Codex/GPT models. Similar to Claude Code: reads files, runs commands, edits code. Uses a sandboxed execution environment.\ngithub.com/openai/codex\n\n\nOpenCode\nCLI agent\nAn open-source, model-agnostic CLI agent that supports many providers (Anthropic, OpenAI, Google, Moonshot, Zhipu, etc.) through a unified interface. Provides file editing, shell access, and LSP integration.\nopencode.ai\n\n\nBrowser Agent\nChat interface\nStandard browser chat interfaces (ChatGPT at chat.openai.com, Claude at claude.ai). The user pastes prompt and data context manually; the model responds in a single turn without file system access.\n–\n\n\n\n\n\nModels Used\nAll prices are per 1M tokens (input/output) via OpenCode Zen (proxy/router). Browser pricing differs (typically subscription-based).\n\n\n\n\n\n\n\n\n\nModel\nProvider\nInput / Output\nDescription\n\n\n\n\nClaude Opus 4.6\nAnthropic\n$5.00 / $25.00\nAnthropic’s most capable model. Strong at complex reasoning, long-context analysis, and careful instruction following.\n\n\nClaude Sonnet 4.6\nAnthropic\n$3.00 / $15.00\nMid-tier Claude model. Balances capability with speed and cost. Often nearly as capable as Opus for straightforward coding tasks.\n\n\nClaude Haiku 4.5\nAnthropic\n$1.00 / $5.00\nAnthropic’s smallest and fastest model. Optimized for speed and low cost; less capable on complex multi-step tasks.\n\n\nGPT-5.3 Codex\nOpenAI\n$1.75 / $14.00\nOpenAI’s code-specialized model variant. Strong at code generation and tool use.\n\n\nGPT-5.2 / Codex\nOpenAI\n$1.75 / $14.00\nSlightly older OpenAI model. Available in both standard and Codex (code-tuned) variants.\n\n\nGemini 3.1 Pro\nGoogle\n$2.00 / $12.00\nGoogle’s mid-to-large model. Long context window (up to 1M tokens). Competitive on coding and reasoning benchmarks.\n\n\nKimi K2.5\nMoonshot AI\n$0.60 / $3.00\nChinese AI lab Moonshot’s model. Very low cost. Strong on Chinese-language tasks; English coding performance varies.\n\n\nGLM-5\nZhipu AI\n$1.00 / $3.20\nChinese AI lab Zhipu’s latest model (successor to ChatGLM series). Low cost; less established on English coding benchmarks.\n\n\nChatGPT\nOpenAI\nSubscription\nThe model behind the ChatGPT browser interface. Used here via the browser chat interface, not an API.\n\n\n\nNote: Prices shown reflect the context windows used here (under 200K tokens)."
  },
  {
    "objectID": "demos/2-tools-introduction/summary.html#rendered-notebooks",
    "href": "demos/2-tools-introduction/summary.html#rendered-notebooks",
    "title": "Comparing AI Tool Outputs: Maternal Health Dataset Analysis",
    "section": "Rendered Notebooks",
    "text": "Rendered Notebooks\nAll notebooks are available as rendered HTML. Each used the same synthetic maternal health dataset (simulated_maternal_health_data.csv, n = 50,000).\n\nChat PromptDetailed PromptSimple Prompt\n\n\n\nBrowser Agent + ChatGPT\nBrowser Agent + Claude\n\n\n\n\nClaude Code + Opus 4.6\nClaude Code + Sonnet 4.6\nCodex CLI + GPT-5.2\nCodex CLI + GPT-5.3\nOpenCode + Opus 4.6\nOpenCode + Haiku 4.5\nOpenCode + Gemini 3.1 Pro\nOpenCode + Codex GPT-5.3\nOpenCode + Kimi K2.5\nOpenCode + GLM-5 (not rendered; code has structural issues)\n\n\n\n\nClaude Code + Opus 4.6\nClaude Code + Sonnet 4.6\nCodex CLI + GPT-5.2\nCodex CLI + GPT-5.3\nOpenCode + Opus 4.6\nOpenCode + Haiku 4.5\nOpenCode + GLM-5\nOpenCode + Gemini 3.1 Pro\nOpenCode + Codex GPT-5.3\nOpenCode + Kimi K2.5 (hallucinated data; errors on render)"
  },
  {
    "objectID": "demos/2-tools-introduction/summary.html#feature-comparison",
    "href": "demos/2-tools-introduction/summary.html#feature-comparison",
    "title": "Comparing AI Tool Outputs: Maternal Health Dataset Analysis",
    "section": "At-a-Glance Feature Comparison",
    "text": "At-a-Glance Feature Comparison\nEach model was evaluated on the same set of features. A check (✓) indicates the notebook included that element; blank means it did not.\n\nChat & Detailed PromptSimple Prompt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nBrowser ChatGPT\nBrowser Claude\nCC Opus\nCC Sonnet\nCodex GPT-5.2\nCodex GPT-5.3\nOC Opus\nOC Haiku\nOC GLM-5\nOC Gemini\nOC Codex 5.3\nOC Kimi\n\n\n\n\njanitor::clean_names()\n✓\n✓\n\n\n\n\n\n\n✓\n✓\n\n✓\n\n\nOrdered factors\n✓\n✓\n✓\n✓\n\n✓\n✓\n\n\n\n\n✓\n\n\nScaled continuous vars\n\n\n\n\n\n\n\n✓\n\n\n\n\n\n\nFitted Poisson first\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n\n✓\n\n\n\nFitted NB\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n\nFormal overdispersion test\n✓\n✓\n\n\n\n\n✓\n✓\n\n\n\n\n\n\nTested zero-inflation\n\n✓\n\n\n\n✓\n\n\n\n\n\n\n\n\nRandom effects (GLMM)\n✓\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIC/BIC comparison\n\n✓\n✓\n✓\n\n✓\n✓\n✓\n\n\n\n✓\n\n\nDHARMa diagnostics\n✓\n✓\n\n\n\n\n\n\n\n\n\n\n\n\nResidual plots\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n✓\n\n\n\nRootogram/expected counts\n\n\n\n\n\n\n✓\n\n\n\n\n\n\n\nCoefficient/forest plot\n\n\n\n\n\n\n✓\n\n\n\n\n\n\n\nRelational visualizations\n✓\n✓\n✓\n✓\n\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n\nInfluence diagnostics\n\n\n\n\n\n\n\n\n\n\n\n✓\n\n\nExplicit limitations section\n\n\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n\n# Predictors\n6\n9\n7\n7\n6\n6\n9\n5\n4\n4\n8\n7\n\n\n# Visualizations\n5\n6\n5\n5\n4\n4\n10+\n7\n5\n3\n3\n6\n\n\n\n\nCC = Claude Code, OC = OpenCode, Codex = Codex CLI.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nCC Opus\nCC Sonnet\nOC Opus\nOC Haiku\nOC GLM-5\nOC Gemini\nOC Codex 5.3\nOC Kimi*\nCodex GPT-5.2\nCodex GPT-5.3\n\n\n\n\nChose logistic regression\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n✓\n✓\n\n\nUsed correct data columns\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n\n✓\n✓\n\n\nOrdered factors\n✓\n✓\n✓\n\n\n\n\n\n\n\n\n\nQuadratic/nonlinear terms\n\n\n\n\n\n\n✓\n\n✓\n\n\n\nComposite comorbidity var\n\n\n✓\n\n\n\n\n\n\n\n\n\nState as predictor\n\n\n\n✓\n\n\n✓\n\n✓\n\n\n\nTrain/test split\n\n\n\n✓\n\n\n✓\n\n\n\n\n\nConfusion matrix\n\n✓\n\n✓\n\n\n✓\n✓\n\n✓\n\n\nAUC/ROC\n\n\n\n✓\n✓\n\n✓\n\n\n\n\n\nResidual diagnostics\n\n✓\n✓\n\n\n\n\n\n\n\n\n\nPseudo-R²\n✓\n✓\n✓\n\n\n\n\n\n✓\n✓\n\n\nBrier score\n\n\n\n\n\n\n\n\n✓\n\n\n\nOR/forest plot\n✓\n\n✓\n\n\n\n\n\n\n\n\n\nRelational visualizations\n✓\n✓\n✓\n\n\n\n\n\n\n✓\n\n\nExplicit limitations\n\n✓\n✓\n\n\n\n\n\n✓\n✓\n\n\n# Predictors\n16\n16\n10\nAll\n15\n6\nAll+splines\n6\nAll+state\n16\n\n\n# Visualizations\n6\n5\n7\n1\n1\n0\n0\n6\n0\n4\n\n\n\n\n* Kimi K2.5 produced output but hallucinated the data structure (used Kaggle maternal health risk columns instead of the actual dataset; would error on render)."
  },
  {
    "objectID": "demos/2-tools-introduction/summary.html#chat-detailed-prompt-results",
    "href": "demos/2-tools-introduction/summary.html#chat-detailed-prompt-results",
    "title": "Comparing AI Tool Outputs: Maternal Health Dataset Analysis",
    "section": "Chat & Detailed Prompt Results",
    "text": "Chat & Detailed Prompt Results\nThe chat and detailed prompts both directed models to analyze comorbidity_count (the sum of 8 binary comorbidity indicators). All completed notebooks used negative binomial regression; most compared it against Poisson to address overdispersion.\n\nData Cleaning\nData-cleaning approaches varied considerably:\n\nColumn naming: Most used janitor::clean_names(); Claude Code and Codex variants used manual transformations or custom normalize_names() functions\nFactor ordering: Claude Code (Opus/Sonnet), OpenCode Opus, and OpenCode Kimi created ordered income/education factors with explicit levels; others used unordered factors or levels that sometimes mismatched data values\nBinary variables: Most converted comorbidities to integer (0/1); Gemini 3.1 Pro and Kimi K2.5 used logical (TRUE/FALSE); Haiku 4.5 used labeled factors (“No”/“Yes”); Browser Claude used robust case_when() logic for mixed string/logical inputs\nScaling: Haiku 4.5 was the only notebook that centered and scaled continuous predictors (age, distance) before modeling\nDerived variables: Browser Claude created log1p_distance and distance_10; OpenCode Codex GPT-5.3 created distance_to_provider_10 (distance/10)\n\n\n\nVisualizations\nAll notebooks were asked to visualize relationships among postnatal care receipt, insurance, race/ethnicity, and distance to provider. Quality varied:\n\nMost comprehensive visualization set: OpenCode + Opus 4.6 produced 10+ plots, including a coefficient forest plot and observed-vs-expected rootogram, formatted with kableExtra and patchwork\nUnique approaches: Codex CLI GPT-5.3 created a race/ethnicity-by-insurance heatmap; Browser ChatGPT compared observed vs Poisson-expected comorbidity distributions; Kimi K2.5 included an insurance-by-race/ethnicity interaction plot\nMost limited visualization scope: Codex CLI GPT-5.2 produced only univariate bar charts/histograms, with no cross-variable relationships\n\n\n\nModeling Decisions\nAll completed notebooks used negative binomial models, but the path differed:\n\nOverdispersion thresholds: Codex variants used explicit thresholds (1.2 or 1.5 Pearson/df); others used AIC comparisons or variance/mean ratios without explicit cutoffs\nTest methods: OpenCode Opus 4.6 and Haiku 4.5 used formal likelihood ratio tests; Browser ChatGPT used DHARMa testDispersion(); most others used informal AIC comparisons\nMixed effects: Browser ChatGPT was the only notebook to use a GLMM (lme4::glmer with provider_id as random intercept) – all others used standard GLMs\nZero-inflation: Browser Claude was the only notebook to formally test zero-inflation, fitting a ZINB model via pscl::zeroinfl() and comparing with a Vuong test; Codex CLI GPT-5.3 checked observed vs predicted zero rates\nNo Poisson comparison: GLM-5, Gemini 3.1 Pro, and Kimi K2.5 skipped Poisson and fit only NB models\n\n\n\nModel Specifications\nPredictor sets ranged from 4 to 9 variables:\n\nLargest predictor set (9 predictors): OpenCode Opus 4.6 and Browser Claude included age, income, education, race/ethnicity, insurance, job type, dependents, distance, and postnatal care receipt\nSmallest predictor set (4 predictors): GLM-5 and Gemini 3.1 Pro used only age, insurance, distance, and race/ethnicity\nPostnatal care as predictor: Browser Claude, OpenCode Opus 4.6, Haiku 4.5, and OpenCode Codex GPT-5.3 included received_comprehensive_postnatal_care as a predictor of comorbidity count\nDistance scaling: OpenCode Codex GPT-5.3 divided distance by 10; Browser Claude used log1p transformation; Browser ChatGPT used scale(); Haiku centered and scaled\nState: No chat/detailed notebook included state as a predictor (though several noted this as a limitation)\n\n\n\nDiagnostics Quality\nDiagnostic sophistication varied widely:\n\nHighest diagnostic depth: OpenCode Opus 4.6 (formal LR test, rootogram, residuals-vs-fitted with loess, deviance residual histogram, coefficient forest plot); Browser Claude (full DHARMa suite: testDispersion(), testZeroInflation(), testUniformity(), plus Vuong test and AIC/BIC table)\nGood: Haiku 4.5 (Pearson residuals vs fitted, histogram, Q-Q, index plots, formal LR test); Claude Code variants and Codex CLI GPT-5.3 (standard base R plot() residual panels plus AIC comparisons); Kimi K2.5 (Cook’s distance and influence diagnostics)\nMinimal: Gemini 3.1 Pro produced no residual diagnostics at all; GLM-5 had only a Q-Q plot\n\n\n\nResponsible Reporting\nAll completed notebooks noted that associations should not be interpreted causally, but depth varied:\n\nMost explicit limitations coverage: Claude Code (Opus and Sonnet) each listed 6 explicit limitations, including cross-sectional design, unmodeled clustering, self-reported income bias, synthetic data, and prompt-driven covariate selection\nOpenCode Opus 4.6 provided the most detailed prose discussion of model assumptions and their implications\nKimi K2.5 included a thorough limitations section covering causality, data quality, generalizability, clustering, missing data, and unmeasured confounding, plus 5 future research recommendations\nHaiku 4.5 addressed causality, synthetic data, clustering, race as social construct, and unmeasured confounders\nBrowser agents: ChatGPT had no explicit limitations section; Claude had minimal discussion\nMost limited limitations coverage: GLM-5 and Codex CLI GPT-5.2 mentioned limitations only briefly"
  },
  {
    "objectID": "demos/2-tools-introduction/summary.html#simple-prompt-results",
    "href": "demos/2-tools-introduction/summary.html#simple-prompt-results",
    "title": "Comparing AI Tool Outputs: Maternal Health Dataset Analysis",
    "section": "Simple Prompt Results",
    "text": "Simple Prompt Results\nThe simple prompt gave no guidance on outcome variable or modeling strategy. Every completed notebook using correct columns chose logistic regression on received_comprehensive_postnatal_care.\n\nKimi K2.5: A Different Analysis Entirely\nOpenCode + Kimi K2.5 hallucinated the data structure. Instead of actual columns, it referenced RiskLevel, SystolicBP, DiastolicBP, BS, BodyTemp, and HeartRate – columns from the Kaggle “Maternal Health Risk Data Set.” It then fit a multinomial logistic model for 3-level risk categories. This code would fail on render because those columns are absent from the CSV. This appears to be pattern retrieval from a similarly named dataset rather than reading the provided data.\nIt would likely be easy to steer the model back to the correct columns with a short corrective prompt. I left this run unchanged because the failure mode itself is instructive.\n\n\nModeling Strategy\nThe 9 completed notebooks using correct data all chose logistic regression, but implementation varied:\n\nStandard GLM: Most used glm(family = binomial) with linear predictor terms\nNonlinear terms: OpenCode Codex GPT-5.3 used natural splines (ns(age, df = 4) and ns(distance_to_provider, df = 4)); Codex CLI GPT-5.2 used a quadratic age term (age + I(age^2)) and log-transformed distance\nComposite variables: OpenCode Opus 4.6 uniquely created a comorbidity_count predictor (sum of 8 binary indicators) instead of including each individually\nPredictive framing: OpenCode Haiku 4.5 and OpenCode Codex GPT-5.3 used 80/20 train/test splits with classification metrics – a machine-learning framing rather than statistical inference\n\n\n\nPredictor Selection\nPredictor count and choice varied widely:\n\nMost parsimonious (6 predictors): Gemini 3.1 Pro used only age, education, insurance, distance, obesity, and hypertension\nKitchen sink (~ .): Haiku 4.5 included everything except id and provider_id, including all 51 state levels\nState fixed effects: Codex CLI GPT-5.2 and OpenCode Codex GPT-5.3 also included state\nComposite approach: OpenCode Opus 4.6 collapsed 8 comorbidity indicators into a single comorbidity_count variable for parsimony\nStandard approach (16 predictors): Claude Code variants and Codex CLI GPT-5.3 included all substantive predictors except state, with individual comorbidity terms\n\n\n\nVisualization Quality\n\nMost comprehensive visualization set: OpenCode Opus 4.6 produced 7 subplots across 4 figure chunks using patchwork, including an odds-ratio forest plot with confidence intervals\nGood: Claude Code Opus (5 plots + forest plot), Claude Code Sonnet (5 plots including age-by-education interaction), Codex CLI GPT-5.3 (4 plots)\nMinimal: Haiku 4.5 and GLM-5 each produced only 1 plot\nNone: Gemini 3.1 Pro, OpenCode Codex GPT-5.3, and Codex CLI GPT-5.2 produced zero visualizations\n\n\n\nDiagnostics\n\nHighest diagnostic depth: OpenCode Opus 4.6 used binned residuals (appropriate for logistic regression), deviance residuals, Pearson residuals, and pseudo-R²\nClassification-oriented: Haiku 4.5 hand-coded AUC via the trapezoid rule; Codex GPT-5.3 implemented a custom rank-based AUC function; GLM-5 used the pROC package; Codex CLI GPT-5.2 reported Brier score alongside accuracy\nStandard: Claude Code variants and Codex CLI GPT-5.3 reported AIC, McFadden pseudo-R², and odds ratio tables\nMinimal: Gemini 3.1 Pro reported only the model summary with exp(coef()) – no AIC, no residuals, no classification metrics\n\n\n\nConclusions Drawn\nDespite different predictor sets, completed notebooks converged on similar substantive findings:\n\nInsurance: All found significant associations between insurance type and postnatal care receipt\nEducation: Higher education levels consistently associated with higher care receipt\nDistance: Greater distance to provider associated with lower care receipt\nComorbidities: Claude Code Opus 4.6 noted the “paradoxical” finding that comorbidities were positively associated with care (likely reflecting clinical follow-up rather than health status)\n\n\n\nLimitations\n\nMost explicit limitations coverage (6 items): OpenCode Opus 4.6 explicitly discussed observational design, clustering, self-reported income, synthetic data, no interaction terms, and multiple comparisons\nGood: Claude Code Sonnet (4 items), Codex CLI GPT-5.3 (3 items), Codex CLI GPT-5.2 (noted modest fit and suggested additional modeling strategies)\nIncomplete: GLM-5’s conclusion contained unfilled placeholder text\nNone mentioned: Haiku 4.5, Gemini 3.1 Pro, OpenCode Codex GPT-5.3, and Claude Code Opus 4.6 (which discussed findings interpretively but had no formal limitations section)"
  },
  {
    "objectID": "demos/2-tools-introduction/summary.html#harness-comparison",
    "href": "demos/2-tools-introduction/summary.html#harness-comparison",
    "title": "Comparing AI Tool Outputs: Maternal Health Dataset Analysis",
    "section": "Harness Comparison: Same Model, Different Harness",
    "text": "Harness Comparison: Same Model, Different Harness\nTwo models were run across multiple harnesses, allowing direct comparison of how the CLI tool/harness shapes output for the same underlying model.\n\nOpus 4.6: Claude Code vs OpenCode\nOpus 4.6 was run via Claude Code and OpenCode for both detailed and simple prompts – 4 notebooks total.\n\nDetailed PromptSimple Prompt\n\n\n\n\n\n\n\n\n\n\nDimension\nClaude Code\nOpenCode\n\n\n\n\nStructure\nClean, pedagogical sections with narrative transitions\nMore comprehensive; publication-style formatting with kableExtra\n\n\nPredictors\n7 (excluded job_type, postnatal care)\n9 (broadest set; included job_type, postnatal care)\n\n\nOverdispersion\nInformal: deviance/df + AIC comparison\nFormal: AIC + likelihood ratio test with chi-square p-value\n\n\nVisualizations\n5 (relational bar charts, boxplots, distribution)\n10+ (added forest plot, rootogram, residual panels via patchwork)\n\n\nDiagnostics\nBase R plot() 2x2 panel\nLR test, rootogram, residuals-vs-fitted with loess, deviance histogram\n\n\nLimitations\n6 explicit items, well-organized list\nDetailed prose discussing each assumption and its implications\n\n\nFormatting\nStandard kable tables\nkableExtra with significance stars, polished layout\n\n\n\nTakeaway:\nBoth runs completed end-to-end analyses. The Claude Code version used a more instructional structure. The OpenCode version included a larger predictor set, more formal tests, more plots, and more diagnostics. In practice, the two versions differ mostly in depth and presentation style.\n\n\n\n\n\n\n\n\n\n\nDimension\nClaude Code\nOpenCode\n\n\n\n\nOutcome\nLogistic on postnatal care\nLogistic on postnatal care\n\n\nPredictors\n16 individual (all comorbidities separate)\n10 (collapsed comorbidities into composite count)\n\n\nVisualizations\n5 plots + OR forest plot\n7 sub-plots via patchwork + OR forest plot\n\n\nDiagnostics\nAIC, pseudo-R², OR table\nAIC, pseudo-R², binned residuals, deviance residuals, Pearson residuals\n\n\nUnique\nComorbidity prevalence table, paradoxical finding discussion\nBinned residual plot (rare, appropriate for logistic regression)\n\n\nLimitations\nImplicit only (interpretive notes, no formal section)\n6 explicit items including clustering, interactions, multiple comparisons\n\n\n\nTakeaway:\nThe same pattern appears in the simple prompt. Claude Code used a more teaching-oriented structure. OpenCode included additional diagnostics (including binned residuals) and a longer limitations section. OpenCode also used a composite comorbidity predictor instead of 8 separate binary terms.\n\n\n\n\n\nCodex GPT-5.3: Codex CLI vs OpenCode\nGPT-5.3 was run via Codex CLI and OpenCode for both prompts – 4 notebooks total.\n\nDetailed PromptSimple Prompt\n\n\n\n\n\n\n\n\n\n\nDimension\nCodex CLI\nOpenCode\n\n\n\n\nStructure\nStraightforward code-first approach\nSimilar code-first approach\n\n\nPredictors\n6 (care status, distance, insurance, race, age, dependents)\n8 (added edu, income)\n\n\nOverdispersion\nPearson/df with threshold 1.2; also AIC\nPearson/df with threshold 1.5\n\n\nVisualizations\n4 (including unique heatmap tile)\n3 (boxplot, bar charts)\n\n\nDiagnostics\nResiduals + observed vs predicted zero rates\nBase R residuals + Q-Q plot\n\n\nLimitations\n5 items (mean-variance, independence, functional form, omitted vars, outcome construction)\n4 items (confounding, outcome construction, residual caveats, data specificity)\n\n\nNotable\nHeatmap of care rate by race x insurance\nScaled distance by dividing by 10; hand-computed Wald CIs\n\n\n\nTakeaway:\nThe two harnesses produced similar GPT-5.3 outputs on the detailed prompt. Codex CLI added a heatmap, a zero-rate check, and a longer limitations section. Relative to the Opus comparison, differences were smaller.\n\n\n\n\n\n\n\n\n\n\nDimension\nCodex CLI\nOpenCode\n\n\n\n\nPredictors\n16 (no state)\nAll including state + natural splines\n\n\nVisualizations\n4 (care by insurance, care by race, distance histogram, distance boxplot)\n0\n\n\nDiagnostics\nAIC, pseudo-R², confusion matrix\nCustom AUC, classification metrics, prevalence-based threshold\n\n\nLimitations\n3 explicit items\nNone\n\n\nApproach\nStatistical inference framing\nMachine learning framing (train/test split)\n\n\n\nTakeaway:\nHere, harnesses produced markedly different analyses from the same model. Codex CLI used a traditional statistical-analysis framing with visualizations, pseudo-R², and a limitations section. OpenCode used a predictive-modeling framing with splines, train/test splits, and custom AUC code, but no visualizations or limitations section.\n\n\n\n\n\nSummary: Does Harness Matter?\nYes, but the effect depends on the model.\n\nFor Opus 4.6, harness effects were large. OpenCode runs included more predictors, tests, and diagnostics; Claude Code runs were more templated and instructional in structure.\nFor GPT-5.3, harness effects were smaller on the detailed prompt and larger on the simple prompt (statistical vs ML framing split).\nClaude Code reduced between-run variance. Opus and Sonnet in Claude Code produced very similar notebook structure, predictors, diagnostics, and limitations.\nOpenCode showed wider spread across models. OpenCode outputs ranged from very detailed runs to clear failure modes (for example, Kimi K2.5 hallucinating the data structure)."
  },
  {
    "objectID": "demos/2-tools-introduction/summary.html#scientific-conclusions",
    "href": "demos/2-tools-introduction/summary.html#scientific-conclusions",
    "title": "Comparing AI Tool Outputs: Maternal Health Dataset Analysis",
    "section": "Would Different Approaches Change the Scientific Conclusions?",
    "text": "Would Different Approaches Change the Scientific Conclusions?\nNotebooks used different modeling choices, and those choices changed coefficient size and significance calls. Below are coefficient/SE snapshots for common terms.\n\nDetailed Prompt: Coefficient Snapshots (Count Models)\n\n\n\n\n\n\n\n\n\n\n\nRun\nModel\nAge coef (SE)\nDistance coef (SE)\nInsurance: Private coef (SE)\nInsurance: State-provided coef (SE)\n\n\n\n\nBrowser ChatGPT\nGLMM\n0.0155 (0.00086)\n-0.00210 (0.00528) [a]\n-0.00008 (0.01249)\n-0.0164 (0.01381)\n\n\nClaude Code + Opus 4.6\nNB\n0.0159 (0.00093)\n-0.00014 (0.00038)\n-0.0030 (0.01326)\n-0.0166 (0.01443)\n\n\nOpenCode + Opus 4.6\nNB\n0.0164 (0.00083)\n-0.00022 (0.00034)\n-0.0140 (0.01287)\n-0.0282 (0.01308)\n\n\nOpenCode + Codex GPT-5.3\nNB\n0.016 (0.001) [b]\n-0.001 (0.004) [c]\n-0.005 (0.013)\n-0.017 (0.014)\n\n\n\n[a] scale(distance_to_provider) in that run (standardized distance, not raw miles).\n[b] Values rounded to 3 decimals in rendered output.\n[c] distance_to_provider_10 in that run (distance divided by 10).\n\n\nSimple Prompt: Coefficient Snapshots (Logistic Models)\n\n\n\n\n\n\n\n\n\n\nRun\nAge coef (SE)\nDistance coef (SE)\nInsurance: Private coef (SE)\nInsurance: State-provided coef (SE)\n\n\n\n\nClaude Code + Opus 4.6\n-0.00334 (0.00164) [d]\n-0.00252 (0.00067)***\n0.19172 (0.02515)***\n0.11964 (0.02586)***\n\n\nClaude Code + Sonnet 4.6\n-0.00201 (0.00185)\n-0.00244 (0.00075)**\n0.16606 (0.02841)***\n0.07981 (0.02861)**\n\n\nOpenCode + Opus 4.6\n-0.00311 (0.00164) [d]\n-0.00254 (0.00067)***\n0.19162 (0.02514)***\n0.11986 (0.02585)***\n\n\nOpenCode + Haiku 4.5\n-0.00088 (0.00185)\n-0.00271 (0.00075)***\n0.09487 (0.02952)**\n0.01126 (0.03014)\n\n\nOpenCode + Gemini 3.1 Pro\n-0.00391 (0.00157)*\n-0.00256 (0.00067)***\n0.26084 (0.02247)***\n0.12962 (0.02572)***\n\n\n\n[d] Marginal (p &lt; 0.10 but &gt;= 0.05).\nQuick read: distance remains negative and usually highly significant across runs; insurance coefficients stay positive but vary in magnitude (especially in Haiku); age is the least stable in significance.\n\n\nWhat Remained Robust\nDespite all these differences, a few findings held across every specification:\n\nAge positively predicts comorbidity count (all detailed notebooks agree)\nInsurance type is significantly associated with both comorbidity count and postnatal care receipt (direction varies by reference category)\nHigher education is associated with higher postnatal care receipt (all simple notebooks that included it agree)\nDistance to provider has a small or null association with comorbidity count but a more meaningful negative association with postnatal care receipt"
  },
  {
    "objectID": "demos/2-tools-introduction/summary.html#key-takeaways",
    "href": "demos/2-tools-introduction/summary.html#key-takeaways",
    "title": "Comparing AI Tool Outputs: Maternal Health Dataset Analysis",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nModel choice affects analysis depth. In this set of runs, Opus 4.6 and GPT-5.3 generally included more diagnostics, visualizations, and limitations detail. Gemini 3.1 Pro and GLM-5 generally included fewer of these elements. Haiku 4.5 and Kimi K2.5 varied more across prompt types.\nHarness (CLI tool) shapes output style. Claude Code outputs were more uniform in structure. OpenCode outputs showed more variation in scope and style. Codex CLI was between those two patterns.\nCore model-family choices were similar across completed runs. Most detailed runs used NB-family count models and most simple runs used logistic models. Differences were mainly in assumption checks, diagnostics, limitations reporting, and visualization coverage.\nFailure modes varied. One run hallucinated the data structure entirely (Simple Kimi K2.5). One produced incomplete placeholder text (Simple GLM-5). One had structural code issues (Detailed GLM-5). These failures were more common in OpenCode, where models had less scaffolding.\nPrompt specificity changed behavior for some models. Kimi K2.5 produced a detailed-prompt run with influence diagnostics and detailed limitations, then hallucinated the dataset structure on the simple prompt. Haiku 4.5 showed a similar but less extreme shift from detailed to simple prompts. Opus and GPT-5.3 changed less across prompt specificity in this experiment."
  },
  {
    "objectID": "demos/2-tools-introduction/summary.html#did-detailed-prompts-produce-more-unified-results",
    "href": "demos/2-tools-introduction/summary.html#did-detailed-prompts-produce-more-unified-results",
    "title": "Comparing AI Tool Outputs: Maternal Health Dataset Analysis",
    "section": "Did Detailed Prompts Produce More Unified Results?",
    "text": "Did Detailed Prompts Produce More Unified Results?\nShort answer: partly yes. Detailed prompts increased convergence on analysis target and workflow, but did not fully standardize model specification.\nImportant: outcomes and model families differ between detailed and simple prompts, so coefficient magnitudes are not directly comparable across prompt types.\n\n\n\n\n\n\n\n\nConvergence indicator\nDetailed prompt\nSimple prompt\n\n\n\n\nTarget variable convergence\nHigh: prompt specified comorbidity_count\nModerate-high: most chose received_comprehensive_postnatal_care, but not required\n\n\nModel-family convergence\nHigh: count-model workflow with NB emphasis\nHigh: logistic was the dominant choice\n\n\nPredictor-set dispersion\nModerate: roughly 4 to 9 predictors in many runs\nHigh: ranged from parsimonious to near-kitchen-sink specifications\n\n\nUse of additional structural terms\nMixed: some included postnatal care as a predictor of comorbidity count\nMixed: some included state fixed effects/nonlinear terms; others did not\n\n\nDiagnostics/reporting consistency\nModerate-high: diagnostics sections usually present\nMixed: diagnostics depth ranged from minimal to extensive\n\n\nFailure-mode frequency\nLower in detailed runs\nHigher in simple runs (including hallucinated structure and placeholder text)\n\n\n\nInterpretation: detailed prompts made outputs more similar on target and workflow. Variation still remained in covariates, transformations/scaling, diagnostics depth, and control choices that affect interpretation."
  },
  {
    "objectID": "demos/2-tools-introduction/source/02-tools-detailed-template.html",
    "href": "demos/2-tools-introduction/source/02-tools-detailed-template.html",
    "title": "Tools Comparison (Detailed): {{HARNESS_LABEL}} + {{MODEL_LABEL}}",
    "section": "",
    "text": "Harness: {HARNESS}\nModel: {MODEL}\nPrompt type: detailed"
  },
  {
    "objectID": "demos/2-tools-introduction/source/02-tools-detailed-template.html#run-metadata",
    "href": "demos/2-tools-introduction/source/02-tools-detailed-template.html#run-metadata",
    "title": "Tools Comparison (Detailed): {{HARNESS_LABEL}} + {{MODEL_LABEL}}",
    "section": "",
    "text": "Harness: {HARNESS}\nModel: {MODEL}\nPrompt type: detailed"
  },
  {
    "objectID": "demos/2-tools-introduction/source/02-tools-detailed-template.html#setup",
    "href": "demos/2-tools-introduction/source/02-tools-detailed-template.html#setup",
    "title": "Tools Comparison (Detailed): {{HARNESS_LABEL}} + {{MODEL_LABEL}}",
    "section": "Setup",
    "text": "Setup\n\nlibrary(dplyr)\n\ndf &lt;- readr::read_csv(here::here(\"data/synthetic/simulated_maternal_health_data.csv\"))"
  },
  {
    "objectID": "demos/2-tools-introduction/source/02-tools-detailed-template.html#prompt",
    "href": "demos/2-tools-introduction/source/02-tools-detailed-template.html#prompt",
    "title": "Tools Comparison (Detailed): {{HARNESS_LABEL}} + {{MODEL_LABEL}}",
    "section": "Prompt",
    "text": "Prompt\nI want you to help me analyze this dataset: `data/synthetic/simulated_maternal_health_data.csv`\n\nMy goal is to produce a thorough analysis that is statistically sound and lays out all steps of the analysis.\n\nPlease do the following:\n\n1. Load the dataset and normalize columns/types.\n   - Use clear snake_case names.\n   - Ensure categorical vs numeric types are explicit.\n\n2. Summarize the data.\n   - Report row count, column count, missingness, and key descriptive summaries.\n   - Flag suspicious values or quality concerns.\n\n3. Create key visualizations.\n   - Show relationships for:\n     - `received_comprehensive_postnatal_care`\n     - `distance_to_provider`\n     - `insurance`\n     - `race_ethnicity`\n\n4. Build a count outcome and model it.\n   - Define `comorbidity_count` as the row-wise sum of:\n     - `obesity`,\n     - `multiple_gestation`,\n     - `diabetes`,\n     - `heart_disease`,\n     - `placenta_previa`,\n     - `hypertension`,\n     - `gest_hypertension`,\n     - `preeclampsia`.\n   - Select an appropriate count-model strategy (Poisson vs negative binomial, with justification).\n   - Fit the model and report interpretable results.\n\n5. Report results responsibly.\n   - Do not overstate causality.\n   - Include model assumptions, diagnostics, and limitations.\n\nConstraints:\n- Prefer readable, modular, reproducible code that follows R community idioms.\n- Report back a summary of what you changed."
  },
  {
    "objectID": "demos/1-eda/1-eda-opencode-minimax.html",
    "href": "demos/1-eda/1-eda-opencode-minimax.html",
    "title": "EDA Example: Palmer Penguins",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(penguins)"
  },
  {
    "objectID": "demos/1-eda/1-eda-example-sonnet.html",
    "href": "demos/1-eda/1-eda-example-sonnet.html",
    "title": "EDA Example: Palmer Penguins",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(penguins)"
  },
  {
    "objectID": "demos/1-eda/1-eda-example-claude.html",
    "href": "demos/1-eda/1-eda-example-claude.html",
    "title": "EDA Example: Palmer Penguins",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(penguins)\npenguins &lt;- penguins |&gt; filter(!is.na(sex))"
  },
  {
    "objectID": "demos/1-eda/1-eda-example-claude.html#distributions-of-key-variables",
    "href": "demos/1-eda/1-eda-example-claude.html#distributions-of-key-variables",
    "title": "EDA Example: Palmer Penguins",
    "section": "Distributions of Key Variables",
    "text": "Distributions of Key Variables\n\nlibrary(patchwork)\n\np1 &lt;- ggplot(penguins, aes(x = bill_length_mm, fill = species)) +\n  geom_histogram(alpha = 0.7, position = \"identity\", bins = 30) +\n  labs(x = \"Bill Length (mm)\", y = \"Count\")\n\np2 &lt;- ggplot(penguins, aes(x = bill_depth_mm, fill = species)) +\n  geom_histogram(alpha = 0.7, position = \"identity\", bins = 30) +\n  labs(x = \"Bill Depth (mm)\", y = \"Count\")\n\np3 &lt;- ggplot(penguins, aes(x = flipper_length_mm, fill = species)) +\n  geom_histogram(alpha = 0.7, position = \"identity\", bins = 30) +\n  labs(x = \"Flipper Length (mm)\", y = \"Count\")\n\np4 &lt;- ggplot(penguins, aes(x = body_mass_g, fill = species)) +\n  geom_histogram(alpha = 0.7, position = \"identity\", bins = 30) +\n  labs(x = \"Body Mass (g)\", y = \"Count\")\n\n(p1 + p2) / (p3 + p4) +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "demos/1-eda/1-eda-example-claude.html#relationships-between-variables",
    "href": "demos/1-eda/1-eda-example-claude.html#relationships-between-variables",
    "title": "EDA Example: Palmer Penguins",
    "section": "Relationships Between Variables",
    "text": "Relationships Between Variables\n\nlibrary(GGally)\n\npenguins |&gt;\n  select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex) |&gt;\n  ggpairs(\n    columns = 2:5,\n    aes(color = species, alpha = 0.6),\n    upper = list(continuous = wrap(\"cor\", size = 3)),\n    lower = list(continuous = wrap(\"points\", size = 1)),\n    diag = list(continuous = wrap(\"densityDiag\", alpha = 0.5))\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = sex)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(x = \"Bill Length (mm)\", y = \"Bill Depth (mm)\",\n       title = \"Bill Length vs. Depth by Species and Sex\")\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "demos/1-eda/1-eda-example-claude.html#table-1-summary-statistics",
    "href": "demos/1-eda/1-eda-example-claude.html#table-1-summary-statistics",
    "title": "EDA Example: Palmer Penguins",
    "section": "Table 1: Summary Statistics",
    "text": "Table 1: Summary Statistics\n\nlibrary(gtsummary)\n\npenguins |&gt;\n  select(species, sex, island, bill_length_mm, bill_depth_mm,\n         flipper_length_mm, body_mass_g) |&gt;\n  tbl_summary(\n    by = species,\n    label = list(\n      sex ~ \"Sex\",\n      island ~ \"Island\",\n      bill_length_mm ~ \"Bill Length (mm)\",\n      bill_depth_mm ~ \"Bill Depth (mm)\",\n      flipper_length_mm ~ \"Flipper Length (mm)\",\n      body_mass_g ~ \"Body Mass (g)\"\n    ),\n    statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\",\n      all_categorical() ~ \"{n} ({p}%)\"\n    ),\n    digits = all_continuous() ~ 1\n  ) |&gt;\n  add_overall() |&gt;\n  add_p() |&gt;\n  bold_labels()\n\n\n\nTable 1: Characteristics of Palmer Penguins by Species\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall\nN = 3331\nAdelie\nN = 1461\nChinstrap\nN = 681\nGentoo\nN = 1191\np-value2\n\n\n\n\nSex\n\n\n\n\n\n\n\n\n&gt;0.9\n\n\n    female\n165 (50%)\n73 (50%)\n34 (50%)\n58 (49%)\n\n\n\n\n    male\n168 (50%)\n73 (50%)\n34 (50%)\n61 (51%)\n\n\n\n\nIsland\n\n\n\n\n\n\n\n\n&lt;0.001\n\n\n    Biscoe\n163 (49%)\n44 (30%)\n0 (0%)\n119 (100%)\n\n\n\n\n    Dream\n123 (37%)\n55 (38%)\n68 (100%)\n0 (0%)\n\n\n\n\n    Torgersen\n47 (14%)\n47 (32%)\n0 (0%)\n0 (0%)\n\n\n\n\nBill Length (mm)\n44.0 (5.5)\n38.8 (2.7)\n48.8 (3.3)\n47.6 (3.1)\n&lt;0.001\n\n\nBill Depth (mm)\n17.2 (2.0)\n18.3 (1.2)\n18.4 (1.1)\n15.0 (1.0)\n&lt;0.001\n\n\nFlipper Length (mm)\n201.0 (14.0)\n190.1 (6.5)\n195.8 (7.1)\n217.2 (6.6)\n&lt;0.001\n\n\nBody Mass (g)\n4,207.1 (805.2)\n3,706.2 (458.6)\n3,733.1 (384.3)\n5,092.4 (501.5)\n&lt;0.001\n\n\n\n1 n (%); Mean (SD)\n\n\n2 Pearson’s Chi-squared test; Kruskal-Wallis rank sum test"
  },
  {
    "objectID": "course_docs/course-project.html",
    "href": "course_docs/course-project.html",
    "title": "Course Project",
    "section": "",
    "text": "The only assignment for this course to attempt something ambitious with AI tools and report honestly on what happened.\nPick a task that feels beyond your current ability, or in a domain you know little about, and use AI tools to make a serious attempt at it. You do not need to write a single line of code yourself. You can let the AI write all of it. In fact, I would encourage you to see how far you can go. What I want to see is the attempt, the reflection, and the honest accounting of what worked and what didn’t."
  },
  {
    "objectID": "course_docs/course-project.html#overview",
    "href": "course_docs/course-project.html#overview",
    "title": "Course Project",
    "section": "",
    "text": "The only assignment for this course to attempt something ambitious with AI tools and report honestly on what happened.\nPick a task that feels beyond your current ability, or in a domain you know little about, and use AI tools to make a serious attempt at it. You do not need to write a single line of code yourself. You can let the AI write all of it. In fact, I would encourage you to see how far you can go. What I want to see is the attempt, the reflection, and the honest accounting of what worked and what didn’t."
  },
  {
    "objectID": "course_docs/course-project.html#deliverables",
    "href": "course_docs/course-project.html#deliverables",
    "title": "Course Project",
    "section": "Deliverables",
    "text": "Deliverables\n\n1. Topic proposal\nDue: February 26 (before Session 2)\nEmail a short description of your proposed moonshot task (a few sentences is fine) to ewestlund@jhu.edu.\n\n\n2. Project and reflection\nDue: March 12 (Session 4)\nSubmit your project materials and a written reflection.\nProject materials: Share via a GitHub repository (preferred) or a zip file emailed to ewestlund@jhu.edu. Include whatever you produced — code, output, data, notebooks, etc. The project does not need to be finished.\nWritten reflection (2–3 pages): Your reflection must address:\n\nWhat you attempted. Describe the task and why you chose it.\nWhat worked and what didn’t. Be specific. Which parts did AI tools handle well? Where did they fail, mislead, or produce something you couldn’t use?\nHow you checked your work. Explain how you verified that AI-generated output was correct. What strategies did you use? What did you catch? What might you have missed?\n\n\n\n3. In-class share (Session 4)\nDuring Session 4, each student will give an informal 3-minute summary of their moonshot to the class. No slides required. We will discuss as a group after each share.\nThe project does not need to be “finished” by Session 4. Incomplete attempts with honest reflection are perfectly acceptable."
  },
  {
    "objectID": "course_docs/course-project.html#guidelines",
    "href": "course_docs/course-project.html#guidelines",
    "title": "Course Project",
    "section": "Guidelines",
    "text": "Guidelines\n\nNo code writing required. You may let AI tools generate all code. The point is to explore what the tools can and cannot do, not to demonstrate your own coding ability.\nHonest reporting matters more than success. A failed attempt with a thoughtful reflection is worth more than a polished result with no critical analysis.\nUse any AI tools you want. ChatGPT, Claude, Copilot, Cursor, Claude Code, or anything else. You may use multiple tools and compare them.\nDocument your process. Save chat logs, screenshots, or session histories where possible. These can support your reflection but are not required as part of the submission."
  },
  {
    "objectID": "course_docs/course-project.html#timeline",
    "href": "course_docs/course-project.html#timeline",
    "title": "Course Project",
    "section": "Timeline",
    "text": "Timeline\n\n\n\nDate\nMilestone\n\n\n\n\nFebruary 19 (Session 1)\nAssignment introduced; begin brainstorming\n\n\nFebruary 26 (Session 2)\nTopic proposal due via email\n\n\nMarch 5 (Session 3)\nProgress check-in during class\n\n\nMarch 12 (Session 4)\nProject and reflection due; in-class share"
  },
  {
    "objectID": "course_docs/syllabus.html",
    "href": "course_docs/syllabus.html",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Erik Westlund, PhD\nDepartment of Biostatistics\nJohns Hopkins Bloomberg School of Public Health\newestlund@jhu.edu"
  },
  {
    "objectID": "course_docs/syllabus.html#instructor",
    "href": "course_docs/syllabus.html#instructor",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Erik Westlund, PhD\nDepartment of Biostatistics\nJohns Hopkins Bloomberg School of Public Health\newestlund@jhu.edu"
  },
  {
    "objectID": "course_docs/syllabus.html#course-description",
    "href": "course_docs/syllabus.html#course-description",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Course Description",
    "text": "Course Description\nAs AI tools are rapidly adopted across research and industry, there is a growing need for statisticians and data scientists to understand what these tools are capable of and how to use them responsibly. This course provides practical approaches for integrating large language models and agent-based tools into statistical workflows. Students learn how to structure AI-assisted processes for analysis, simulation, and pipeline development, along with core skills in context management and agent orchestration. The course emphasizes AI safety, privacy, and responsible handling of sensitive data. Students will discuss these topics and do hands-on exercises to test the strengths and weaknesses of AI tools in practice."
  },
  {
    "objectID": "course_docs/syllabus.html#course-details",
    "href": "course_docs/syllabus.html#course-details",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Course Details",
    "text": "Course Details\n\n\n\nDates\nFebruary 19, February 26, March 5, March 12, 2026\n\n\nTime\n9:00–10:20 AM ET\n\n\nLocation\nZoom (link provided on CoursePlus/Email)\n\n\nFormat\nLecture, live demos, discussion, hands-on exercises\n\n\nGrading\nPass/Fail"
  },
  {
    "objectID": "course_docs/syllabus.html#course-learning-objectives",
    "href": "course_docs/syllabus.html#course-learning-objectives",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nBy the end of this course, students will be able to:\n\nDescribe how large language models work at a conceptual level, including their capabilities and limitations for statistical work.\nEvaluate the ethical implications of AI tool use in research, including privacy, bias, reproducibility, and academic integrity.\nUse code to work with data, not the LLM itself, to protect sensitive and regulated data (including PHI) in privacy-sensitive contexts.\nNavigate the landscape of AI tools — chat interfaces, IDE integrations, CLI agents, and supporting tools — and select appropriate tools for different tasks.\nUse AI assistants to write, debug, and audit code for data cleaning, visualization, and statistical analysis.\nBuild analysis workflows on synthetic data and deploy validated code to secure data environments using Git.\nCritically assess AI-generated output and identify errors, hallucinations, and inappropriate statistical choices."
  },
  {
    "objectID": "course_docs/syllabus.html#schedule",
    "href": "course_docs/syllabus.html#schedule",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Schedule",
    "text": "Schedule\n\nSession 1 (February 19): Foundations: History, Today, and Ethics\nWe will share our experiences using AI and study the foundations of AI tools. We will discuss ethics and concerns people have with using the tools.\n\n\nSession 2 (February 26): The Toolbox\nWe will introduce and demonstrate various tools available to us, including major models (Claude, GPT, Gemini), open source models, chat tools, IDEs (VS Code, Positron, Cursor), CLI tools (Claude Code, Codex, OpenCode), and supporting utilities (tmux, Git).\n\nDue before class: Email course project topic to ewestlund@jhu.edu\n\n\n\nSession 3 (March 5): AI-Assisted Statistical Workflows\nWe will discuss how to use these tools to summarize datasets, generate synthetic/simulated data, and build models. We will pay special attention to how to work with PHI using these tools.\n\nDue: Make progress on course project; prepare for check-in\n\n\n\nSession 4 (March 12): Synthesis and Looking Forward\nWe will spend most of this session sharing our experiences with our projects, but will leave it open to address lingering questions.\n\nDue: Course project materials and 2–3 page reflection; in-class share"
  },
  {
    "objectID": "course_docs/syllabus.html#course-project",
    "href": "course_docs/syllabus.html#course-project",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Course Project",
    "text": "Course Project\nEach student will attempt an ambitious task using AI tools — something you know little about, or that feels beyond your current capability. The goal is not to produce publication-quality work. It is to explore the limits and possibilities of these tools in a low-stakes way and to report honestly on what happened. See the course project page for full details.\n\n\n\nDate\nMilestone\n\n\n\n\nFebruary 19 (Session 1)\nAssignment introduced; begin brainstorming\n\n\nFebruary 26 (Session 2)\nTopic proposal due via email\n\n\nMarch 5 (Session 3)\nProgress check-in during class\n\n\nMarch 12 (Session 4)\nProject and reflection due; in-class share"
  },
  {
    "objectID": "course_docs/syllabus.html#tools-and-subscriptions",
    "href": "course_docs/syllabus.html#tools-and-subscriptions",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Tools and Subscriptions",
    "text": "Tools and Subscriptions\nStudents should subscribe to at least one AI tool for the duration of the course. Options include:\n\n\n\nTool\nCost\n\n\n\n\nOpenAI ChatGPT Plus\n$20/month\n\n\nAnthropic Claude Pro\n$20/month\n\n\nGoogle Gemini Advanced\n$20/month (one month free trial)\n\n\nCursor Pro\n$20/month\n\n\nGitHub Copilot Pro\n$10/month\n\n\n\nVariety across the class is encouraged — we will compare how different tools handle the same problems. If financing is a concern, please reach out to the instructor. Google Gemini Advanced offers a free trial that can cover the course period.\nStudents should also ensure that Git is installed and a GitHub account is registered before the first class. Mac and Linux typically have Git pre-installed. Otherwise, follow directions at git-scm.com."
  },
  {
    "objectID": "course_docs/syllabus.html#course-materials",
    "href": "course_docs/syllabus.html#course-materials",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Course Materials",
    "text": "Course Materials\nThere is no required textbook. Course readings and further reading are listed on the course website. All course materials are available on GitHub."
  },
  {
    "objectID": "course_docs/syllabus.html#methods-of-assessment",
    "href": "course_docs/syllabus.html#methods-of-assessment",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Methods of Assessment",
    "text": "Methods of Assessment\nThis is a pass/fail course. Assessment is based on:\n\nParticipation (50%): Attend all sessions and engage actively in discussions and hands-on exercises.\nCourse project (50%): Attempt your project and present your experience to the class in Session 4."
  },
  {
    "objectID": "course_docs/syllabus.html#generative-ai-policy",
    "href": "course_docs/syllabus.html#generative-ai-policy",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Generative AI Policy",
    "text": "Generative AI Policy\nUsing AI tools is the subject of this course. Their use is permitted, encouraged, and expected. It is nevertheless the student’s responsibility to understand the output of these tools and ensure their correctness. Students are strongly encouraged to approach these tools as learning aids and not crutches."
  },
  {
    "objectID": "course_docs/syllabus.html#academic-ethics",
    "href": "course_docs/syllabus.html#academic-ethics",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Academic Ethics",
    "text": "Academic Ethics\nStudents enrolled in the Bloomberg School of Public Health of The Johns Hopkins University assume an obligation to conduct themselves in a manner appropriate to the University’s mission as an institution of higher education. Students should be familiar with the policies and procedures specified under Policy and Procedure Manual Student-01 (Academic Ethics) and the Student Conduct Code (Student-06), available at my.publichealth.jhu.edu."
  },
  {
    "objectID": "course_docs/syllabus.html#student-health-and-well-being",
    "href": "course_docs/syllabus.html#student-health-and-well-being",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Student Health and Well-being",
    "text": "Student Health and Well-being\nIf you are struggling with anxiety, stress, depression, or other mental health related concerns, please consider connecting with resources:\n\nStudent support: bit.ly/bsphstudentsupport\nMental Health Services: wellbeing.jhu.edu/MentalHealthServices\nBehavioral Health Crisis Support Team (24/7): 410-516-9355"
  },
  {
    "objectID": "course_docs/syllabus.html#disability-accommodations",
    "href": "course_docs/syllabus.html#disability-accommodations",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Disability Accommodations",
    "text": "Disability Accommodations\nStudent Disability Services (SDS) provides accessible and inclusive educational experiences for students with disabilities. To request accommodations:\n\nComplete the SDS online application via AIM\nSubmit documentation using the provided link after application submission\nSchedule a meeting with Audrey Ndaba\n\nMore information: Student Disability Services"
  },
  {
    "objectID": "demos/1-eda/1-eda-example-opencode-codex.html",
    "href": "demos/1-eda/1-eda-example-opencode-codex.html",
    "title": "EDA Example: Palmer Penguins",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(penguins)"
  },
  {
    "objectID": "demos/1-eda/1-eda-openai-codex.html",
    "href": "demos/1-eda/1-eda-openai-codex.html",
    "title": "EDA Example: Palmer Penguins",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(penguins)"
  },
  {
    "objectID": "demos/2-tools-introduction/source/02-tools-chat-template.html",
    "href": "demos/2-tools-introduction/source/02-tools-chat-template.html",
    "title": "Tools Comparison (Chat): {{HARNESS_LABEL}} + {{MODEL_LABEL}}",
    "section": "",
    "text": "Harness: {HARNESS}\nModel: {MODEL}\nPrompt type: chat"
  },
  {
    "objectID": "demos/2-tools-introduction/source/02-tools-chat-template.html#run-metadata",
    "href": "demos/2-tools-introduction/source/02-tools-chat-template.html#run-metadata",
    "title": "Tools Comparison (Chat): {{HARNESS_LABEL}} + {{MODEL_LABEL}}",
    "section": "",
    "text": "Harness: {HARNESS}\nModel: {MODEL}\nPrompt type: chat"
  },
  {
    "objectID": "demos/2-tools-introduction/source/02-tools-chat-template.html#setup",
    "href": "demos/2-tools-introduction/source/02-tools-chat-template.html#setup",
    "title": "Tools Comparison (Chat): {{HARNESS_LABEL}} + {{MODEL_LABEL}}",
    "section": "Setup",
    "text": "Setup\n\nlibrary(dplyr)\n\ndf &lt;- readr::read_csv(here::here(\"data/synthetic/simulated_maternal_health_data.csv\"))\n\ndf |&gt; glimpse()\n\nRows: 50,000\nColumns: 20\n$ id                                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1…\n$ provider_id                           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ state                                 &lt;chr&gt; \"AK\", \"AK\", \"AK\", \"AK\", \"AK\", \"A…\n$ received_comprehensive_postnatal_care &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,…\n$ self_report_income                    &lt;chr&gt; \"$50,000–$74,999\", \"$50,000–$74,…\n$ age                                   &lt;dbl&gt; 31, 26, 33, 37, 21, 33, 25, 34, …\n$ edu                                   &lt;chr&gt; \"hs\", \"hs\", \"some_college\", \"hs\"…\n$ race_ethnicity                        &lt;chr&gt; \"white\", \"white\", \"white\", \"asia…\n$ insurance                             &lt;chr&gt; \"no_insurance\", \"no_insurance\", …\n$ job_type                              &lt;chr&gt; \"unskilled\", \"unskilled\", \"unski…\n$ dependents                            &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,…\n$ distance_to_provider                  &lt;dbl&gt; 1.7889754, 10.8991396, 35.122047…\n$ obesity                               &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,…\n$ multiple_gestation                    &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,…\n$ diabetes                              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ heart_disease                         &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,…\n$ placenta_previa                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ hypertension                          &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ gest_hypertension                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ preeclampsia                          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…"
  },
  {
    "objectID": "demos/2-tools-introduction/source/02-tools-chat-template.html#prompt",
    "href": "demos/2-tools-introduction/source/02-tools-chat-template.html#prompt",
    "title": "Tools Comparison (Chat): {{HARNESS_LABEL}} + {{MODEL_LABEL}}",
    "section": "Prompt",
    "text": "Prompt\nI want you to help me analyze a dataset on maternal health.\n\nMy goal is to produce a thorough analysis that is statistically sound and lays out all steps of the analysis.\n\nI've pasted below the results of the `glimpse` function from dplyr, which shows you variable names, types, and some example values.\n\nGive me code, labeled by bullet number below to:\n\n1. Help me ensure variable names are standardized and that they're correctly typed in the R dataframe.\n\n2. Summarize the data:\n   - Report row count, column count, missingness, and key descriptive summaries.\n   - Flag suspicious values or quality concerns.\n\n3. Create key visualizations.\n   - Show relationships for:\n     - `received_comprehensive_postnatal_care`\n     - `distance_to_provider`\n     - `insurance`\n     - `race_ethnicity`\n\n4. Help me build a count outcome and model it.\n   - Define `comorbidity_count` as the row-wise sum of:\n     - `obesity`,\n     - `multiple_gestation`,\n     - `diabetes`,\n     - `heart_disease`,\n     - `placenta_previa`,\n     - `hypertension`,\n     - `gest_hypertension`,\n     - `preeclampsia`.\n   - Give me diagnostics to help me determine if I've picked an appropriate model given the data.\n   - Fit the model and report interpretable results.\n\nConstraints:\n- Prefer readable, modular, reproducible code that follows R community idioms.\n- Report back a summary of what you changed.\n\nPlease give me a single codeblock back I can cut and paste back into a quarto notebook."
  },
  {
    "objectID": "demos/2-tools-introduction/source/02-tools-simple-template.html",
    "href": "demos/2-tools-introduction/source/02-tools-simple-template.html",
    "title": "Tools Comparison (Simple): {{HARNESS_LABEL}} + {{MODEL_LABEL}}",
    "section": "",
    "text": "Harness: {HARNESS}\nModel: {MODEL}\nPrompt type: simple"
  },
  {
    "objectID": "demos/2-tools-introduction/source/02-tools-simple-template.html#run-metadata",
    "href": "demos/2-tools-introduction/source/02-tools-simple-template.html#run-metadata",
    "title": "Tools Comparison (Simple): {{HARNESS_LABEL}} + {{MODEL_LABEL}}",
    "section": "",
    "text": "Harness: {HARNESS}\nModel: {MODEL}\nPrompt type: simple"
  },
  {
    "objectID": "demos/2-tools-introduction/source/02-tools-simple-template.html#setup",
    "href": "demos/2-tools-introduction/source/02-tools-simple-template.html#setup",
    "title": "Tools Comparison (Simple): {{HARNESS_LABEL}} + {{MODEL_LABEL}}",
    "section": "Setup",
    "text": "Setup\n\nlibrary(dplyr)\n\ndf &lt;- readr::read_csv(here::here(\"data/synthetic/simulated_maternal_health_data.csv\"))"
  },
  {
    "objectID": "demos/2-tools-introduction/source/02-tools-simple-template.html#prompt",
    "href": "demos/2-tools-introduction/source/02-tools-simple-template.html#prompt",
    "title": "Tools Comparison (Simple): {{HARNESS_LABEL}} + {{MODEL_LABEL}}",
    "section": "Prompt",
    "text": "Prompt\nStudy this data set (`data/synthetic/simulated_maternal_health_data.csv`), show me descriptive statistics, pick a modeling strategy, and report the model. Be thorough."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health\nInstructor: Erik Westlund (ewestlund@jhu.edu)\nDates: February 19, February 26, March 5, March 12, 2026\nTime: 9:00–10:20 AM ET"
  },
  {
    "objectID": "index.html#sessions",
    "href": "index.html#sessions",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Sessions",
    "text": "Sessions\n\nSession 1 (February 19): Foundations: History, Today, and Ethics\nWe will share our experiences using AI and study the foundations of AI tools. We will discuss ethics/concerns people have with using the tools.\n\nSlides\n\n\n\nSession 2 (February 26): Tools Introduction\nWe will focus on practical tool use for biostatistics, with an emphasis on responsible use and verification.\n\nModel selection basics: model size, reasoning vs instruct behavior, local vs cloud, and cost/access tradeoffs\nInterfaces and harnesses: chat tools, browser/IDE agents, and CLI agents (Claude Code, Codex, OpenCode)\nOne standardized workflow: compare chat/simple/detailed prompts across harnesses and models on synthetic maternal health data\nSafety guardrails: no PHI in external tools, context boundaries, and verify-before-trust\nSlides\nExperiment: Comparing AI Tool Outputs on Maternal Health Data\nRendered Notebooks (all runs)\nPrompt Template: Chat\nPrompt Template: Detailed\nPrompt Template: Simple\nDue before class: Email course project topic to ewestlund@jhu.edu\n\n\n\nSession 3 (March 5): AI-Assisted Statistical Workflows\nWe will discuss how to use these tools to:\n\nSummarize datasets\nGenerate synthetic/simulated data\nBuild models\n\nWe will pay special attention to how to work with PHI using these tools\n\nSlides [coming soon]\nDue: Make progress on moonshot; prepare for check-in\n\n\n\nSession 4 (March 12): Synthesis and Looking Forward\nWe will spend most of this session sharing our experiences with our projects, but will leave it open to address lingering questions.\n\nSlides [coming soon]\nDue: Course project: project materials and 2–3 page reflection; in-class share"
  },
  {
    "objectID": "index.html#course-readings",
    "href": "index.html#course-readings",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Course Readings",
    "text": "Course Readings\nWhile I am not requiring you to read these papers, I am providing them for reference. The slides often reference them using the identifying numbers below. The concepts/problems these papers investigate and discuss are still relevant. Nevertheless, both the speed at which LLM technology is advancing as well the artificiality of many of the benchmarks should lead you to interpret presented data with caution.\n\nPublished\n\nNi, A., Yin, P., Zhao, Y., Riddell, M., Feng, T., Shen, R., Yin, S., Liu, Y., Yavuz, S., Xiong, C., Joty, S., Zhou, Y., Radev, D., & Cohan, A. (2024). L2CEval: Evaluating language-to-code generation capabilities of large language models. Transactions of the Association for Computational Linguistics, 12, 1311–1329. doi:10.1162/tacl_a_00705\nJiang, J., Wang, F., Shen, J., Kim, S., & Kim, S. (2025). A survey on large language models for code generation. ACM Transactions on Software Engineering and Methodology. doi:10.1145/3747588\nChen, S., Pusarla, P., & Ray, B. (2025). DyCodeEval: Dynamic benchmarking of reasoning capabilities in code large language models under data contamination. Proceedings of the 42nd International Conference on Machine Learning (ICML), PMLR 267, 8890–8909. proceedings.mlr.press\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., & Steinhardt, J. (2021). Measuring mathematical problem solving with the MATH dataset. Proceedings of NeurIPS 2021, Datasets and Benchmarks Track. openreview.net\nPhan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Zhang, C. B. C., Shaaban, M., Ling, J., Shi, S., Choi, M., Agrawal, A., Chopra, A., Khoja, A., Kim, R., Ren, R., Hausenloy, J., Zhang, O., Mazeika, M., Yue, S., Wang, A., & Hendrycks, D. (2025). Humanity’s Last Exam. Nature. doi:10.1038/s41586-025-09962-4\nTambon, F., Moradi Dakhel, A., Nikanjam, A., Khomh, F., Desmarais, M. C., & Antoniol, G. (2025). Bugs in large language models generated code: An empirical study. Empirical Software Engineering, 30(3). doi:10.1007/s10664-025-10614-4\nPan, R., Ibrahimzada, A. R., Krishna, R., Sankar, D., Pougeum Wassi, L., Merler, M., Sobolev, B., Pavuluri, R., Sinha, S., & Jabbarvand, R. (2024). Lost in translation: A study of bugs introduced by large language models while translating code. Proceedings of ICSE ’24. doi:10.1145/3597503.3639226\nTang, N., Chen, M., Ning, Z., Bansal, A., Huang, Y., McMillan, C., & Li, T. J.-J. (2024). Developer behaviors in validating and repairing LLM-generated code using eye tracking and IDE actions. IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2024), 40–46. doi:10.1109/VL/HCC60511.2024.00015\nZhou, X., Liang, P., Zhang, B., Li, Z., Ahmad, A., Shahin, M., & Waseem, M. (2024). Exploring the problems, their causes and solutions of AI pair programming: A study on GitHub and Stack Overflow. Journal of Systems and Software, 219, 112204. doi:10.1016/j.jss.2024.112204\nTang, L., Liu, J., Liu, Z., Yang, X., & Bao, L. (2025). LLM4SZZ: Enhancing SZZ algorithm with context-enhanced assessment on large language models. Proceedings of the ACM on Software Engineering, 2(ISSTA), 343–365. doi:10.1145/3728885\nDinh, T., Zhao, J., Tan, S., Negrinho, R., Lausen, L., Zha, S., & Karypis, G. (2023). Large language models of code fail at completing code with potential bugs. Proceedings of NeurIPS 2023. proceedings.neurips.cc\n\n\n\nPreprints\n\nFu, L., Chai, H., Du, K., Zhang, W., Luo, S., Lin, J., Fang, Y., Rui, R., Guan, H., Liu, J., Qi, S., Fan, L., Lei, J., Liu, Y., Wang, J., Zhang, K., Zhang, W., & Yu, Y. (2024). CodeApex: A bilingual programming evaluation benchmark for large language models. arXiv:2309.01940\nLu, Y., Yang, R., Zhang, Y., Yu, S., Dai, R., Wang, Z., Xiang, J., E, W., Gao, S., Ruan, X., Huang, Y., Xi, C., Hu, H., Fu, Y., Yu, Q., Wei, X., Gu, J., Sun, R., Jia, J., & Zhou, F. (2025). StatEval: A comprehensive benchmark for large language models in statistics. arXiv:2510.09517\nFu, Y., Ou, L., Chen, M., Wan, Y., Peng, H., & Khot, T. (2023). Chain-of-Thought Hub: A continuous effort to measure large language models’ reasoning performance. arXiv:2305.17306\nOpu, M. N. I., Wang, S., & Chowdhury, S. (2025). LLM-based detection of tangled code changes for higher-quality method-level bug datasets. arXiv:2505.08263\nIslam, N., Ayon, R. S., Thomas, D. G., Ahmed, S., & Wardat, M. (2026). When agents fail: A comprehensive study of bugs in LLM agents with automated labeling. arXiv:2601.15232\nGloaguen, T., Mundler, N., Muller, M., Raychev, V., & Vechev, M. (2026). Evaluating AGENTS.md: Are repository-level context files helpful for coding agents? arXiv:2602.11988"
  },
  {
    "objectID": "index.html#further-reading",
    "href": "index.html#further-reading",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Further Reading",
    "text": "Further Reading\n\nCode Quality, Bugs, and Security\n\nPearce, H., Ahmad, B., Tan, B., Dolan-Gavitt, B., & Karri, R. (2022). Asleep at the keyboard? Assessing the security of GitHub Copilot’s code contributions. IEEE Symposium on Security and Privacy (S&P 2022), 754–768. doi:10.1109/SP46214.2022.9833571\nPerry, N., Srivastava, M., Kumar, D., & Boneh, D. (2023). Do users write more insecure code with AI assistants? ACM Conference on Computer and Communications Security (CCS 2023). doi:10.1145/3576915.3623157\nSandoval, G., Pearce, H., Nys, T., Karri, R., Garg, S., & Dolan-Gavitt, B. (2023). Lost at C: A user study on the security implications of large language model code assistants. USENIX Security Symposium 2023, 2205–2222. usenix.org\nJesse, K., Ahmed, T., Devanbu, P., & Morgan, E. (2023). Large language models and simple, stupid bugs. IEEE/ACM 20th International Conference on Mining Software Repositories (MSR 2023), 563–575. doi:10.1109/MSR59073.2023.00082\nAsare, O., Nagappan, M., & Asokan, N. (2023). Is GitHub’s Copilot as bad as humans at introducing vulnerabilities in code? Empirical Software Engineering, 28, 129. doi:10.1007/s10664-023-10380-1\nLiu, J., Xia, C. S., Wang, Y., & Zhang, L. (2023). Is your code generated by ChatGPT really correct? Rigorous evaluation of large language models for code generation. Proceedings of NeurIPS 2023. proceedings.neurips.cc\nDu, X., Liu, M., Wang, K., Wang, H., et al. (2024). Evaluating large language models in class-level code generation. Proceedings of ICSE 2024. doi:10.1145/3597503.3639219\n\n\n\nAI-Assisted Programming\n\nVaithilingam, P., Zhang, T., & Glassman, E. L. (2022). Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. CHI Conference on Human Factors in Computing Systems Extended Abstracts (CHI EA ’22). doi:10.1145/3491101.3519665\nNguyen, N. & Nadi, S. (2022). An empirical evaluation of GitHub Copilot’s code suggestions. 19th International Conference on Mining Software Repositories (MSR ’22). doi:10.1145/3524842.3528470\nDakhel, A. M., Majdinasab, V., Nikanjam, A., Khomh, F., Desmarais, M. C., & Jiang, Z. M. J. (2023). GitHub Copilot AI pair programmer: Asset or liability? Journal of Systems and Software, 203, 111734. doi:10.1016/j.jss.2023.111734\nBarke, S., James, M. B., & Polikarpova, N. (2023). Grounded Copilot: How programmers interact with code-generating models. Proceedings of the ACM on Programming Languages (OOPSLA), 7(OOPSLA1), 85–111. doi:10.1145/3586030\nZiegler, A., Kalliamvakou, E., Li, X. A., Rice, A., et al. (2024). Measuring GitHub Copilot’s impact on productivity. Communications of the ACM, 67(3), 54–63. doi:10.1145/3633453\nLiang, J. T., Yang, C., & Myers, B. A. (2024). A large-scale survey on the usability of AI programming assistants: Successes and challenges. Proceedings of ICSE 2024. doi:10.1145/3597503.3608128\nMurali, V., Maddila, C., Ahmad, I., Bolin, M., et al. (2024). AI-assisted code authoring at scale: Fine-tuning, deploying, and mixed methods evaluation. Proceedings of the ACM on Software Engineering (PACMSE/FSE), 1(FSE), 1066–1085. doi:10.1145/3643774\n\n\n\nBenchmarks and Evaluation\n\nJimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., & Narasimhan, K. R. (2024). SWE-bench: Can language models resolve real-world GitHub issues? ICLR 2024 (Oral). openreview.net\nLai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, W., Fried, D., Wang, S., & Yu, T. (2023). DS-1000: A natural and reliable benchmark for data science code generation. Proceedings of ICML 2023, PMLR 202, 18319–18345. proceedings.mlr.press\n\n\n\nReasoning Capabilities and Limitations\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q. V., & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. Proceedings of NeurIPS 2022. proceedings.neurips.cc\nValmeekam, K., Marquez, M., Sreedharan, S., & Kambhampati, S. (2023). On the planning abilities of large language models — a critical investigation. Proceedings of NeurIPS 2023 (Spotlight). proceedings.neurips.cc\nKambhampati, S., Valmeekam, K., Guan, L., Verma, M., Stechly, K., et al. (2024). Position: LLMs can’t plan, but can help planning in LLM-Modulo frameworks. Proceedings of ICML 2024 (Spotlight), PMLR 235, 22895–22907. proceedings.mlr.press\n\n\n\nEthics and Responsible AI\n\nWeidinger, L., Mellor, J., Rauh, M., Griffin, C., et al. (2022). Taxonomy of risks posed by language models. ACM Conference on Fairness, Accountability, and Transparency (FAccT 2022). doi:10.1145/3531146.3533088\nHosseini, M., Resnik, D. B., & Holmes, K. (2023). The ethics of disclosing the use of artificial intelligence tools in writing scholarly manuscripts. Research Ethics, 19(4), 449–465. doi:10.1177/17470161231180449\nLiao, Q. V. & Vaughan, J. W. (2024). AI transparency in the age of LLMs: A human-centered research roadmap. Harvard Data Science Review, Special Issue 5. doi:10.1162/99608f92.8036d03b\n\n\n\nHuman Factors in AI-Assisted Coding\n\nMozannar, H., Bansal, G., Fourney, A., & Horvitz, E. (2024). Reading between the lines: Modeling user behavior and costs in AI-assisted programming. CHI 2024 (Honorable Mention). doi:10.1145/3613904.3641936\nFerdowsi, K., Huang, R., James, M. B., Polikarpova, N., & Lerner, S. (2024). Validating AI-generated code with live programming. CHI 2024. doi:10.1145/3613904.3642495"
  },
  {
    "objectID": "slides/02-tools-introduction.html#section",
    "href": "slides/02-tools-introduction.html#section",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Session 1 Wrap-Up"
  },
  {
    "objectID": "slides/02-tools-introduction.html#model-improvement-over-time",
    "href": "slides/02-tools-introduction.html#model-improvement-over-time",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Model Improvement Over Time",
    "text": "Model Improvement Over Time\n\nModels have become better on many benchmarks: coding, math, and factual recall\nLarger context windows and stronger instruction-following\nBetter tooling integration (agents, code execution, retrieval)"
  },
  {
    "objectID": "slides/02-tools-introduction.html#model-improvement-limitations",
    "href": "slides/02-tools-introduction.html#model-improvement-limitations",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Model Improvement: Limitations",
    "text": "Model Improvement: Limitations\n\nOften still brittle with noisy context\nOften still overconfident when wrong\nOften still sensitive to prompt and evaluation setup"
  },
  {
    "objectID": "slides/02-tools-introduction.html#improvement-vs.-reliability",
    "href": "slides/02-tools-introduction.html#improvement-vs.-reliability",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Improvement vs. Reliability",
    "text": "Improvement vs. Reliability\nWorking distinction for class discussion:\n\nImprovement = higher average performance on selected tasks\nReliability = dependable behavior in your real workflow\n\n\nProgress can be real without being enough for high-stakes trust."
  },
  {
    "objectID": "slides/02-tools-introduction.html#goodharts-law",
    "href": "slides/02-tools-introduction.html#goodharts-law",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Goodhart’s Law",
    "text": "Goodhart’s Law\n“When a measure becomes a target, it ceases to be a good measure.”\n\nApplied here:\n\nIf leaderboard scores become the target, model behavior can optimize for tests\nReported gains may overstate real-world usefulness\nContamination, narrow tasks, and benchmark gaming can hide weakness"
  },
  {
    "objectID": "slides/02-tools-introduction.html#goodhart-style-questions",
    "href": "slides/02-tools-introduction.html#goodhart-style-questions",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Goodhart-style Questions",
    "text": "Goodhart-style Questions\n\nAsk: What is being measured?\nAsk: What is being optimized?\nAsk: What is left out?\n\n\nFor statistical work, does the benchmark capture context quality, uncertainty, and auditability?"
  },
  {
    "objectID": "slides/02-tools-introduction.html#discussion",
    "href": "slides/02-tools-introduction.html#discussion",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Discussion",
    "text": "Discussion\n\nIs there any area you would not use an LLM today, even if accuracy improved?\n\n\n\nIs it ethical to use LLMs that trained on data they did not acquire through legal and/or ethical means?\n\n\n\n\nOther comments/questions?"
  },
  {
    "objectID": "slides/02-tools-introduction.html#section-1",
    "href": "slides/02-tools-introduction.html#section-1",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Model Choice\n\n\nNavigating the landscape of language models"
  },
  {
    "objectID": "slides/02-tools-introduction.html#follow-along-on-github",
    "href": "slides/02-tools-introduction.html#follow-along-on-github",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Follow Along on GitHub",
    "text": "Follow Along on GitHub\nAll the code, datasets, and examples for this session are available on GitHub.\n\nFollow along: You can clone or pull down the repository to run the examples yourself.\nReproducible: All workflows and agent tasks shown today are in the repo.\nExperiment: Feel free to run the agent prompts on your own machine."
  },
  {
    "objectID": "slides/02-tools-introduction.html#model-dimensions-size-vs.-cost",
    "href": "slides/02-tools-introduction.html#model-dimensions-size-vs.-cost",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Model Dimensions: Size vs. Cost",
    "text": "Model Dimensions: Size vs. Cost\nModels are categorized loosely by their parameter size:\n\nFrontier/Heavy (e.g., Opus 4.6, GPT-5.3): Maximum capability. Highest latency/cost. Best for complex logic.\nMid-weight (e.g., Sonnet 4.6): The “daily drivers”. Good balance of speed, cost, and capability.\nLight/Fast (e.g., Haiku 4.5, Gemini Flash): Very fast, cheap. Great for simple tasks and repetitive agent use."
  },
  {
    "objectID": "slides/02-tools-introduction.html#access-and-cost-management",
    "href": "slides/02-tools-introduction.html#access-and-cost-management",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Access and Cost Management",
    "text": "Access and Cost Management\nHow you access models dictates how much you pay. There are three main ways to buy access:\n\nFirst-Party Subscriptions\nDirect APIs (Pay-per-token)\nThird-Party Aggregators"
  },
  {
    "objectID": "slides/02-tools-introduction.html#first-party-subscriptions",
    "href": "slides/02-tools-introduction.html#first-party-subscriptions",
    "title": "AI Tools for Data Science and Statistics",
    "section": "1. First-Party Subscriptions",
    "text": "1. First-Party Subscriptions\n(e.g., ChatGPT Plus, Claude Pro, GitHub Copilot)\n\nCost: Usually ~$20/month flat rate.\nLimits: Usage caps (e.g., “50 messages every 3 hours”).\nBest for: Chat interfaces, daily general use, browser-based coding.\nData Policy: Often heavily subsidized because your inputs are used for model training (unless you explicitly opt out).\nCatch: You don’t have API keys, so you cannot always plug these into custom CLI tools or scripts."
  },
  {
    "objectID": "slides/02-tools-introduction.html#direct-apis-pay-per-token",
    "href": "slides/02-tools-introduction.html#direct-apis-pay-per-token",
    "title": "AI Tools for Data Science and Statistics",
    "section": "2. Direct APIs (Pay-per-token)",
    "text": "2. Direct APIs (Pay-per-token)\n\nCost: You pay exactly for what you use (input + output tokens).\nLimits: Determined by your monthly spending limit or prepaid balance.\nBest for: CLI agents (Claude Code, Codex CLI), automated scripts, large batch jobs.\nData Policy: Providers typically do not use API data for training (good for personal privacy). HOWEVER: Paying for API access does not make it safe or legal to send PHI over the wire!\nCatch: Can get expensive quickly, especially with large context windows and/or extensive agent workflows."
  },
  {
    "objectID": "slides/02-tools-introduction.html#third-party-aggregators",
    "href": "slides/02-tools-introduction.html#third-party-aggregators",
    "title": "AI Tools for Data Science and Statistics",
    "section": "3. Third-Party Aggregators",
    "text": "3. Third-Party Aggregators\n(e.g., OpenRouter, OpenCode Zen)\n\nCost: Buy prepaid credits to access any provider’s models through a single interface. Often pay close to full API rates.\nLimits: Flexible; allows switching between Anthropic, OpenAI, Google, and open-source models seamlessly.\nBest for: Testing multiple models without managing 5 different API accounts.\nCatch: You must trust a middleman with your data. You may end up relying on models you haven’t fully vetted."
  },
  {
    "objectID": "slides/02-tools-introduction.html#cloud-models-api-web",
    "href": "slides/02-tools-introduction.html#cloud-models-api-web",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Cloud Models (API / Web)",
    "text": "Cloud Models (API / Web)\n\nManaged by providers (Anthropic, OpenAI, Google)\nRequires sending your data to external servers\nHighest performance ceiling"
  },
  {
    "objectID": "slides/02-tools-introduction.html#local-models-ollama-llama.cpp",
    "href": "slides/02-tools-introduction.html#local-models-ollama-llama.cpp",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Local Models (Ollama / Llama.cpp)",
    "text": "Local Models (Ollama / Llama.cpp)\n\nRun entirely on your machine\nTotal data privacy (safe for some PHI under right conditions)\nPerformance limited by your computer’s RAM and GPU"
  },
  {
    "objectID": "slides/02-tools-introduction.html#quantization-running-locally",
    "href": "slides/02-tools-introduction.html#quantization-running-locally",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Quantization (Running Locally)",
    "text": "Quantization (Running Locally)\nHow do you fit a 70-billion parameter model on a laptop? Quantization.\n\nCompresses the precision of the model’s weights (e.g., from 16-bit to 4-bit)\nDramatically reduces memory usage and increases speed\nTradeoff: Lowers capability and reasoning precision"
  },
  {
    "objectID": "slides/02-tools-introduction.html#instruct-models",
    "href": "slides/02-tools-introduction.html#instruct-models",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Instruct Models",
    "text": "Instruct Models\n\nRespond immediately based on context\nFast and cheap\nMore prone to hallucination on complex logic"
  },
  {
    "objectID": "slides/02-tools-introduction.html#reasoning-models",
    "href": "slides/02-tools-introduction.html#reasoning-models",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Reasoning Models",
    "text": "Reasoning Models\n(e.g., OpenAI o1, DeepSeek-R1)\n\nSpend extra compute to generate internal “thought” chains\nThink before outputting text\nBetter at math and coding logic\nSlower and more expensive"
  },
  {
    "objectID": "slides/02-tools-introduction.html#section-2",
    "href": "slides/02-tools-introduction.html#section-2",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Chat vs. Agents"
  },
  {
    "objectID": "slides/02-tools-introduction.html#chat-interfaces",
    "href": "slides/02-tools-introduction.html#chat-interfaces",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Chat Interfaces",
    "text": "Chat Interfaces\nThe standard way most people use AI:\n\nPaste in some context and a question\nGet an answer back\nPaste the answer into your editor, wire it up, test.\nRepeat"
  },
  {
    "objectID": "slides/02-tools-introduction.html#the-chat-limit",
    "href": "slides/02-tools-introduction.html#the-chat-limit",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Chat Limit",
    "text": "The Chat Limit\nYou act as a middleman.\n\nThe model has no persistent context of your files.\nIt cannot test the code it writes.\nIt relies entirely on your prompt clarity."
  },
  {
    "objectID": "slides/02-tools-introduction.html#agents",
    "href": "slides/02-tools-introduction.html#agents",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Agents",
    "text": "Agents\nAn Agent is not a different AI model. It is a harness around a model.\nHarness = Model + Tools + Permissions + Context + Workflow\nRather than just returning text, the model can:\n\nread a file in your project\ngrep (search) for a variable name\nedit files directly\nbash (run) the script to see if it throws an error\n\nThis process iterates between calling tools, editing files, querying models, to achieve a task."
  },
  {
    "objectID": "slides/02-tools-introduction.html#the-agent-workflow",
    "href": "slides/02-tools-introduction.html#the-agent-workflow",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Agent Workflow",
    "text": "The Agent Workflow\n\nYou give a high-level goal (“Create a summary stats table for my dataset”).\nThe agent explores the directory.\nIt writes the code and saves it.\nIt runs the code, reads the error log, and fixes its own mistake.\nIt presents the final result."
  },
  {
    "objectID": "slides/02-tools-introduction.html#command-line-cli-agents",
    "href": "slides/02-tools-introduction.html#command-line-cli-agents",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Command Line (CLI) Agents",
    "text": "Command Line (CLI) Agents\n\nClaude Code: Anthropic’s CLI agent.\nCodex: OpenAI’s CLI agent.\nGemini CLI: Google’s CLI agent.\n\nThere are other harnesses/agents like OpenCode (CLI) that can connect to model providers and perform similar tasks."
  },
  {
    "objectID": "slides/02-tools-introduction.html#browser-ide-agents",
    "href": "slides/02-tools-introduction.html#browser-ide-agents",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Browser / IDE Agents",
    "text": "Browser / IDE Agents\n\nVS Code: Install Copilot extension and use agents to edit within your project.\nCursor / Windsurf: Entire code editors built around agentic loops.\nWeb interfaces with “Advanced Data Analysis” (e.g., ChatGPT data execution)."
  },
  {
    "objectID": "slides/02-tools-introduction.html#security-data-privacy",
    "href": "slides/02-tools-introduction.html#security-data-privacy",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Security: Data Privacy",
    "text": "Security: Data Privacy\n\nNo PHI in external tools. If you allow the LLM access to directories with PHI, it will send them over the wire.\nData Boundaries: Agents can read anything in the folder you give them access to."
  },
  {
    "objectID": "slides/02-tools-introduction.html#security-system-risks",
    "href": "slides/02-tools-introduction.html#security-system-risks",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Security: System Risks",
    "text": "Security: System Risks\nGiving an AI the ability to run commands carries risk.\n\nLeast Privilege: Don’t run agents as “root” or give them access to sensitive system paths.\nAudit Trails: Good agents ask for permission before running destructive commands (rm, git push)."
  },
  {
    "objectID": "slides/02-tools-introduction.html#verify-before-trust",
    "href": "slides/02-tools-introduction.html#verify-before-trust",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Verify Before Trust",
    "text": "Verify Before Trust\nAgents can still fail. They might force code to run by ignoring a statistical assumption, rather than fixing the logic.\n\nSyntatically valid != Statistically correct\nCheck tables and plots for accuracy.\nUse other agents to review the work of your first agent."
  },
  {
    "objectID": "slides/02-tools-introduction.html#live-demo-harness-comparison",
    "href": "slides/02-tools-introduction.html#live-demo-harness-comparison",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Live Demo: Harness Comparison",
    "text": "Live Demo: Harness Comparison\nWe will run the exact same prompt across different agents and models.\nThe Task: Take data/synthetic/simulated_maternal_health_data.csv. Normalize it, summarize missingness, create key plots, and fit a count model (comorbidity_count).\nWe will evaluate: Code quality, statistical judgment, and prompt adherence."
  }
]