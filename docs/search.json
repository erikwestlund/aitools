[
  {
    "objectID": "slides/01-foundations.html#section",
    "href": "slides/01-foundations.html#section",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Good Old-Fashioned AI\n\n\nBefore LLMs: 70 years of trying to make machines think."
  },
  {
    "objectID": "slides/01-foundations.html#the-initial-vision-1950s-1960s",
    "href": "slides/01-foundations.html#the-initial-vision-1950s-1960s",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Initial Vision (1950s-1960s)",
    "text": "The Initial Vision (1950s-1960s)\n\n1950: Turing proposes the imitation game. “Can machines think?”\n1956: Dartmouth workshop coins “Artificial Intelligence.” Marvin Minsky, Herbert Simon, and others predict human-level AI within a generation.\n1958: Frank Rosenblatt builds the Perceptron. The first neural network. The New York Times reports the Navy expects it to “walk, talk, see, write, reproduce itself, and be conscious.”\n\n\nMarvin Minsky (1967): “Within a generation… the problem of creating ‘artificial intelligence’ will substantially be solved.”"
  },
  {
    "objectID": "slides/01-foundations.html#the-symbolic-era-gofai",
    "href": "slides/01-foundations.html#the-symbolic-era-gofai",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Symbolic Era: GOFAI",
    "text": "The Symbolic Era: GOFAI\n“Good Old-Fashioned AI” (Haugeland 1985) posits that intelligence is symbol manipulation.\nThe approach: Encode human knowledge as logical rules, then reason over them.\n\n\n\n\n\n\n\n\nSystem\nWhat It Did\nLimitation\n\n\n\n\nELIZA (1966)\nPattern-matched to simulate a therapist\nNo understanding; pure string tricks\n\n\nSHRDLU (1971)\nUnderstood natural language about blocks on a table\nOnly worked in a tiny toy world\n\n\nMYCIN (1976)\nDiagnosed bacterial infections with ~600 rules\nCouldn’t learn; every rule hand-written\n\n\nCYC (1984-)\nAttempted to encode all common sense\n40+ years later, still not done"
  },
  {
    "objectID": "slides/01-foundations.html#the-critics",
    "href": "slides/01-foundations.html#the-critics",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Critics",
    "text": "The Critics\nTwo philosophers saw fundamental problems:\nHubert Dreyfus [What Computers Can’t Do; (1972)]: Human expertise is embodied and intuitive, not rule-based. A chess master doesn’t search a tree. They see the board. GOFAI can’t capture this. (His brother Stuart Dreyfus, an operations researcher, co-developed the skill acquisition model behind this critique.)\n\nJohn Searle [“Chinese Room”; (1980)]: A system can manipulate symbols perfectly and understand nothing. Syntax is not semantics.\n\n\nDreyfus again [What Computers Still Can’t Do; (1992)]: After 20 more years, the same problems remain. GOFAI is still a failure."
  },
  {
    "objectID": "slides/01-foundations.html#ai-winters",
    "href": "slides/01-foundations.html#ai-winters",
    "title": "AI Tools for Data Science and Statistics",
    "section": "AI “Winters”",
    "text": "AI “Winters”\nOverpromising leads to underfunding. Twice:\nFirst AI Winter (1974-1980)\n\nLighthill Report (UK, 1973): AI has failed to deliver on its promises\nDARPA cuts funding; the field contracts\n\nSecond AI Winter (1987-1993)\n\nExpert systems boom, then bust. Too brittle, too expensive to maintain\nJapan’s Fifth Generation Computer Systems fails to provide a foundation for AI\nNeural network research stalls"
  },
  {
    "objectID": "slides/01-foundations.html#ai-revival-1990s-now",
    "href": "slides/01-foundations.html#ai-revival-1990s-now",
    "title": "AI Tools for Data Science and Statistics",
    "section": "AI Revival (1990s-now)",
    "text": "AI Revival (1990s-now)\nAI stopped trying to be “intelligent” and started being useful:\n\nMachine learning replaces hand-coded rules with learning from data\nStatistical methods dominate: SVMs, random forests, boosting. These are tools many of you already likely use.\nDeep learning breakthrough (2012): AlexNet wins ImageNet by a landslide using a neural network and GPUs\n\n\nThe shift: from “encode what we know” to “learn from what we have.”"
  },
  {
    "objectID": "slides/01-foundations.html#what-searle-and-dreyfus-would-say-now",
    "href": "slides/01-foundations.html#what-searle-and-dreyfus-would-say-now",
    "title": "AI Tools for Data Science and Statistics",
    "section": "What Searle and Dreyfus Would Say Now",
    "text": "What Searle and Dreyfus Would Say Now\nTheir critiques haven’t gone away. They’ve shapeshifted:\n\n\n\n\n\n\n\nGOFAI Critique\nLLM Version\n\n\n\n\nCan’t handle ambiguity\nHandles surface ambiguity; fails on deep reasoning\n\n\nBrittle when rules don’t cover the case\nFails catastrophically on buggy or novel contexts\n\n\nSyntax without semantics (Searle)\nGenerates fluent text without “understanding” it\n\n\nNo embodied knowledge (Dreyfus)\nNo experience, no clinical intuition, no common sense grounding\n\n\nCan’t plan or reason (both)\nLLMs fail on classical planning tasks that require genuine reasoning (Valmeekam et al. 2023; Kambhampati et al. 2024)\n\n\n\n\nThe question hasn’t changed: Is sophisticated pattern matching the same as understanding?"
  },
  {
    "objectID": "slides/01-foundations.html#the-hype-cycle-is-not-new",
    "href": "slides/01-foundations.html#the-hype-cycle-is-not-new",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Hype Cycle Is Not New",
    "text": "The Hype Cycle Is Not New\n\n\n\n\n\n\n\n\nEra\nThe Promise\nWhat Actually Happened\n\n\n\n\n1960s\n“AI in a generation”\nTwo AI winters\n\n\n1980s\nExpert systems will replace professionals\nToo brittle; collapsed\n\n\n2010s\nSelf-driving cars by 2020\nStill not solved in 2026\n\n\n2020s\nAGI is imminent; all jobs automated\n???\n\n\n\n\nWe are somewhere on this curve. Knowing the history helps you locate where."
  },
  {
    "objectID": "slides/01-foundations.html#section-1",
    "href": "slides/01-foundations.html#section-1",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "What is a Large Language Model?\n\n\nWhat follows is simplified for teaching. The goal is a useful mental model, not technical precision."
  },
  {
    "objectID": "slides/01-foundations.html#the-simplest-explanation",
    "href": "slides/01-foundations.html#the-simplest-explanation",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Simplest Explanation",
    "text": "The Simplest Explanation\nAn LLM is a function that predicts the next word.\n\nGiven a sequence of words, it outputs a probability distribution over what comes next.\n\n\nIt is often called “sophisticated autocomplete” — though with advances in context windows and planning-like behavior on some tasks, the analogy has limits."
  },
  {
    "objectID": "slides/01-foundations.html#tokens-and-embeddings",
    "href": "slides/01-foundations.html#tokens-and-embeddings",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Tokens and Embeddings",
    "text": "Tokens and Embeddings\n\nText is split into tokens (subword units, not whole words)\n\n“biostatistics” \\(\\rightarrow\\) “bio” + “stat” + “istics”\n\nEach token is mapped to a vector (a list of numbers)\nThese vectors capture meaning through position in high-dimensional space\n\n\nPrincipal Components Analysis analogy: Just as PCA finds axes of variation in your data, embeddings find axes of meaning in language. Words with similar meanings cluster together."
  },
  {
    "objectID": "slides/01-foundations.html#attention",
    "href": "slides/01-foundations.html#attention",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Attention",
    "text": "Attention\nThe core innovation (Vaswani et al. 2017):\nWords look at other words to determine their meaning.\n\n\n“They walked along the bank of the river.”\n\n\n“She deposited the check at the bank.”\n\nSame word, different meaning. Attention resolves this by weighing context.\n\n\nAn analogy: Wittgenstein’s “meaning is use” [Philosophical Investigations; (1953)]: a word’s meaning isn’t fixed; it comes from how it’s used in context. Attention mechanisms capture something structurally similar."
  },
  {
    "objectID": "slides/01-foundations.html#how-do-llms-learn",
    "href": "slides/01-foundations.html#how-do-llms-learn",
    "title": "AI Tools for Data Science and Statistics",
    "section": "How Do LLMs “Learn?”",
    "text": "How Do LLMs “Learn?”\nThree stages. Most capability comes from stage 1. Stages 2-3 mostly shape behavior, not knowledge.\n\nPre-training: Predict the next token across massive text corpora (internet, books, code, curated datasets). The model learns statistical patterns of language and knowledge. It stores weights and distributed patterns, not documents.\nInstruction tuning (Supervised Fine-Tuning): Fine-tune on curated prompt-response examples so the model follows instructions and behaves like an assistant. This changes behavior more than knowledge.\nPreference optimization (Reinforcement Learning from Human Feedback, or similar): Humans compare outputs; a reward model learns their preferences; the model is optimized to produce responses humans rate as more helpful and aligned."
  },
  {
    "objectID": "slides/01-foundations.html#key-concepts-for-users",
    "href": "slides/01-foundations.html#key-concepts-for-users",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Key Concepts for Users",
    "text": "Key Concepts for Users\n\n\n\n\n\n\n\nConcept\nWhat It Means\n\n\n\n\nContext window\nHow much text the model can “see” at once (200K-400K tokens for frontier models)\n\n\nTemperature\nControls randomness. Low = deterministic, high = creative\n\n\nStochasticity\nSame prompt can give different answers each time\n\n\nNo memory\nEach conversation starts from scratch (unless you provide context)"
  },
  {
    "objectID": "slides/01-foundations.html#what-an-llm-is-not",
    "href": "slides/01-foundations.html#what-an-llm-is-not",
    "title": "AI Tools for Data Science and Statistics",
    "section": "What an LLM Is Not",
    "text": "What an LLM Is Not\n\nNot a database. It doesn’t “look up” answers. It generates them.\nNot a search engine. It doesn’t retrieve documents (unless given tools to do so).\nNot deterministic. Same input \\(\\neq\\) same output.\nNot “thinking” the way you do. It can produce both correct reasoning chains and convincing nonsense, with no reliable way to tell the difference from the inside.\nNot an expert. It can sound authoritative while being completely wrong."
  },
  {
    "objectID": "slides/01-foundations.html#section-2",
    "href": "slides/01-foundations.html#section-2",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "A Brief History of LLMs (2017-2026)"
  },
  {
    "objectID": "slides/01-foundations.html#the-foundation-2017-2020",
    "href": "slides/01-foundations.html#the-foundation-2017-2020",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Foundation (2017-2020)",
    "text": "The Foundation (2017-2020)\n\n2017: “Attention Is All You Need” introduces the Transformer architecture\n2018: GPT-1 (117M parameters). Language modeling as pre-training\n2019: GPT-2 (1.5B). “Too dangerous to release”\n2020: GPT-3 (175B). Can perform tasks from just a few examples in the prompt, without additional training\n\nWhere we were on math: The best models scored 3–7% on the MATH benchmark. GPT-2 1.5B reached 6.9% after pretraining on a math corpus and fine-tuning; GPT-3 175B managed only 5.2% few-shot. (Hendrycks et al. 2021)"
  },
  {
    "objectID": "slides/01-foundations.html#the-scaling-era-2021-2023",
    "href": "slides/01-foundations.html#the-scaling-era-2021-2023",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Scaling Era (2021-2023)",
    "text": "The Scaling Era (2021-2023)\n\n2021: Codex, which was GPT-3 fine-tuned on code, powers GitHub Copilot\n2022: ChatGPT launches (November). AI enters mainstream consciousness\n2023: GPT-4 arrives. A leap:\n\n80.5% on HumanEval (code generation) (Ni et al. 2024)\n42.5% on MATH (competition math) (Fu et al. 2023)\n86.4% on MMLU (academic knowledge) (Fu et al. 2023)\n\nOpen-source explosion: LLaMA, Code Llama, StarCoder, Mistral"
  },
  {
    "objectID": "slides/01-foundations.html#the-reasoning-era-2024-2025",
    "href": "slides/01-foundations.html#the-reasoning-era-2024-2025",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Reasoning Era (2024-2025)",
    "text": "The Reasoning Era (2024-2025)\n\n2024: OpenAI releases o1. A “reasoning model” that thinks step-by-step\n2025: o3-mini, DeepSeek-R1 push reasoning further\n\no3-mini: 13.4% on Humanity’s Last Exam (vs. GPT-4o at 2.7%) (Phan et al. 2025)\n\n\n\nKey insight: Reasoning models use the same underlying architecture. What’s different is how they’re trained and how they spend compute: they think longer before answering, they’re trained on different data mixtures, and they check their own work. It’s a harness of the models."
  },
  {
    "objectID": "slides/01-foundations.html#what-makes-reasoning-models-different",
    "href": "slides/01-foundations.html#what-makes-reasoning-models-different",
    "title": "AI Tools for Data Science and Statistics",
    "section": "What Makes Reasoning Models Different?",
    "text": "What Makes Reasoning Models Different?\nReasoning models (o1, o3-mini, DeepSeek-R1) are built on the same transformer architecture as every other LLM.\nThe difference is how they spend compute:\n\nA standard model answers immediately — one pass through the network\nA reasoning model “thinks out loud” before answering (you see this as a loading delay)\nIt spends more compute per question, trading speed and cost for accuracy\n\n\no3-mini scored ~5x higher than GPT-4o on Humanity’s Last Exam (Phan et al. 2025)"
  },
  {
    "objectID": "slides/01-foundations.html#why-reasoning-models-matter",
    "href": "slides/01-foundations.html#why-reasoning-models-matter",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Why Reasoning Models Matter",
    "text": "Why Reasoning Models Matter\nWhat changed beyond just “thinking longer”:\n\nTrained on different data mixtures emphasizing step-by-step problem solving\nBuilt-in verification loops — the model checks its own work\nPreference optimization tuned specifically for careful deliberation\n\n\nThe tradeoff: slower, more expensive, but substantially better on hard problems.\n\n\nThe architecture is the same. The way it’s trained and run is different."
  },
  {
    "objectID": "slides/01-foundations.html#what-likely-still-holds",
    "href": "slides/01-foundations.html#what-likely-still-holds",
    "title": "AI Tools for Data Science and Statistics",
    "section": "What Likely Still Holds",
    "text": "What Likely Still Holds\nEven with better models, these problems are probably not solved:\n\nBuggy context catastrophe. Models still don’t detect upstream bugs (Dinh et al. 2023)\nPoor calibration. Overconfidence is architectural, not just a training issue (Phan et al. 2025)\nSecurity vulnerabilities. No systematic re-evaluation on frontier models (Zhou et al. 2024)\nBenchmark contamination. Gets worse as training data grows (Chen, Pusarla, and Ray 2025)\nAgent self-repair at 4%. Agents still can’t fix their own mistakes (Islam et al. 2026)\n\n\nWorking assumption: ~30% of generated code still needs careful review."
  },
  {
    "objectID": "slides/01-foundations.html#today-february-2026",
    "href": "slides/01-foundations.html#today-february-2026",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Today (February 2026)",
    "text": "Today (February 2026)\n\n\n\n\nClaude Opus 4.6\nGPT-5.3-Codex\n\n\n\n\nDeveloper\nAnthropic\nOpenAI\n\n\nContext window\n200K tokens (1M beta)\n400K tokens\n\n\nPricing (input)\n$5 / 1M tokens\n$1.75 / 1M tokens\n\n\nArchitecture\nReasoning model\nReasoning (code-optimized)\n\n\n\nPricing and specs from vendor documentation as of Feb 2026; subject to change."
  },
  {
    "objectID": "slides/01-foundations.html#the-speed-of-change",
    "href": "slides/01-foundations.html#the-speed-of-change",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Speed of Change",
    "text": "The Speed of Change\nMATH benchmark progression (competition-level mathematics):\n\n\n\n\n\n\n\n\n\nYear\nModel\nMATH Score\nConditions\n\n\n\n\n2021\nGPT-2 1.5B\n6.9%\nMath-pretrained + fine-tuned (Hendrycks et al. 2021)\n\n\n2021\nGPT-3 175B\n5.2%\nFew-shot (Hendrycks et al. 2021)\n\n\n2023\nGPT-4\n42.5%\n(Fu et al. 2023)\n\n\n2025\no3-mini\n~87%\nReported\n\n\n2026\nGPT-5.3-Codex\n~96%\nReported\n\n\n\nCompetition math went from “unsolved” to “near-saturated” in 3 years."
  },
  {
    "objectID": "slides/01-foundations.html#section-3",
    "href": "slides/01-foundations.html#section-3",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "What Does the Research Say?"
  },
  {
    "objectID": "slides/01-foundations.html#where-llms-succeed",
    "href": "slides/01-foundations.html#where-llms-succeed",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Where LLMs Succeed",
    "text": "Where LLMs Succeed\n\n\n\nTask\nPerformance\nSource\n\n\n\n\nCode generation (syntactically valid)\n91.5%\n(Ni et al. 2024)\n\n\nGrade-school math (GSM8k)\n92.0%\n(Fu et al. 2023)\n\n\nAcademic knowledge (MMLU)\n86.4%\n(Fu et al. 2023)\n\n\nUndergraduate statistics\n82.85%\n(Lu et al. 2025)\n\n\nPython function synthesis (HumanEval)\n80.5%\n(Ni et al. 2024)\n\n\nData science code (DS-1000)\n43.3% (Codex)\n(Lai et al. 2023)\n\n\n\n\nPattern: LLMs excel at tasks with clear patterns, well-represented training data, and unambiguous evaluation criteria. Note the gap between generic coding benchmarks and data-science-specific ones."
  },
  {
    "objectID": "slides/01-foundations.html#where-llms-struggle",
    "href": "slides/01-foundations.html#where-llms-struggle",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Where LLMs Struggle",
    "text": "Where LLMs Struggle\n\n\n\nTask\nPerformance\nSource\n\n\n\n\nResearch-level statistics\n&lt;57%\n(Lu et al. 2025)\n\n\nExpert-level questions (HLE)\n2.7-13.4%\n(Phan et al. 2025)\n\n\nReal-world code translation\n8.1% (GPT-4)\n(Pan et al. 2024)\n\n\nCode completion with buggy context\n0.5-3.1%\n(Dinh et al. 2023)\n\n\nAgent self-repair\n4%\n(Islam et al. 2026)\n\n\n\n\nPattern: LLMs fail when tasks require genuine reasoning, when context is messy, or when there’s no clear template to follow."
  },
  {
    "objectID": "slides/01-foundations.html#the-buggy-context-problem",
    "href": "slides/01-foundations.html#the-buggy-context-problem",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Buggy Context Problem",
    "text": "The Buggy Context Problem\nA single bug in the surrounding code is catastrophic:\n\n\n\nModel\nClean Context\nBuggy Context\n\n\n\n\nInCoder-6.7B\n54.9%\n2.4%\n\n\nCodeGen-16B\n50.0%\n3.1%\n\n\nStarCoder-15B\n41.1%\n1.2%\n\n\n\n90% of failures: the model propagates the bug without reacting to it. (Dinh et al. 2023)\n\nTakeaway: Context quality &gt; model quality."
  },
  {
    "objectID": "slides/01-foundations.html#overconfidence",
    "href": "slides/01-foundations.html#overconfidence",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Overconfidence",
    "text": "Overconfidence\nLLMs are wrong frequently and act certain almost always:\n\nCalibration error on expert questions: 70-89% RMS error across all models (Phan et al. 2025)\nOn MATH, confidence is near 100% regardless of correctness (Hendrycks et al. 2021)\n29% of generated Python code contains bugs (Tambon et al. 2024)\n~40% of Copilot scenarios produced at least one vulnerable code suggestion (Pearce et al. 2022); 29.8% of snippets contain security vulnerabilities across 38 CWE categories (Zhou et al. 2024)\n\n\nIt gets worse: Users with AI assistants wrote significantly less secure code and were more confident in it (Perry et al. 2023). The overconfidence is contagious."
  },
  {
    "objectID": "slides/01-foundations.html#benchmarks-lie",
    "href": "slides/01-foundations.html#benchmarks-lie",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Benchmarks Lie",
    "text": "Benchmarks Lie\nWhen models have seen the test before, scores are ~3x too high:\nA code-generation model that had seen all the benchmark problems during training solved 68% of them. The same model, given new problems testing the same skills, solved only 22%. (Chen, Pusarla, and Ray 2025)\n\nEven uncontaminated benchmarks are too easy: Code that passes HumanEval often fails on more rigorous test suites — the benchmark rewards surface correctness, not robustness (Liu et al. 2023).\n\n\nBenchmark vs. real-world gap:\nGPT-4 code translation: 47.3% on benchmarks, 8.1% on real-world projects. (Pan et al. 2024)\n\n\nBe skeptical of headline numbers."
  },
  {
    "objectID": "slides/01-foundations.html#the-models-have-changed",
    "href": "slides/01-foundations.html#the-models-have-changed",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Models Have Changed",
    "text": "The Models Have Changed\nFrontier comparison. Literature-era best vs. February 2026:\n\n\n\n\n\n\n\n\n\nBenchmark\nBest in Papers\n2026 Frontier\nChange\n\n\n\n\nMATH (competition)\n42.5% (GPT-4) (Fu et al. 2023)\n~96% (GPT-5.3-Codex)\n+54 pts\n\n\nHLE (expert questions)\n13.4% (o3-mini) (Phan et al. 2025)\n53.1% (Opus 4.6, with tools)\n+40 pts\n\n\nHumanEval (code gen)\n80.5% (GPT-4) (Ni et al. 2024)\n~95% (Opus 4.6)\n+15 pts\n\n\nMMLU (academic)\n86.4% (GPT-4) (Fu et al. 2023)\n~93% (GPT-5.3-Codex)\n+7 pts\n\n\nSWE-bench Verified (Jimenez et al. 2024)\n–\n80.8% (Opus 4.6)\n–\n\n\n\nEnormous improvement in 2-3 years. But recall: benchmarks lie."
  },
  {
    "objectID": "slides/01-foundations.html#summary-what-the-research-shows",
    "href": "slides/01-foundations.html#summary-what-the-research-shows",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Summary: What the Research Shows",
    "text": "Summary: What the Research Shows\n\n\nLLMs are tools, not experts. They excel at patterns; they can struggle with reasoning.\n\n\n\n\nContext quality is important. One upstream bug causes catastrophic failure.\n\n\n\n\nValidate everything. ~29% bugs, ~30% security vulnerabilities, terrible calibration."
  },
  {
    "objectID": "slides/01-foundations.html#summary-what-this-means-for-you",
    "href": "slides/01-foundations.html#summary-what-this-means-for-you",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Summary: What This Means for You",
    "text": "Summary: What This Means for You\n\nPrompting matters. Few-shot examples, chain-of-thought (Wei et al. 2022), and explicit framing improve results 5-50 percentage points.\n\n\n\nModel selection matters. The gap between models is enormous (2.7% to 53.1% on expert questions; the high end is with tools).\n\n\n\n\nHuman expertise is essential. Developers who know code is AI-generated catch 13 percentage points more bugs (Tang et al. 2024)."
  },
  {
    "objectID": "slides/01-foundations.html#section-4",
    "href": "slides/01-foundations.html#section-4",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Hype, Skepticism, and Nuance"
  },
  {
    "objectID": "slides/01-foundations.html#the-bull-case",
    "href": "slides/01-foundations.html#the-bull-case",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Bull Case",
    "text": "The Bull Case\nThe optimists have real evidence:\n\nMATH benchmark: ~7% \\(\\rightarrow\\) ~96% in a few years\nSWE-bench Verified: 80.8% (Opus 4.6 fixing real GitHub issues)\nCosts dropped 10-20x in 2 years (GPT-4 at $30/M tokens \\(\\rightarrow\\) GPT-5.3-Codex at $1.75/M)\nMcKinsey: 60-70% of work activities could be automated\nGoldman Sachs: generative AI could raise global GDP by 7%"
  },
  {
    "objectID": "slides/01-foundations.html#the-bear-case",
    "href": "slides/01-foundations.html#the-bear-case",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Bear Case",
    "text": "The Bear Case\nThe skeptics also have real evidence:\n\nBenchmark contamination inflates scores ~3x (Chen, Pusarla, and Ray 2025)\n29% of generated code has bugs (Tambon et al. 2024)\n~40% of Copilot scenarios produced vulnerable suggestions (Pearce et al. 2022)\nBuggy context drops performance from 50% to 2% (Dinh et al. 2023)\nBest agent self-repair: 4% accuracy (Islam et al. 2026)\nReal-world code translation: 8.1% (vs. 47.3% on benchmarks) (Pan et al. 2024)"
  },
  {
    "objectID": "slides/01-foundations.html#the-nuance",
    "href": "slides/01-foundations.html#the-nuance",
    "title": "AI Tools for Data Science and Statistics",
    "section": "The Nuance",
    "text": "The Nuance\n\n\nGood at:\n\nFirst drafts and boilerplate\nSyntax and API lookup\nStandard patterns\nExplaining code\nBrainstorming approaches\n\n\nPoor at:\n\nJudgment calls\nSecurity-sensitive code\nMessy or buggy contexts\nNovel algorithms\nKnowing when it’s wrong\n\n\n\n“High pattern matching, low judgment.”"
  },
  {
    "objectID": "slides/01-foundations.html#biostatistics-and-the-job-market",
    "href": "slides/01-foundations.html#biostatistics-and-the-job-market",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Biostatistics and the Job Market",
    "text": "Biostatistics and the Job Market\nWhat AI can do well in our field:\n\nWrite code for data cleaning, visualization, standard analyses\nGenerate boilerplate for reports and documentation\nTranslate between programming languages\nExplain unfamiliar code or statistical concepts\n\nIn practice, productivity gains are real but modest and uneven (Ziegler et al. 2024; Liang, Yang, and Myers 2024).\n\nWhat AI cannot do:\n\nDesign a study\nInterpret clinical context\nMake judgment calls about model assumptions\nNavigate IRB requirements and data use agreements\nExplain findings to a collaborator\n\n\n\nThe job changes. It doesn’t disappear."
  },
  {
    "objectID": "slides/01-foundations.html#discussion-prompts",
    "href": "slides/01-foundations.html#discussion-prompts",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Discussion Prompts",
    "text": "Discussion Prompts\n\nWould you trust AI-generated code in a clinical trial analysis? Under what conditions?\nIf an LLM writes 70% of a methods section, who is the author?\nShould researchers be required to disclose AI tool use? At what level of detail? (see Hosseini, Resnik, and Holmes 2023; Liao and Vaughan 2024)\nLLMs are trained on public code and text, often without consent. Is this ethical? Does it matter if the model is open-source vs. proprietary? (see Weidinger et al. 2022)\nIf AI tools make data analysis faster and cheaper, what happens to the value of your degree?"
  },
  {
    "objectID": "slides/01-foundations.html#whats-next",
    "href": "slides/01-foundations.html#whats-next",
    "title": "AI Tools for Data Science and Statistics",
    "section": "What’s Next",
    "text": "What’s Next\nToday’s hands-on exercise: Everyone gets the same statistical task. Use your own tool. We’ll compare results.\nMoonshot assignment: Brainstorm a task that feels beyond your current ability. We’ll work on it throughout the course.\nHomework:\n\nTry a second AI tool you haven’t used before for a small task\nNote the differences from your primary tool\nStart thinking about your moonshot\n\n\n\n\n\nChen, Simin, Pranav Pusarla, and Baishakhi Ray. 2025. “DyCodeEval: Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination.” In Proceedings of the 42nd International Conference on Machine Learning (ICML). Vol. 267. PMLR. https://arxiv.org/abs/2503.04149.\n\n\nDinh, Tuan, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, and George Karypis. 2023. “Large Language Models of Code Fail at Completing Code with Potential Bugs.” In Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS). https://arxiv.org/abs/2306.03438.\n\n\nDreyfus, Hubert L. 1972. What Computers Can’t Do: A Critique of Artificial Reason. New York: Harper & Row.\n\n\n———. 1992. What Computers Still Can’t Do: A Critique of Artificial Reason. Cambridge, MA: MIT Press.\n\n\nFu, Yao, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. 2023. “Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models’ Reasoning Performance.” https://arxiv.org/abs/2305.17306.\n\n\nHaugeland, John. 1985. Artificial Intelligence: The Very Idea. Cambridge, MA: MIT Press.\n\n\nHendrycks, Dan, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. “Measuring Mathematical Problem Solving with the MATH Dataset.” In Proceedings of the 35th Conference on Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track. https://arxiv.org/abs/2103.03874.\n\n\nHosseini, Mohammad, David B. Resnik, and Kristi Holmes. 2023. “The Ethics of Disclosing the Use of Artificial Intelligence Tools in Writing Scholarly Manuscripts.” Research Ethics 19 (4): 449–65. https://doi.org/10.1177/17470161231180449.\n\n\nIslam, Niful, Ragib Shahariar Ayon, Deepak George Thomas, Shibbir Ahmed, and Mohammad Wardat. 2026. “When Agents Fail: A Comprehensive Study of Bugs in LLM Agents with Automated Labeling.” https://arxiv.org/abs/2601.15232.\n\n\nJimenez, Carlos E., John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. 2024. “SWE-Bench: Can Language Models Resolve Real-World GitHub Issues?” In International Conference on Learning Representations (ICLR 2024). https://arxiv.org/abs/2310.06770.\n\n\nKambhampati, Subbarao, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Siddarth, Anil Garg, and Raghav Mangla. 2024. “Position: LLMs Can’t Plan, but Can Help Planning in LLM-Modulo Frameworks.” In Proceedings of the 41st International Conference on Machine Learning (ICML 2024), 235:22895–907. PMLR. https://arxiv.org/abs/2402.01817.\n\n\nLai, Yuhang, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2023. “DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.” In Proceedings of the 40th International Conference on Machine Learning (ICML 2023), 202:18319–45. PMLR. https://arxiv.org/abs/2211.11501.\n\n\nLiang, Jenny T., Chenyang Yang, and Brad A. Myers. 2024. “A Large-Scale Survey on the Usability of AI Programming Assistants: Successes and Challenges.” In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering (ICSE ’24). https://doi.org/10.1145/3597503.3608128.\n\n\nLiao, Q. Vera, and Jennifer Wortman Vaughan. 2024. “AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap.” Harvard Data Science Review, no. Special Issue 5. https://doi.org/10.1162/99608f92.8036d03b.\n\n\nLiu, Jiawei, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. “Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation.” In Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS). https://arxiv.org/abs/2305.01210.\n\n\nLu, Yuchen, Run Yang, Yichen Zhang, Shuguang Yu, Runpeng Dai, Ziwei Wang, Jiayi Xiang, et al. 2025. “StatEval: A Comprehensive Benchmark for Large Language Models in Statistics.” https://arxiv.org/abs/2510.09517.\n\n\nNi, Ansong, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, et al. 2024. “L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models.” https://arxiv.org/abs/2309.17446.\n\n\nPan, Rangeet, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pougeum Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. 2024. “Lost in Translation: A Study of Bugs Introduced by Large Language Models While Translating Code.” In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering (ICSE ’24). Lisbon, Portugal. https://doi.org/10.1145/3597503.3639226.\n\n\nPearce, Hammond, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. 2022. “Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions.” In IEEE Symposium on Security and Privacy (s&p 2022), 754–68. https://doi.org/10.1109/SP46214.2022.9833571.\n\n\nPerry, Neil, Megha Srivastava, Deepak Kumar, and Dan Boneh. 2023. “Do Users Write More Insecure Code with AI Assistants?” In ACM Conference on Computer and Communications Security (CCS 2023). https://doi.org/10.1145/3576915.3623157.\n\n\nPhan, Long, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, et al. 2025. “Humanity’s Last Exam.” Nature. https://doi.org/10.1038/s41586-025-09962-4.\n\n\nSearle, John R. 1980. “Minds, Brains, and Programs.” Behavioral and Brain Sciences 3 (3): 417–24. https://doi.org/10.1017/S0140525X00005756.\n\n\nTambon, Florian, Arghavan Moradi Dakhel, Amin Nikanjam, Foutse Khomh, Michel C. Desmarais, and Giuliano Antoniol. 2024. “Bugs in Large Language Models Generated Code: An Empirical Study.” https://arxiv.org/abs/2403.08937.\n\n\nTang, Ningzhi, Meng Chen, Zheng Ning, Aakash Bansal, Yu Huang, Collin McMillan, and Toby Jia-Jun Li. 2024. “A Study on Developer Behaviors for Validating and Repairing LLM-Generated Code Using Eye Tracking and IDE Actions.” https://arxiv.org/abs/2405.16081.\n\n\nValmeekam, Karthik, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. 2023. “On the Planning Abilities of Large Language Models — a Critical Investigation.” In Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023). https://arxiv.org/abs/2305.15771.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Advances in Neural Information Processing Systems 30 (NeurIPS).\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V. Le, and Denny Zhou. 2022. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” In Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS 2022). https://arxiv.org/abs/2201.11903.\n\n\nWeidinger, Laura, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, et al. 2022. “Taxonomy of Risks Posed by Language Models.” In ACM Conference on Fairness, Accountability, and Transparency (FAccT 2022). https://doi.org/10.1145/3531146.3533088.\n\n\nWittgenstein, Ludwig. 1953. Philosophical Investigations. Oxford: Blackwell.\n\n\nZhou, Xiyu, Peng Liang, Beiqi Zhang, Zengyang Li, Aakash Ahmad, Mojtaba Shahin, and Muhammad Waseem. 2024. “Exploring the Problems, Their Causes and Solutions of AI Pair Programming: A Study on GitHub and Stack Overflow.” Journal of Systems and Software 219: 112204. https://doi.org/10.1016/j.jss.2024.112204.\n\n\nZiegler, Albert, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2024. “Measuring GitHub Copilot’s Impact on Productivity.” Communications of the ACM 67 (3): 54–63. https://doi.org/10.1145/3633453."
  },
  {
    "objectID": "course_docs/syllabus.html",
    "href": "course_docs/syllabus.html",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Erik Westlund, PhD\nDepartment of Biostatistics\nJohns Hopkins Bloomberg School of Public Health\newestlund@jhu.edu"
  },
  {
    "objectID": "course_docs/syllabus.html#instructor",
    "href": "course_docs/syllabus.html#instructor",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Erik Westlund, PhD\nDepartment of Biostatistics\nJohns Hopkins Bloomberg School of Public Health\newestlund@jhu.edu"
  },
  {
    "objectID": "course_docs/syllabus.html#course-description",
    "href": "course_docs/syllabus.html#course-description",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Course Description",
    "text": "Course Description\nAs AI tools are rapidly adopted across research and industry, there is a growing need for statisticians and data scientists to understand what these tools are capable of and how to use them responsibly. This course provides practical approaches for integrating large language models and agent-based tools into statistical workflows. Students learn how to structure AI-assisted processes for analysis, simulation, and pipeline development, along with core skills in context management and agent orchestration. The course emphasizes AI safety, privacy, and responsible handling of sensitive data. Students will discuss these topics and do hands-on exercises to test the strengths and weaknesses of AI tools in practice."
  },
  {
    "objectID": "course_docs/syllabus.html#course-details",
    "href": "course_docs/syllabus.html#course-details",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Course Details",
    "text": "Course Details\n\n\n\nDates\nFebruary 19, February 26, March 5, March 12, 2026\n\n\nTime\n9:00–10:20 AM ET\n\n\nLocation\nZoom (link provided on CoursePlus/Email)\n\n\nFormat\nLecture, live demos, discussion, hands-on exercises\n\n\nGrading\nPass/Fail"
  },
  {
    "objectID": "course_docs/syllabus.html#course-learning-objectives",
    "href": "course_docs/syllabus.html#course-learning-objectives",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nBy the end of this course, students will be able to:\n\nDescribe how large language models work at a conceptual level, including their capabilities and limitations for statistical work.\nEvaluate the ethical implications of AI tool use in research, including privacy, bias, reproducibility, and academic integrity.\nUse code to work with data, not the LLM itself, to protect sensitive and regulated data (including PHI) in privacy-sensitive contexts.\nNavigate the landscape of AI tools — chat interfaces, IDE integrations, CLI agents, and supporting tools — and select appropriate tools for different tasks.\nUse AI assistants to write, debug, and audit code for data cleaning, visualization, and statistical analysis.\nBuild analysis workflows on synthetic data and deploy validated code to secure data environments using Git.\nCritically assess AI-generated output and identify errors, hallucinations, and inappropriate statistical choices."
  },
  {
    "objectID": "course_docs/syllabus.html#schedule",
    "href": "course_docs/syllabus.html#schedule",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Schedule",
    "text": "Schedule\n\nSession 1 (February 19): Foundations: History, Today, and Ethics\nWe will share our experiences using AI and study the foundations of AI tools. We will discuss ethics and concerns people have with using the tools.\n\n\nSession 2 (February 26): The Toolbox\nWe will introduce and demonstrate various tools available to us, including major models (Claude, GPT, Gemini), open source models, chat tools, IDEs (VS Code, Positron, Cursor), CLI tools (Claude Code, Codex, OpenCode), and supporting utilities (tmux, Git).\n\nDue before class: Email course project topic to ewestlund@jhu.edu\n\n\n\nSession 3 (March 5): AI-Assisted Statistical Workflows\nWe will discuss how to use these tools to summarize datasets, generate synthetic/simulated data, and build models. We will pay special attention to how to work with PHI using these tools.\n\nDue: Make progress on course project; prepare for check-in\n\n\n\nSession 4 (March 12): Synthesis and Looking Forward\nWe will spend most of this session sharing our experiences with our projects, but will leave it open to address lingering questions.\n\nDue: Course project materials and 2–3 page reflection; in-class share"
  },
  {
    "objectID": "course_docs/syllabus.html#course-project",
    "href": "course_docs/syllabus.html#course-project",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Course Project",
    "text": "Course Project\nEach student will attempt an ambitious task using AI tools — something you know little about, or that feels beyond your current capability. The goal is not to produce publication-quality work. It is to explore the limits and possibilities of these tools in a low-stakes way and to report honestly on what happened. See the course project page for full details.\n\n\n\nDate\nMilestone\n\n\n\n\nFebruary 19 (Session 1)\nAssignment introduced; begin brainstorming\n\n\nFebruary 26 (Session 2)\nTopic proposal due via email\n\n\nMarch 5 (Session 3)\nProgress check-in during class\n\n\nMarch 12 (Session 4)\nProject and reflection due; in-class share"
  },
  {
    "objectID": "course_docs/syllabus.html#tools-and-subscriptions",
    "href": "course_docs/syllabus.html#tools-and-subscriptions",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Tools and Subscriptions",
    "text": "Tools and Subscriptions\nStudents should subscribe to at least one AI tool for the duration of the course. Options include:\n\n\n\nTool\nCost\n\n\n\n\nOpenAI ChatGPT Plus\n$20/month\n\n\nAnthropic Claude Pro\n$20/month\n\n\nGoogle Gemini Advanced\n$20/month (one month free trial)\n\n\nCursor Pro\n$20/month\n\n\nGitHub Copilot Pro\n$10/month\n\n\n\nVariety across the class is encouraged — we will compare how different tools handle the same problems. If financing is a concern, please reach out to the instructor. Google Gemini Advanced offers a free trial that can cover the course period.\nStudents should also ensure that Git is installed and a GitHub account is registered before the first class. Mac and Linux typically have Git pre-installed. Otherwise, follow directions at git-scm.com."
  },
  {
    "objectID": "course_docs/syllabus.html#course-materials",
    "href": "course_docs/syllabus.html#course-materials",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Course Materials",
    "text": "Course Materials\nThere is no required textbook. Course readings and further reading are listed on the course website."
  },
  {
    "objectID": "course_docs/syllabus.html#methods-of-assessment",
    "href": "course_docs/syllabus.html#methods-of-assessment",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Methods of Assessment",
    "text": "Methods of Assessment\nThis is a pass/fail course. Assessment is based on:\n\nParticipation (50%): Attend all sessions and engage actively in discussions and hands-on exercises.\nCourse project (50%): Attempt your project and present your experience to the class in Session 4."
  },
  {
    "objectID": "course_docs/syllabus.html#generative-ai-policy",
    "href": "course_docs/syllabus.html#generative-ai-policy",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Generative AI Policy",
    "text": "Generative AI Policy\nUsing AI tools is the subject of this course. Their use is permitted, encouraged, and expected. It is nevertheless the student’s responsibility to understand the output of these tools and ensure their correctness. Students are strongly encouraged to approach these tools as learning aids and not crutches."
  },
  {
    "objectID": "course_docs/syllabus.html#academic-ethics",
    "href": "course_docs/syllabus.html#academic-ethics",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Academic Ethics",
    "text": "Academic Ethics\nStudents enrolled in the Bloomberg School of Public Health of The Johns Hopkins University assume an obligation to conduct themselves in a manner appropriate to the University’s mission as an institution of higher education. Students should be familiar with the policies and procedures specified under Policy and Procedure Manual Student-01 (Academic Ethics) and the Student Conduct Code (Student-06), available at my.publichealth.jhu.edu."
  },
  {
    "objectID": "course_docs/syllabus.html#student-health-and-well-being",
    "href": "course_docs/syllabus.html#student-health-and-well-being",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Student Health and Well-being",
    "text": "Student Health and Well-being\nIf you are struggling with anxiety, stress, depression, or other mental health related concerns, please consider connecting with resources:\n\nStudent support: bit.ly/bsphstudentsupport\nMental Health Services: wellbeing.jhu.edu/MentalHealthServices\nBehavioral Health Crisis Support Team (24/7): 410-516-9355"
  },
  {
    "objectID": "course_docs/syllabus.html#disability-accommodations",
    "href": "course_docs/syllabus.html#disability-accommodations",
    "title": "Advanced Topics in Biostatistics: AI Tools for Data Science and Statistics",
    "section": "Disability Accommodations",
    "text": "Disability Accommodations\nStudent Disability Services (SDS) provides accessible and inclusive educational experiences for students with disabilities. To request accommodations:\n\nComplete the SDS online application via AIM\nSubmit documentation using the provided link after application submission\nSchedule a meeting with Audrey Ndaba\n\nMore information: Student Disability Services"
  },
  {
    "objectID": "course_docs/course-project.html",
    "href": "course_docs/course-project.html",
    "title": "Course Project",
    "section": "",
    "text": "The only assignment for this course to attempt something ambitious with AI tools and report honestly on what happened.\nPick a task that feels beyond your current ability, or in a domain you know little about, and use AI tools to make a serious attempt at it. You do not need to write a single line of code yourself. You can let the AI write all of it. In fact, I would encourage you to see how far you can go. What I want to see is the attempt, the reflection, and the honest accounting of what worked and what didn’t."
  },
  {
    "objectID": "course_docs/course-project.html#overview",
    "href": "course_docs/course-project.html#overview",
    "title": "Course Project",
    "section": "",
    "text": "The only assignment for this course to attempt something ambitious with AI tools and report honestly on what happened.\nPick a task that feels beyond your current ability, or in a domain you know little about, and use AI tools to make a serious attempt at it. You do not need to write a single line of code yourself. You can let the AI write all of it. In fact, I would encourage you to see how far you can go. What I want to see is the attempt, the reflection, and the honest accounting of what worked and what didn’t."
  },
  {
    "objectID": "course_docs/course-project.html#deliverables",
    "href": "course_docs/course-project.html#deliverables",
    "title": "Course Project",
    "section": "Deliverables",
    "text": "Deliverables\n\n1. Topic proposal\nDue: February 26 (before Session 2)\nEmail a short description of your proposed moonshot task (a few sentences is fine) to ewestlund@jhu.edu.\n\n\n2. Project and reflection\nDue: March 12 (Session 4)\nSubmit your project materials and a written reflection.\nProject materials: Share via a GitHub repository (preferred) or a zip file emailed to ewestlund@jhu.edu. Include whatever you produced — code, output, data, notebooks, etc. The project does not need to be finished.\nWritten reflection (2–3 pages): Your reflection must address:\n\nWhat you attempted. Describe the task and why you chose it.\nWhat worked and what didn’t. Be specific. Which parts did AI tools handle well? Where did they fail, mislead, or produce something you couldn’t use?\nHow you checked your work. Explain how you verified that AI-generated output was correct. What strategies did you use? What did you catch? What might you have missed?\n\n\n\n3. In-class share (Session 4)\nDuring Session 4, each student will give an informal 3-minute summary of their moonshot to the class. No slides required. We will discuss as a group after each share.\nThe project does not need to be “finished” by Session 4. Incomplete attempts with honest reflection are perfectly acceptable."
  },
  {
    "objectID": "course_docs/course-project.html#guidelines",
    "href": "course_docs/course-project.html#guidelines",
    "title": "Course Project",
    "section": "Guidelines",
    "text": "Guidelines\n\nNo code writing required. You may let AI tools generate all code. The point is to explore what the tools can and cannot do, not to demonstrate your own coding ability.\nHonest reporting matters more than success. A failed attempt with a thoughtful reflection is worth more than a polished result with no critical analysis.\nUse any AI tools you want. ChatGPT, Claude, Copilot, Cursor, Claude Code, or anything else. You may use multiple tools and compare them.\nDocument your process. Save chat logs, screenshots, or session histories where possible. These can support your reflection but are not required as part of the submission."
  },
  {
    "objectID": "course_docs/course-project.html#timeline",
    "href": "course_docs/course-project.html#timeline",
    "title": "Course Project",
    "section": "Timeline",
    "text": "Timeline\n\n\n\nDate\nMilestone\n\n\n\n\nFebruary 19 (Session 1)\nAssignment introduced; begin brainstorming\n\n\nFebruary 26 (Session 2)\nTopic proposal due via email\n\n\nMarch 5 (Session 3)\nProgress check-in during class\n\n\nMarch 12 (Session 4)\nProject and reflection due; in-class share"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Tools for Data Science and Statistics",
    "section": "",
    "text": "Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health\nInstructor: Erik Westlund, PhD (ewestlund@jhu.edu)\nDates: February 19, February 26, March 5, March 12, 2026\nTime: 9:00–10:20 AM ET"
  },
  {
    "objectID": "index.html#sessions",
    "href": "index.html#sessions",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Sessions",
    "text": "Sessions\n\nSession 1 (February 19): Foundations: History, Today, and Ethics\nWe will share our experiences using AI and study the foundations of AI tools. We will discuss ethics/concerns people have with using the tools.\n\nSlides\n\n\n\nSession 2 (February 26): The Toolbox\nWe will introduce and demonstrate various tools available to us, including:\n\nMajor models: Claude Opus/Sonnet, Open AI ChatGPT/Codex, Google Gemini\nOpen source models: there are many\nTools: Chat tools (Claude, ChatGPT, Gemini), IDEs (e.g., VS Code, Positron, Cursor, etc.), CLI tools/harnesses (e.g., Claude Code, OpenAI Codex, OpenCode), and software utilities to improve your workflow (tmux, git)\nSlides [coming soon]\nDue before class: Email course project topic to ewestlund@jhu.edu\n\n\n\nSession 3 (March 5): AI-Assisted Statistical Workflows\nWe will discuss how to use these tools to:\n\nSummarize datasets\nGenerate synthetic/simulated data\nBuild models\n\nWe will pay special attention to how to work with PHI using these tools\n\nSlides [coming soon]\nDue: Make progress on moonshot; prepare for check-in\n\n\n\nSession 4 (March 12): Synthesis and Looking Forward\nWe will spend most of this session sharing our experiences with our projects, but will leave it open to address lingering questions.\n\nSlides [coming soon]\nDue: Course project: project materials and 2–3 page reflection; in-class share"
  },
  {
    "objectID": "index.html#course-readings",
    "href": "index.html#course-readings",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Course Readings",
    "text": "Course Readings\nWhile I am not requiring you to read these papers, I am providing them for reference. The slides often reference them using the identifying numbers below. The concepts/problems these papers investigate and discuss are still relevant. Nevertheless, both the speed at which LLM technology is advancing as well the artificiality of many of the benchmarks should lead you to interpret presented data with caution.\n\nPublished\n\nNi, A., Yin, P., Zhao, Y., Riddell, M., Feng, T., Shen, R., Yin, S., Liu, Y., Yavuz, S., Xiong, C., Joty, S., Zhou, Y., Radev, D., & Cohan, A. (2024). L2CEval: Evaluating language-to-code generation capabilities of large language models. Transactions of the Association for Computational Linguistics, 12, 1311–1329. doi:10.1162/tacl_a_00705\nJiang, J., Wang, F., Shen, J., Kim, S., & Kim, S. (2025). A survey on large language models for code generation. ACM Transactions on Software Engineering and Methodology. doi:10.1145/3747588\nChen, S., Pusarla, P., & Ray, B. (2025). DyCodeEval: Dynamic benchmarking of reasoning capabilities in code large language models under data contamination. Proceedings of the 42nd International Conference on Machine Learning (ICML), PMLR 267, 8890–8909. proceedings.mlr.press\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., & Steinhardt, J. (2021). Measuring mathematical problem solving with the MATH dataset. Proceedings of NeurIPS 2021, Datasets and Benchmarks Track. openreview.net\nPhan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Zhang, C. B. C., Shaaban, M., Ling, J., Shi, S., Choi, M., Agrawal, A., Chopra, A., Khoja, A., Kim, R., Ren, R., Hausenloy, J., Zhang, O., Mazeika, M., Yue, S., Wang, A., & Hendrycks, D. (2025). Humanity’s Last Exam. Nature. doi:10.1038/s41586-025-09962-4\nTambon, F., Moradi Dakhel, A., Nikanjam, A., Khomh, F., Desmarais, M. C., & Antoniol, G. (2025). Bugs in large language models generated code: An empirical study. Empirical Software Engineering, 30(3). doi:10.1007/s10664-025-10614-4\nPan, R., Ibrahimzada, A. R., Krishna, R., Sankar, D., Pougeum Wassi, L., Merler, M., Sobolev, B., Pavuluri, R., Sinha, S., & Jabbarvand, R. (2024). Lost in translation: A study of bugs introduced by large language models while translating code. Proceedings of ICSE ’24. doi:10.1145/3597503.3639226\nTang, N., Chen, M., Ning, Z., Bansal, A., Huang, Y., McMillan, C., & Li, T. J.-J. (2024). Developer behaviors in validating and repairing LLM-generated code using eye tracking and IDE actions. IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2024), 40–46. doi:10.1109/VL/HCC60511.2024.00015\nZhou, X., Liang, P., Zhang, B., Li, Z., Ahmad, A., Shahin, M., & Waseem, M. (2024). Exploring the problems, their causes and solutions of AI pair programming: A study on GitHub and Stack Overflow. Journal of Systems and Software, 219, 112204. doi:10.1016/j.jss.2024.112204\nTang, L., Liu, J., Liu, Z., Yang, X., & Bao, L. (2025). LLM4SZZ: Enhancing SZZ algorithm with context-enhanced assessment on large language models. Proceedings of the ACM on Software Engineering, 2(ISSTA), 343–365. doi:10.1145/3728885\nDinh, T., Zhao, J., Tan, S., Negrinho, R., Lausen, L., Zha, S., & Karypis, G. (2023). Large language models of code fail at completing code with potential bugs. Proceedings of NeurIPS 2023. proceedings.neurips.cc\n\n\n\nPreprints\n\nFu, L., Chai, H., Du, K., Zhang, W., Luo, S., Lin, J., Fang, Y., Rui, R., Guan, H., Liu, J., Qi, S., Fan, L., Lei, J., Liu, Y., Wang, J., Zhang, K., Zhang, W., & Yu, Y. (2024). CodeApex: A bilingual programming evaluation benchmark for large language models. arXiv:2309.01940\nLu, Y., Yang, R., Zhang, Y., Yu, S., Dai, R., Wang, Z., Xiang, J., E, W., Gao, S., Ruan, X., Huang, Y., Xi, C., Hu, H., Fu, Y., Yu, Q., Wei, X., Gu, J., Sun, R., Jia, J., & Zhou, F. (2025). StatEval: A comprehensive benchmark for large language models in statistics. arXiv:2510.09517\nFu, Y., Ou, L., Chen, M., Wan, Y., Peng, H., & Khot, T. (2023). Chain-of-Thought Hub: A continuous effort to measure large language models’ reasoning performance. arXiv:2305.17306\nOpu, M. N. I., Wang, S., & Chowdhury, S. (2025). LLM-based detection of tangled code changes for higher-quality method-level bug datasets. arXiv:2505.08263\nIslam, N., Ayon, R. S., Thomas, D. G., Ahmed, S., & Wardat, M. (2026). When agents fail: A comprehensive study of bugs in LLM agents with automated labeling. arXiv:2601.15232\nGloaguen, T., Mundler, N., Muller, M., Raychev, V., & Vechev, M. (2026). Evaluating AGENTS.md: Are repository-level context files helpful for coding agents? arXiv:2602.11988"
  },
  {
    "objectID": "index.html#further-reading",
    "href": "index.html#further-reading",
    "title": "AI Tools for Data Science and Statistics",
    "section": "Further Reading",
    "text": "Further Reading\n\nCode Quality, Bugs, and Security\n\nPearce, H., Ahmad, B., Tan, B., Dolan-Gavitt, B., & Karri, R. (2022). Asleep at the keyboard? Assessing the security of GitHub Copilot’s code contributions. IEEE Symposium on Security and Privacy (S&P 2022), 754–768. doi:10.1109/SP46214.2022.9833571\nPerry, N., Srivastava, M., Kumar, D., & Boneh, D. (2023). Do users write more insecure code with AI assistants? ACM Conference on Computer and Communications Security (CCS 2023). doi:10.1145/3576915.3623157\nSandoval, G., Pearce, H., Nys, T., Karri, R., Garg, S., & Dolan-Gavitt, B. (2023). Lost at C: A user study on the security implications of large language model code assistants. USENIX Security Symposium 2023, 2205–2222. usenix.org\nJesse, K., Ahmed, T., Devanbu, P., & Morgan, E. (2023). Large language models and simple, stupid bugs. IEEE/ACM 20th International Conference on Mining Software Repositories (MSR 2023), 563–575. doi:10.1109/MSR59073.2023.00082\nAsare, O., Nagappan, M., & Asokan, N. (2023). Is GitHub’s Copilot as bad as humans at introducing vulnerabilities in code? Empirical Software Engineering, 28, 129. doi:10.1007/s10664-023-10380-1\nLiu, J., Xia, C. S., Wang, Y., & Zhang, L. (2023). Is your code generated by ChatGPT really correct? Rigorous evaluation of large language models for code generation. Proceedings of NeurIPS 2023. proceedings.neurips.cc\nDu, X., Liu, M., Wang, K., Wang, H., et al. (2024). Evaluating large language models in class-level code generation. Proceedings of ICSE 2024. doi:10.1145/3597503.3639219\n\n\n\nAI-Assisted Programming\n\nVaithilingam, P., Zhang, T., & Glassman, E. L. (2022). Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. CHI Conference on Human Factors in Computing Systems Extended Abstracts (CHI EA ’22). doi:10.1145/3491101.3519665\nNguyen, N. & Nadi, S. (2022). An empirical evaluation of GitHub Copilot’s code suggestions. 19th International Conference on Mining Software Repositories (MSR ’22). doi:10.1145/3524842.3528470\nDakhel, A. M., Majdinasab, V., Nikanjam, A., Khomh, F., Desmarais, M. C., & Jiang, Z. M. J. (2023). GitHub Copilot AI pair programmer: Asset or liability? Journal of Systems and Software, 203, 111734. doi:10.1016/j.jss.2023.111734\nBarke, S., James, M. B., & Polikarpova, N. (2023). Grounded Copilot: How programmers interact with code-generating models. Proceedings of the ACM on Programming Languages (OOPSLA), 7(OOPSLA1), 85–111. doi:10.1145/3586030\nZiegler, A., Kalliamvakou, E., Li, X. A., Rice, A., et al. (2024). Measuring GitHub Copilot’s impact on productivity. Communications of the ACM, 67(3), 54–63. doi:10.1145/3633453\nLiang, J. T., Yang, C., & Myers, B. A. (2024). A large-scale survey on the usability of AI programming assistants: Successes and challenges. Proceedings of ICSE 2024. doi:10.1145/3597503.3608128\nMurali, V., Maddila, C., Ahmad, I., Bolin, M., et al. (2024). AI-assisted code authoring at scale: Fine-tuning, deploying, and mixed methods evaluation. Proceedings of the ACM on Software Engineering (PACMSE/FSE), 1(FSE), 1066–1085. doi:10.1145/3643774\n\n\n\nBenchmarks and Evaluation\n\nJimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., & Narasimhan, K. R. (2024). SWE-bench: Can language models resolve real-world GitHub issues? ICLR 2024 (Oral). openreview.net\nLai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, W., Fried, D., Wang, S., & Yu, T. (2023). DS-1000: A natural and reliable benchmark for data science code generation. Proceedings of ICML 2023, PMLR 202, 18319–18345. proceedings.mlr.press\n\n\n\nReasoning Capabilities and Limitations\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q. V., & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. Proceedings of NeurIPS 2022. proceedings.neurips.cc\nValmeekam, K., Marquez, M., Sreedharan, S., & Kambhampati, S. (2023). On the planning abilities of large language models — a critical investigation. Proceedings of NeurIPS 2023 (Spotlight). proceedings.neurips.cc\nKambhampati, S., Valmeekam, K., Guan, L., Verma, M., Stechly, K., et al. (2024). Position: LLMs can’t plan, but can help planning in LLM-Modulo frameworks. Proceedings of ICML 2024 (Spotlight), PMLR 235, 22895–22907. proceedings.mlr.press\n\n\n\nEthics and Responsible AI\n\nWeidinger, L., Mellor, J., Rauh, M., Griffin, C., et al. (2022). Taxonomy of risks posed by language models. ACM Conference on Fairness, Accountability, and Transparency (FAccT 2022). doi:10.1145/3531146.3533088\nHosseini, M., Resnik, D. B., & Holmes, K. (2023). The ethics of disclosing the use of artificial intelligence tools in writing scholarly manuscripts. Research Ethics, 19(4), 449–465. doi:10.1177/17470161231180449\nLiao, Q. V. & Vaughan, J. W. (2024). AI transparency in the age of LLMs: A human-centered research roadmap. Harvard Data Science Review, Special Issue 5. doi:10.1162/99608f92.8036d03b\n\n\n\nHuman Factors in AI-Assisted Coding\n\nMozannar, H., Bansal, G., Fourney, A., & Horvitz, E. (2024). Reading between the lines: Modeling user behavior and costs in AI-assisted programming. CHI 2024 (Honorable Mention). doi:10.1145/3613904.3641936\nFerdowsi, K., Huang, R., James, M. B., Polikarpova, N., & Lerner, S. (2024). Validating AI-generated code with live programming. CHI 2024. doi:10.1145/3613904.3642495"
  }
]