---
title: "AI Tools for Data Science and Statistics"
subtitle: "Context Management and AI Workflows"
author: "Erik Westlund"
date: "March 5, 2026"
bibliography: references.bib
format:
  revealjs:
    slide-number: true
    transition: slide
    background-transition: fade
    self-contained: true
    css: slides.css
---

```{r}
#| include: false
if (requireNamespace("framework", quietly = TRUE)) {
  framework::scaffold()
} else {
  if(file.exists("../scaffold.R")) source("../scaffold.R")
}
```

DRAFT!

## Follow Along on GitHub

All the code, datasets, and examples for this session are available on GitHub.

- **Follow along:** You can clone or pull down the repository to run the examples yourself.
- **Reproducible:** All workflows and agent tasks shown today are in the repo.
- **Experiment:** Feel free to run the agent prompts on your own machine.

## {background-color="#2c3e50"}

::: {style="color: white; font-size: 1.5em; text-align: center;"}
Context Management
:::

::: {style="color: #ecf0f1; font-size: 0.9em; text-align: center;"}
Structuring Prompts and Context Files
:::

## The Importance of Context

An LLM is a reasoning engine, but without context, it operates in a vacuum.

- **Under-contextualized:** "Clean this data and run a model." (Model guesses what clean means, guesses the model).
- **Over-contextualized:** "Here is 50 pages of our entire codebase and every README we have." (Model loses focus, token costs explode).

**Goal:** Provide exactly enough context to frame the problem and set constraints, and no more.

## What is a Context File?

Context files (`AGENTS.md`, `CLAUDE.md`, `.cursorrules`) are markdown files placed in the root of your project directory. 

They provide "always-on" instructions to AI agents about:
- Your coding style and conventions (e.g., "Use snake_case and dplyr").
- How to run tests or build the project.
- Specific domain constraints (e.g., "Do not interpret correlations as causal").

When an agent loads your repository, it automatically reads these rules.

## Do Context Files Actually Work?

*Gloaguen et al. (2025): "Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?"*

Tested AI agents on 138 real-world GitHub issues using popular LLMs with and without context files.

**The Findings:**
- LLM-generated context files **reduced** task success rates.
- Developer-provided context files only marginally improved performance (+4%).
- Context files **increased** inference costs by over 20%.

## Why Do They Backfire?

Why does giving the agent more instructions make it perform worse?

- **Over-exploration:** Agents followed context file instructions to test and search extensively, spending more steps traversing files instead of solving the problem.
- **Increased Reasoning Burden:** Adding complex instructions required the LLMs to spend 10-22% more "reasoning tokens" per task.
- **Redundancy:** LLM-generated context files often repeated existing documentation, adding noise without signal.

## Summary

If context files make tasks harder, how should we use them?

**Keep them minimal and constraint-focused:**
1. State your primary toolchain ("Use R, tidyverse, and ggplot2").
2. State absolute constraints ("Never run git commit without asking").
3. State interpretation rules ("Default to descriptive language; do not make causal claims").

Do not try to explain your entire data pipeline in `CLAUDE.md`. Let the agent read the specific code it needs.

## Good vs. Bad Prompts for Stats

**Bad Prompt:**
"Look at my dataset and tell me what the relationship is between age and blood pressure."

**Good Prompt:**
"Using `data/clinical.csv`, fit a linear regression of systolic blood pressure on age, controlling for BMI. Check for heteroskedasticity. Return the model summary table using `gtsummary` and describe the age coefficient descriptively without causal language."

*Specificity prevents hallucinations.*

## {background-color="#2c3e50"}

::: {style="color: white; font-size: 1.5em; text-align: center;"}
The Code-Not-Data Principle
:::

::: {style="color: #ecf0f1; font-size: 0.9em; text-align: center;"}
Bridging AI and Secure Environments
:::

## The PHI Problem

In biostatistics and clinical research, **Protected Health Information (PHI)** is heavily restricted. 

- You **cannot** upload PHI to ChatGPT.
- You **cannot** let Claude Code read a folder containing identifiable patient data.
- Local models are safer, but often lack the reasoning capability of frontier cloud models for complex logic.

How do we use frontier AI tools without leaking data?

## The Code-Not-Data Principle

**Send the schema, not the rows.**

If the AI knows the structure of the data, it can write the code to analyze it, without ever seeing the actual observations.

- Export a codebook or synthetic schema.
- Use the AI in a safe environment to build the analytical pipeline.
- Move the **code** back into the restricted environment to run on the real data.

## Synthetic Data as a Strategy

Synthetic data accelerates the Code-Not-Data loop:

1. Look at your real data (inside the secure enclave).
2. Generate a synthetic dataset with the exact same column names, types, and rough distributions.
3. Move the synthetic dataset to your laptop.
4. Let the AI agent freely write cleaning, EDA, and modeling scripts against the synthetic data.
5. Review the code.
6. Push the code to Git, pull it inside the secure enclave, and run it on the real data.

## Git as a Bridge

Git is the perfect transport mechanism for this workflow.

- It tracks provenance (who wrote this code?).
- It provides a clear audit trail (what did the AI change?).
- It enforces a strict boundary: you commit and push `.R` scripts, while `data/` is strictly `.gitignore`'d.

This cleanly separates the **untrusted reasoning environment** from the **secure execution environment.**

## Hands-On: Secure Workflow Demo

We will now demonstrate this loop:

1. We have a synthetic dataset: `data/synthetic/simulated_maternal_health_data.csv`.
2. We will use an AI agent to build a data cleaning and visualization pipeline.
3. We will enforce strict context rules and iterate.
4. We will audit the output to catch where the AI confidently misleads.
